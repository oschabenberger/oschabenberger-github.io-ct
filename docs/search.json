[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational and Quantitative Thinking",
    "section": "",
    "text": "Preface\nThis material is used to teach UH 2514 Topics in Quantitative and Computational Thinking for Global Citizens in the Honors College of Virginia Tech.\nComputational Thinking (CT) is a problem-solving methodology. It does not imply thinking like a computer scientist, although that connection is often made. CT is about decomposing a problem, recognizing patterns, abstracting solutions, and designing algorithms—repeatable step-by-step instructions. Clearly, computer scientists apply CT in developing software solutions for problems.\nQuantitative Thinking (QT) is also a problem-solving methodology. Its principle approach is to view the world through measurable events, through data.\nThat CT and QT are on the rise is not a surprise. Computerization has followed mechanization during the industrial revolution. “Software is eating the world” as Mark Andreessen of VC firm Andreessen-Horowitz declared in 2011 (Andreessen 2011). In parallel, digitization has turned the world into data. Computer Science has discovered the intrinsic value of data as carrier of information, not just as bits to be stored and exchanged. Data Science has emerged as a combination of the foundational disciplines Statistics, Mathematics, and Computer Science.\nEvery one and every field applies computational and quantitative thinking today. Data literacy—the ability to understand, interpret, critically evaluate, and effectively communicate data in context—is not a technical skill but a fundamental capability for everyone.\nThe 2024 presidential election cycle in the U.S. has laid bare the discrepancy between the official government statistics measuring the health of the economy and the economic reality on the ground. Not because the government was lying to us about inflation, unemployment, or the country’s productivity. But because the way the commonly cited statistics such as unemployment rate, consumer price index (CPI), and gross domestic product (GDP) measure aspects of the economy and not the economic reality of groups. According to Ludwig (2025), for the lower and middle class\n\ndarker assessments of the economy were more authentically tethered to reality\n\nthan the government-reported statistics. For example, the GDP as a measure of a nation’s wealth gives an incomplete picture because it does not tell us how the wealth is shared among the citizens. If one group benefits disproportionately it is no surprise if another group feels left out.\nCritical thinking about data and the result of data analysis is a capability as fundamental as cruising the internet. We have come a long way and still have a long way to go. As Huff (1954) put it so eloquently more than 70 years ago,\n\nMany a statistic is false on its face. It gets by only because the\nmagic of numbers brings about a suspension of common sense\n\nThis remains true today. The antidote is quantitative and compuational awareness and thinking.\nEnjoy.\n\n\n\n\nAndreessen, Mark. 2011. “Why Software Is Eating the World.” https://a16z.com/why-software-is-eating-the-world/.\n\n\nHuff, Darrell. 1954. How to Lie with Statistics. W.W. Norton & Company, New York.\n\n\nLudwig, Eugene. 2025. “Voters Were Right about the Economy. The Data Was Wrong.” Politico. https://www.politico.com/news/magazine/2025/02/11/democrats-tricked-strong-economy-00203464.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Computational Thinking (CT)",
    "section": "",
    "text": "1.1 Elements of CT\nWhat is that like? The five elements of computational thinking are\nLet’s look at the steps in more detail.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Computational Thinking (CT)</span>"
    ]
  },
  {
    "objectID": "intro.html#elements-of-ct",
    "href": "intro.html#elements-of-ct",
    "title": "1  Computational Thinking (CT)",
    "section": "",
    "text": "Problem Definition. What problem are we trying to solve.\nDecomposition (Factoring). Break the problem into smaller parts.\nPattern Recognition. Learning connections and relationships between the parts.\nGeneralization (Abstraction). Recognize relevant details.\nAlgorithmic Design. Define the solution as a series of repeatable steps.\n\n\n\nProblem Definition\nThis should go without saying, before attempting to build a solution one should know what the problem is. It seems obvious but problems are not always what they seem. It takes creativity and thinking to uncover what the real problem is that should be solved. We often take problems as given, fail to uncover the real problem, and end up solving the wrong one (Kroese 2020). Blockbuster turned down a deal with Netflix because it was convinced that customers would not want to have movies delivered to their home. LOL. If customers are telling you that they want “faster horses”, are they looking to you to optimize the amount of hay, water, and carrots you feed horses or are they looking for a faster mode of transportation? Disney’s parks were criticized by customers for long queues for the rides, customers spent hours waiting for their turn. The obvious solutions would be to build more rides or allow fewer visitors in the parks, both would not be good for Disney’s bottom line. Kroese (2020) writes\n\ninstead of investing millions, Disney World simply added themed music, videos, and introduction stories to the waiting areas. Instead of decreasing the waiting time, they increased its value.\n\n\nWe also get it wrong when we are building solutions first and then look for the problems they might solve. The world of technology is littered with solutions looking for a problem.\n\n\n\n\n\n\nSolution looking for a problem\n\n\n\nSolutions looking for problems are commonplace. If someone is trying to sell you a new car when there is nothing wrong with your current car, they (salesman) have a solution (new car) but there is no problem (new car is not needed). So they might try to create a problem by selling you on the benefits of the new model.\nOn the other hand it is good to innovate and create new solutions even if not all problems the innovation solves are known beforehand. When Karl Benz combined in 1885 a one-cylinder internal combustion engine with a carriage (Figure 1.1), he created the first automobile, the Benz Patent-Motorwagen. He probably did not imagine the many future applications of the technology.\n\n\n\n\n\n\nFigure 1.1: Benz Patent-Motorwagen, the first automobile.\n\n\n\n\n\nAnother wonderful example of a solution that did not solve a problem is the story of the company Juicero.\n\n\nExample: Juicero, the juicer no one needed.\n\n\nTrying to cash in on the wellness craze, The Silicon Valley company Juicero aimed to bring the experience of single-serve coffee makers like Keurig to the world of fruit and vegetable juice. Juicero developed a pricey and over-engineered machine to produce single-serve cold-pressed juice at the touch of a button.\n\n\n\n\n\n\nFigure 1.2: The $699 Juicero juicer.\n\n\n\nThe business model was a subscription service. You buy the Juicero juicing machine for the whopping price of $699, subscribe to the service, and the company will regularly send you pouches of diced vegetables and fruits which you run through the juicer. The machine was touted as very powerful, having enough force to “lift two Teslas”. Apparently, the whole thing was a great idea by Silicon Valley standards and Juicero raised $120 million in venture capital funding.\nThe user experience had a few quirks that turned out to be unpopular. You had to use the pouches from the company at $5–$7 per pack. You could buy the pouches only if you owned the $699 juicer. The pouches had an expiration date, lasted only 8 days, and needed to be scanned before running through the juicer; the machine would refuse to process pouches that were past the expiration date.\nThe death knell tolled when Bloomberg reviewed the machine and determined that you can just as easily squeeze the pouches by hand. Check out this Bloomberg video that destroyed the myth of the juicing machine with four tons of pressing power.\nBritish daily newspaper The Guardian mocked Juicero as an example of the Silicon Valley culture to raise large amounts of money for solutions to problems that do not exist.\n\n\n\n\nDecomposition (Factoring)\nThis element of computational thinking ask to break the complex problem into smaller, manageable parts and by doing so helps to focus the solution on the aspects that matter, eliminating extraneous stuff.\nSmaller problems are easier to solve and can be managed independently. A software developer decomposes a task into several functions, for example, one that takes user input, one that sorts data, one that displays results. These functions can be developed separately and are then combined to produce the solution. Sorting can further be decomposed into subproblems, for example, the choice of data structure (list, tree, etc.), the sort algorithm (heap, quicksort, bubble, …) and so on.\nTo understand how something works we can factor it into its parts and study how the individual parts work by themselves. A better understanding of the whole results when we reassemble the components we now understand. For example, to figure out how a bicycle works, decompose it into the frame, seat, handle bars, chain, pedals, crank, derailleurs, brakes, etc.\n\n\nPattern recognition\nPattern recognition is the process of learning connections and relationships between the parts of the problem. In the bicycle example, once we understand the front and rear derailleurs, we understand how they work together in changing gears. Pattern recognition helps to simplify the problem beyond the decomposition by identifying details that are similar or different.\n\n\n\n\n\n\nCarl Friedrich Gauss\n\n\n\nCarl Friedrich Gauss (1777–1855) was one of the greatest thinkers of his time and widely considered one of the greatest mathematicians and scientists of all time. Many disciplines, from astronomy, geodesy, mathematics, statistics, and physics list Gauss as a foundational and major contributor.\nIn The Prince of Mathematics, Tent (2006) tells the story of an arithmetic assignment at the beginning of Gauss’ third school year in Brunswick, Germany. Carl was ten years old at the time. Herr Büttner, the teacher wanted to keep the kids quiet for a while and asked them to find the sum of the first 100 integers, \\[\\sum_{i=1}^{100}i\n\\] The students were to work the answer out on their slates and place them on Herr Büttner’s desk when done. Carl thought about the problem for a minute, wrote one number on his slate and placed it on the teacher’s desk. He was the first to turn in a solution and it took his classmates much longer. The slates were placed on top of the previous solutions as students finished. Many of them got the answer wrong, messing up an addition somewhere along the way. Herr Büttner, going through the slates one by one found one wrong answer after another and expected Gauss’ answer also to be wrong, since the boy had come up with it almost instantly. To his surprise–or dismay–Gauss’ slate showed no work, Carl had written on it just one number, 5,050, the correct answer.\nCarl explained\n\nWell, sir, I thought about it. I realized that those numbers were all in a row, that they were consecutive, so I figured there must be some pattern. So I added the first and the last number: 1 + 100 = 101. Then I added the second and the next to last numbers: 2 + 99 = 101. […] That meant I would find 50 pairs of numbers that always add up to 101, so the whole sum must be 50 x 101 = 5,050\n\nCarl had recognized a pattern that helped him see the connected parts of the problem: a fixed number of partial sums of the same value.\n\n\n\n\nGeneralization (Abstraction)\nOnce the problem is decomposed and the patterns are recognized, we should be able to see the relevant details of the problem and how we go about solving the problem. This is where we derive the core logic of the solution, the rules. For example, to write a computer program to solve a jigsaw puzzle, you would not want to write code specific to one particular puzzle image. You want code that can solve jigsaw puzzles in general. The specific image someone will use for the jigsaw puzzle is an irrelevant detail.\nA rectangle can be decomposed into a series of squares (Figure 1.3). Calculating the area of a rectangle as width x height is a generalization of the rule to calculate the area of a square as width-squared.\n\n\n\n\n\n\nFigure 1.3: Decomposing a 12 x 8 rectangle into six 4 x 4 squares to generalize computation of the area\n\n\n\n\n\nAlgorithm design\nThe final element of CT involves another form of thinking, algorithmic thinking. Here we define the solution as a series of steps to be executed. Algorithmic thinking does not mean the solution has to be implemented by a computer, although this is the case in the narrow sense of CT. The point of the algorithm is to arrive at a set of repeatable, step-by-step instructions, whether these are implemented by humans, machines, or a computer. Capturing the solution in an algorithm is a step toward automation.\n\nFigure 1.4 shows an algorithm to produce pumpkin soup, repeatable instructions that lay out the ingredients and how to process them in steos to transform them into soup.\n\n\n\n\n\n\nFigure 1.4: A recipe for pumpkin soup is an algorithm.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Computational Thinking (CT)</span>"
    ]
  },
  {
    "objectID": "intro.html#making-pumpkin-soup",
    "href": "intro.html#making-pumpkin-soup",
    "title": "1  Computational Thinking (CT)",
    "section": "1.2 Making Pumpkin Soup",
    "text": "1.2 Making Pumpkin Soup\nLet’s apply the elements of computational thinking to the problem of making pumpkin soup.\n\nDecomposition\nDecomposition is the process of breaking down a complex problem into smaller, more manageable parts. In the case of making pumpkin soup, we can break it down into several steps:\n\nIngredients: Identify the key ingredients required for the soup.\n\nPumpkin\nOnion or Leek\nGarlic\nStock (vegetable or chicken)\nCream (optional)\nSalt, pepper, and spices (e.g., nutmeg, cinnamon)\nOlive oil or butter for sautéing\n\nPreparation: Break down the actions involved in preparing the ingredients.\n\nPeel and chop the pumpkin\nChop the onion and garlic\nPrepare spices and seasoning\n\nCooking: Identify the steps in cooking the soup.\n\nSauté the onion and garlic\nAdd the pumpkin and cook it\nAdd stock and bring to a simmer\nPuree the mixture\nAdd cream and season to taste\n\nFinal Steps: Focus on finishing touches.\n\nGarnish (optional)\nServe and taste for seasoning adjustments\n\n\n\n\nPattern Recognition\nWhat are the similar elements or repeating steps in the problem?\n\nCommon cooking steps: Many soups follow a similar structure: sautéing vegetables, adding liquid, simmering, and then blending or pureeing.\nIngredient variations: While the exact ingredients for pumpkin soup may vary (e.g., adding coconut milk instead of cream), the basic framework of the recipe remains the same.\nTiming patterns: There’s a pattern to the cooking times: first sautéing for a few minutes, then simmering the soup for about 20-30 minutes, followed by blending.\n\n\n\nGeneralization\nWe can generalize (abstract) the process of making pumpkin soup into a more general recipe for making any pureed vegetable soup, regardless of the specific ingredients.\n\nEssential components:\n\nA base (onions, garlic, or other aromatics)\nA main vegetable (in this case, pumpkin)\nLiquid (stock, broth, or water)\nSeasoning and optional cream\n\nGeneral process:\n\nSauté aromatics.\nAdd the main vegetable and liquid.\nSimmer until the vegetable is tender.\nBlend until smooth.\nAdjust seasoning and add cream if desired.\n\n\n\n\nAlgorithm Design\nHere is a simple algorithm for making pumpkin soup:\n\nPrepare ingredients:\n\nPeel and chop the pumpkin into cubes.\nChop the onion and garlic.\n\nSauté aromatics:\n\nIn a pot, heat oil or butter over medium heat.\nAdd chopped onion and garlic, sauté for 5 minutes until softened.\n\nCook pumpkin:\n\nAdd chopped pumpkin to the pot and sauté for 5 minutes.\nAdd stock to cover the pumpkin (about 4 cups) and bring to a boil.\n\nSimmer:\n\nLower the heat, cover, and let the soup simmer for 20-30 minutes until the pumpkin is tender.\n\nBlend the soup:\n\nUse an immersion blender or transfer the soup to a blender. Puree until smooth.\n\nAdd cream and seasoning:\n\nStir in cream (optional) and season with salt, pepper, and spices to taste (e.g., nutmeg or cinnamon).\n\nServe:\n\nPour into bowls and garnish with optional toppings (e.g., a swirl of cream, roasted seeds, or fresh herbs).\n\n\nFigure 1.4 is a specific implementation of the algorithm.\nBy applying computational thinking, we decomposed the task of making pumpkin soup into smaller steps, recognized patterns in the cooking process, abstracted the general process for making soups, and designed an algorithm to efficiently make pumpkin soup. This method helps streamline the cooking process, ensures nothing is overlooked and provides a clear, repeatable procedure.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Computational Thinking (CT)</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-intro-ct-importance",
    "href": "intro.html#sec-intro-ct-importance",
    "title": "1  Computational Thinking (CT)",
    "section": "1.3 Importance of CT",
    "text": "1.3 Importance of CT\nComputational thinking as a methodology is applied nowadays in almost any domain and to many problems. Decomposing a problem into parts, recognizing patterns, abstracting the problem essentials and capturing them in an algorithm is an effective approach to deal with many problems.\nComputational thinking is becoming the new literacy of the 21st century because it enables you to bend computation to your needs (Wing 2011). It has begun to influence disciplines and professions beyond science and engineering. Algorithmic medicine, computational archaeology, computational economics, computational finance, computation and journalism, computational law, computational social science, and digital humanities are now areas of active study.\nThe rise of computational thinking is also due to the fact that many more problems are solved through software and computing today than in the past. We have become very good at capturing the essential details of processes and phenomena through models, which by their very nature are abstractions of the prevailing patterns in those processes. And the implementation of the models frequently involves the design and deployment of algorithms.\nFinancial analysts make investment decisions based on mathematical models of market behavior. Banks decide whether to award a loan based on statistical models for the probability of loan default. Insurance premiums are based on risk models. A healthcare provider chooses a treatment plan based on assessment of risk factors and predictions of disease progression, based on medical knowledge and models that describe patient outcomes.\nIn 2011, Mark Andreessen of VC firm Andreessen-Horowitz (known as a16z) declared\n\nSoftware is eating the world.\n\nWhy was this happening? Andreessen (2011) cites several reasons, among them\n\nSix decades into the computer revolution, four decades since the invention of the microprocessor, and two decades into the rise of the modern Internet, all of the technology required to transform industries through software finally works and can be widely delivered at global scale.\n\nThe largest book seller, Amazon, is a software company. Prior to the software revolution, it was brick-and-mortar book stores like Borders. The largest provider of video services, Netflix, is a software company. Previously, you rented physical video cassette tapes at Blockbuster. The music we listen to today is stored digitally as a file, distributed through software, and made audible through software. The best recruiting company, LinkedIn, is a software company. Some of the best movies are created by Pixar, a software company. You get the idea, I’ll stop here.\nWhile software implementations of problem solving have upended many industries, the way we build and use software, and the types of problems we can solve with it, is itself being upended, thanks to the advances in large-language models (LLMs) like ChatGPT, Claude, Gemini, and others.\nThe shift toward computational thinking and the importance of solving problems through software has become most evident in 2024, when Nobel prizes in Physics and Chemistry were awarded not to scientists in those fields, but to computer scientists and artificial intelligence researchers who developed the foundational computational methods that helped to advance Physics and Chemistry (Figure 1.5 and Figure 1.6).\n\n\n\n\n\n\nFigure 1.5: 2024 Nobel Prize winners in Physics\n\n\n\n\n\n\n\n\n\nFigure 1.6: 2024 Nobel Prize winners in Chemistry\n\n\n\nDemis Hassabis, for example, is the CEO of Google DeepMind, the company behind AI projects such as AlphaGo, the reinforcement-learning trained system that accomplished what was thought impossible for a computer to do: beat the best Go player in the world.\nGeoffrey Hinton is a leading figure in research on artificial neural networks and deep learning. He is often considered the “Godfather of AI”. He is co-author of a 1986 paper that popularized back-propagation, an algorithm that efficiently computes the gradients in a neural network with many layers.\nBreaking with tradition, the Nobel committee awarded these prizes not to scientists who developed grand new theories of how the world works, but to scientist who develop the computational tools that help us develop grand new theories of how the world works.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Computational Thinking (CT)</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-intro-wicked",
    "href": "intro.html#sec-intro-wicked",
    "title": "1  Computational Thinking (CT)",
    "section": "1.4 Wicked Problems",
    "text": "1.4 Wicked Problems\nNot all problems are made the same. Simple problems can be dealt with by linear thinking: here is a problem, this is the solution, and the problem goes away. Grint (2022) distinguishes between three types of problems:\n\nTame problems: we know how to fix the problem with standard operating procedures (SOPs). Tame problems require managers to perform or delegate the problem solving.\nCritical problems: crises that are self-evident, and we are hoping someone knows how to sort them out. These problems require commanders who know the solution, are decisice, and direct and coerce others.\nWicked problems: problems we do not know how to fix. Solving wicked problems requires leaders who recognize their own limits and the wicked nature of the problem.\n\nA problem can be tame for some and wicked for others. Performing heart surgery would be a wicked problem for most of us because we have no clue how to go about it. For an experienced heart surgeon who performs the procedures every day this is a tame problem. Participants of the same problem can experience the same situation from different problem perspective. A traffic accident is a tame problem for a cop who deals with accidents routinely. For the drivers involved the problem presents a crisis.\nA single situation can have overlapping problem types and call for a combination of decision styles to deal with it. The Covid-19 pandemic is a case in point. Hoban (2023) writes\n\nvaccine development and distribution are simple problems as they follow known procedures; isolation, distancing and mask wearing are critical problems but both the simple and the critical shade over into the wicked problems of a culture where people may not trust their government, governments may have political agendas, people’s exposure and susceptibility to infection are linked to socioeconomic factors which in turn are downplayed by governments and so on.\n\n\n\nAssignment: CO2 Emissions\n\n\nSome think that we are emitting too much CO2 by burning fossil fuels and that this is a tame problem: move from energy based on fossil fuels to renewable energy sources. No more fossil fuels, no more CO2 emissions from burning fossil fuels, problem solved.\nOthers think that reducing CO2 emissions is a wicked problem. A lot of CO2 emissions are already baked into the system and even if all CO2 emissions would stop today the climate would still be changing. Any action we take to change CO2 emissions changes the system itself. Once we manipulate the system do we have to look at other components besides the climate?\nGive reasons why reducing CO2 emissions is a wicked problem.\n\n\nAre wicked problems so difficult that they defy solution? Should we just throw up our arms and walk away? Not at all. By definition a wicked problem does not have a solution, at least not yet. Heart surgery was a wicked problem for surgeons until medical science advanced sufficiently to make it tameable. Today, heart surgery is a known problem with known solutions and standard operating procedures. Wicked problems require not managers or commanders, but leaders who recognize their own limits and the wickedness of the problem. Leaders who can learn and grow, make and admit mistakes, and seek solutions for the public good.\nHoban (2023) gives three guidelines for dealing with wicked problems:\n\nDo no harm: do nothing that has a chance of doing catastrophic harm. Do not attempt to change the entire system at once.\nFind where you are: you are somewhere in the middle of the problem. Every actor who wants to influence the system is part of the system.\nCreativity through constraints: just like the rules of a game force you to innovate within the constraints of the rules, know what you can and cannot do. Humans not being able to fly or breath underwater are wicked problems. Knowing that we cannot solve them by flapping our arms or by growing gills identifies constraints that point in the direction of a solution.\n\n\n\n\nFigure 1.1: Benz Patent-Motorwagen, the first automobile.\nFigure 1.4: A recipe for pumpkin soup is an algorithm.\nFigure 1.5: 2024 Nobel Prize winners in Physics\nFigure 1.6: 2024 Nobel Prize winners in Chemistry\n\n\n\nAndreessen, Mark. 2011. “Why Software Is Eating the World.” https://a16z.com/why-software-is-eating-the-world/.\n\n\nGrint, Keith. 2022. “Critical Essay: Wicked Problems in the Age of Uncertainty.” Human Relations 75: 1518–32.\n\n\nHoban, Jake. 2023. “Embracing Wicked Problems.” https://medium.com/@personofnorank/embracing-wicked-problems-76ec9c210f29.\n\n\nKroese, Jasper. 2020. “Companies Are Brilliantly Solving the Wrong Problems.” https://marker.medium.com/why-organizations-are-so-good-at-solving-the-wrong-problems-17d414d0259.\n\n\nTent, M. B. W. 2006. The Prince of Mathematics: Carl Friedrich Gauss. CRC Press, Boca Raton, FL.\n\n\nWing, Jeanette M. 2011. Research Notebook: Computational Thinking–What and Why? Carnegie Mellon Univeristy School of Computer Science. https://people.cs.vt.edu/~kafura/CS6604/Papers/CT-What-And-Why.pdf.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Computational Thinking (CT)</span>"
    ]
  },
  {
    "objectID": "quant.html",
    "href": "quant.html",
    "title": "2  Quantitative Thinking (QT)",
    "section": "",
    "text": "2.1 Benefits of Quantification\nThe first obvious benefit of quantification is to make information amenable to mathematical and statistical operations. When mathematical or statistical calculations are concerned, any type of data will eventually be represented as numbers. A photograph turns into row and column indices of pixels and three-number triplets of red, green, and blue intensities. A sophisticated large language model that processes textual information converts the text into numeric representations, a process called encoding.\nQuantification allows us to express relationships: ranking, ordering, and measuring proximity. The Gross Domestic Product (GDP), for example, is use to measure the productivity of a country and is frequently used to rank countries and to delineate groups of countries. For example, a countries’ portion of global GPD is one of the criteria for G20 membership. Throughout history countries have competed for status. Quantifying status is a much better solution than determining status by sending soldiers and weapons across borders.\nAccording to one view, the de-contextualized and value-free mathematical symbols used in statistical analyses assist in achieving objectivity, stability, and fairness in decisions. Quantification is a method of standardization that summarizes and reduces concepts to their essence and allows us to make better decisions. Quantification captures complex systems in easy to understand numbers. As some would argue that works well on paper.\nHuff (1954), in one of the most widely published statistics texts, How to Lie with Statistics, states\nQuantification certainly can achieve the goal of capturing complex systems and assisting with objective and fair decisions. We do have to apply a critical eye though and ask how a metric is defined, named, and determined. Just because you call something an index for X does not mean it is a good measure of X.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quantitative Thinking (QT)</span>"
    ]
  },
  {
    "objectID": "quant.html#benefits-of-quantification",
    "href": "quant.html#benefits-of-quantification",
    "title": "2  Quantitative Thinking (QT)",
    "section": "",
    "text": "Note\n\n\n\nAs we will see in a later chapter, sorting and searching are fundamental. Observe how many times a day you encounter sorted lists, rankings, top-ten results, best-of lists, most-preferred lists and other ways to arrange information by some metric of relevance. Google search would not be so successful if searching would not be combined with ranking results by relevance.\n\n\n\n\n\nMany a statistic is false on its face. It gets by only because the\nmagic of numbers brings about a suspension of common sense\n\n\n\n\nExercise: Unemployment Rate, Consumer Price Index, Gross Domestic Product\n\n\nUR, CPI, and GDP are among the very important statistics reported by the government. They are used as indicators for the health and status of the economy and closely observed by economists, investors, politicians, …, well, everybody.\nDuring the 2024 presidential election cycle in the U.S. these statistics indicated a strong economy. Yet many voters seemed to reject that idea. The disconnect between the reported reality of the economy and the public’s perception of the economy was baffling to many politicians. How can the public believe that the economy is bad if unemployment rate is at a low 4.2%?\nRather than pointing the finger at group think and social media echo chambers, Ludwig (2025) raises an interesting question in this article in Politico:\n\nWhat if the government statistics we rely on as indicators of economic well being are not really measuring what we think they are? What if they are fundamentally flawed and those flaws create the discrepancy between reality and theory?\n\nThis is not a question of partisan politics. The three statistics have been in place for a long time and are calculated the same way regardless of which party holds sway. It is a question of whether the government’s measurements properly capture the realities of the economy as a whole. Ludwig argues that unemployment is higher, wages are lower and growth is not as robust as the three government statistics suggest.\n\nFor each of the three statistics (U3 for unemployment, CPI, and GDP) list the reasons cited in Ludwig (2025) why the statistic fails to measure economic reality.\nIf the values of the statistics were adjusted in the way suggested in the article, how would the reported values change?\nIs Ludwig (2025) arguing that the current statistics are without merit and should be abandoned?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quantitative Thinking (QT)</span>"
    ]
  },
  {
    "objectID": "quant.html#easy-and-difficult-to-quantify",
    "href": "quant.html#easy-and-difficult-to-quantify",
    "title": "2  Quantitative Thinking (QT)",
    "section": "2.2 Easy and Difficult to Quantify",
    "text": "2.2 Easy and Difficult to Quantify\nWhen dealing with inherently measurable attributes such as height or weight, quantification is simple. We need an agreed-upon method of measuring and a system to express the measurement in. The former might be an electronic scale or an analog scale with counterweights for weight measurement, a ruler, yardstick, or laser device for height measurements. It seems obvious to report weights in metric units of milligrams, grams, pounds (500 grams), kilograms, and tons or in U.S. Customary units of ounces, pounds (16 ounces), and tons. As long as we know which measurement units apply, we can all agree on how heavy something is. And we need to keep in mind that the same word can represent different things: a metric pound is 500 grams, a U.S. pound is 453.6 grams. But wait, there is more: Apothecaries’ weights are slightly different, a pound a.p. is 12 ounces. And in some fields, weights are measured entirely differently, diamonds are measured in carats (0.2 grams). In the International System of Units (SI), weight is measured in Newtons, which is gravitational force on a mass, equivalent to kg * m /s2. As long as we know what units are used to report a weight, we can convert it into whatever system we want to use. So we could say that this attribute is easily quantifiable—although there are always wrinkles.\nSome attributes are easy to quantify once we have agreed on a definition and methodology to turn concept into numbers. The unemployment rate and the Consumer Price Index from the previous exercise are examples. We can count employed and unemployed folks and compute the ratio \\[\n\\frac{\\# \\text{unemployed}}{\\# \\text{(employed + unemployed)}}\n\\] The difficulty is to define what constitutes employment or unemployment. Is seasonal work employment? Is an occasional gig job unemployment? How do you capture those who have stopped looking for a job after 200 failed job applications?\nOther attributes are genuinely difficult to quantify by their very nature. They elude a definition we could agree upon. How do you measure happiness? Finland has been declared the happiest country on earth for seven years running. This must involve some form of quantification otherwise we could not rank countries and declare one as “best”. How did they come up with that? The purpose of the World Happiness Report is to review the science of measuring well-being and to use survey measures of life satisfaction across 150 countries. Happiness according to the World Happiness Report is a combination of many other measurements. For example, a rating of one’s overall life satisfaction, the ability to own a home, the availability of public transportation, etc. Clearly, there is a subjective element in choosing the measurable attributes that are supposed to allow inference about the difficult to measure attribute happiness. Not all attributes weigh equally in the determination of happiness, the weighing scheme itself is part of the quantification. Norms and values also must play a role. The availability of public transportation affects quality of life differently in rural Arkansas and in downtown London. In short, the process of how we quantify a difficult-to-measure attribute should be part of the conversation.\n\n\nAssignment: World Happiness Report\n\n\nRead the section Measuring and Explaining National Differences in Life Evaluations in the 2024 World Happiness Report\n\nWhich variables is the ranking of happiness based on?\nHow many citizens of each country participate in the survey?\nDoes WHR collect its own data or does it rely on someone else’s survey?\nThe data includes three indicators for well-being. Are they all used in determining the happiness rankings?\nIn the discussion of the methods, can you determine whether the happiness rankings involve some form of modeling, where survey responses are tied to other variables? If so, what are the variables? Are these reflected in Table 2.1?\n\n\n\nVariables are difficult to quantify for various reasons:\n\nInconsistent views about what they mean—how do you measure freedom?\nDifficult to define—what is trust?\nSubjectivity—introversion\nAbstract concepts—creativity, natural beauty, etc.\n\nHere is a short list of difficult-to-quantify concepts from different areas:\n\nPersonal and Psychological Attributes\n\nEmotions: Happiness, sadness, or anxiety levels can be challenging to measure as they are subjective and context-dependent.\nPersonality traits: Attributes like openness, conscientiousness, or introversion often rely on self-reporting and are hard to measure precisely.\nSelf-esteem: The internal sense of self-worth varies across situations and over time.\nEmpathy: Understanding the emotions of others is influenced by individual perception.\nIntelligence: there are different forms of intelligence (emotional, spatial, linguistic, musical, etc.). IQ tests provide incomplete pictures.\n\n\n\nSocial and Cultural Variables\n\nSocial cohesion: The sense of community and connection within a group.\nCultural values: Abstract beliefs such as collectivism or individualism differ across societies.\nTrust: The degree of confidence people have in others or institutions can be highly context-sensitive.\nSocial norms: Expectations about behavior that are implicit and vary among groups.\n\n\n\nEthical and Moral Attributes\n\nFairness: What is considered “fair” depends on personal, cultural, and contextual factors.\nIntegrity\nJustice: Varied interpretations of what constitutes equitable treatment or outcomes.\n\n\n\nCreative and Artistic Variables\n\nCreativity: Measuring originality and innovation is inherently subjective.\nAesthetic appeal: People’s appreciation of beauty is highly personal and culturally influenced.\nTalent\n\n\n\nEnvironmental and Ecological Factors\n\nBiodiversity value: The intrinsic worth of maintaining species diversity is challenging to calculate in monetary or ecological terms.\nEcosystem health: Assessing overall resilience, productivity, or stability of ecosystems involves many interdependent variables.\nNatural beauty\n\n\n\nEconomic and Market Variables\n\nBrand loyalty is an emotional attachment to a product or service.\nConsumer satisfaction: Highly subjective and influenced by expectations and individual preferences. Innovation potential: The likelihood that a new product or idea will succeed in the market.\n\n\n\nAbstract Concepts\n\nFreedom: The degree to which individuals or groups are free can depend on legal, social, and personal dimensions.\nHappiness: The overall well-being of a population is a composite of subjective factors.\nPotential: The latent ability for growth or success in individuals or systems.\n\nYou can easily grow the list and find examples of difficult-to-quantify variables from many fields. It is a common problem in many sciences: what we are interested in measuring is difficult to quantify and the process of quantification is full of assumptions. Instead of getting at the phenomenon directly, we use other quantities to inform about all or parts of what we are really interested in. These surrogates are known by different names: we call them an indicator, an index (such as the consumer price index), a metric, a score, and so on.\n\n\nExample: Net Promoter Score (NPS)\n\n\nBuilding on the theme of “happiness”, a frequent question asked by companies that sell products or services is “are my customers satisfied and are they loyal?”\nRather than an extensive survey with many questions as in the World Happiness Report, the question of customer satisfaction and loyalty is often distilled into a single metric in the business world, the Net Promoter Score (NPS). NPS is considered by many the gold standard customer experience metric. It rests on a single question: “How likely are you to recommend the companies products or services?”\nThe calculation of NPS is as follows (Figure 2.1):\n\nCustomers answer the question on a scale of 0–10, with 0 being not at all likely to recommend and 10 being extremely likely to recommend.\nBased on their response, customers are categorized as promoters, passives, or detractors. A detractor is someone whose answer was between 0 and 6. A promoter is someone whose answer is 9 or 10. The others, which responded with a 7 or 8 are passives.\nThe NPS is calculated by subtracting the percentage of detractors from the percentage of promoters.\n\nThe NPS ranges from -100 to 100, higher scores imply more promoters. A NPS of 100 is achieved if everyone scores 9 or 10. A score of -100 is achieved if everyone scores 6 or below.\n\n\n\n\n\n\nFigure 2.1: Net promoter score\n\n\n\n\n\n\n\nExercise: Net Promoter Score\n\n\n\nWhat are the assumptions in the NPS calculation?\nCompany Foo improved its NPS from 30 to 40 over the last year. Explain how that can happen.\nWhat does NPS tell you about a company that has many products and/or services?\nWhat impact could cultural differences and societal norms and traditions have on NPS values around the world?\nWhat do you think is a great net promoter score? Does it depend on the industry?\nCompanies are applying NPS in other contexts, not just to measure customer satisfaction. For example, the employee NPS (eNPS) uses NPS methodology and the question “How likely are you to recommend company X as a place of work?” What do you think about that?\nIf you plot NPS by age, what would that look like? In other words, do you expect younger or older consumers to have higher/lower NPS?\nList reasons why NPS is (might be) a troubling indicator. \n\n\n\nThe NPS has many detractors, pun intended. Some describe it as “management snake oil”. Management touts it when the NPS goes up, nobody reports it when it goes down. It continues to be widely used. Forbes reported that in 50 earnings calls of S&P 500 companies NPS was mentioned 150 times in 2018.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quantitative Thinking (QT)</span>"
    ]
  },
  {
    "objectID": "quant.html#indicator-and-index",
    "href": "quant.html#indicator-and-index",
    "title": "2  Quantitative Thinking (QT)",
    "section": "2.3 Indicator and Index",
    "text": "2.3 Indicator and Index\nAn indicator is a quantitative or qualitative factor or variable that offers a direct, simple, unique and reliable signal or means to measure achievements.\nThe economy is a complex system for the distribution of goods and services. Such a complex system defies quantification by a single number. Instead, we use thousands of indicators to give us insight into a particular aspect of the economy: inflation rates, consumer price indices, unemployment numbers, gross domestic product, economic activity, etc.\n\n\nExercise: Quantifying the Economy\n\n\nFind at least two indicators in each of the following aspects:\n\nInternational trade\nHousing and construction\nConsumer spending\nManufacturing\nClimate\nLabor markets\n\nWhat are the indicators used for–that is, what do they indicate?\n\n\nAn indicator is called leading if it is predictive, informing us about what might happen in the future. The job satisfaction in an employee survey is a leading indicator of employee attrition in the future. Unhappy employees are more likely to quit and to move on to other jobs. A lagging indicator is descriptive, it looks at what has happened in the past. Last month’s resignation rate is a lagging indicator for the human resources department.\n\nWhen multiple indicators are combined, we sometimes call it an index, although the distinction is not sharp. Indicators can also be the result of aggregation so that the distinction between indicator and index becomes one of degree of combining information (with an index being more aggregated or combining more individual pieces of information). In some domains, the word index is simply used more frequently than indicators, and vice versa.\n\n\nExample: Body Mass Index (BMI)\n\n\nThe Body Mass Index (BMI) is a medical screening tool for certain health conditions. Colloquially, it is understood as a measure of “fatness”. That is not quite correct, in most people, BMI only correlates with body fat. We will learn more about the concept of correlation (association) later. BMI has become the standard indicator for obesity.\nThe calculation of BMI involves two variables (two indicators): a person’s weight in kilograms (kg) and their height in meters (m): \\[\n\\text{BMI} = \\frac{\\text{Weight in kg}}{(\\text{Height in m})^2}\n\\]\nNotice that the height is squared in the denominator. If you prefer to work in U.S. pounds and inches, the calculation is \\[\n\\text{BMI} = \\frac{\\text{Weight in lbs} \\times 703}{(\\text{Height in inches})^2}\n  \\]\nIn my case (6’3” tall, 208 lbs), the BMI is (208 )/(75^2) = 25.9. According to BMI charts such as this one at the Cleveland Clinic, I am in the overweight range.\nBMI is used widely by medical professionals. People with low values might be at risk for developing anemia, osteoporosis, infertility, malnutrition, and a weakened immune system. High values can indicate a higher risk for heart disease, high blood pressure, type 2 diabetes, gallstones, osteoarthritis, sleep apenea, depression, and certain cancers.\nGeez. It seems that unless you are in the optimal BMI range you are either bound for osteoporosis or osteoarthritis. No wonder folks are obsessing over their BMI.\nThe Cleveland Clinic is quick to point out:\n\nIt’s important to remember that you could have any of the above health conditions without having a high BMI. Similarly, you could have a high BMI without having any of these conditions.\n…\nIt’s important to remember that body fatness isn’t the only determiner of overall health. Several other factors, such as genetics, activity level, smoking cigarettes or using tobacco, drinking alcohol and mental health conditions all affect your overall health and your likelihood of developing certain medical conditions.\n\nAnd\n\nThe standard BMI chart has limitations for various reasons. Because of this,\nit’s important to not put too much emphasis on your BMI.\n\nPeople who are muscular can have a high BMI and still have very low fat mass. The BMI does not distinguish between lean body mass and fat body mass. BMI charts do not distinguish between males and females, although females tend to have more body fat (says the Cleveland Clinic!). People today are taller than when the BMI was developed. The BMI charts do not apply to athletes, children, pregnant people, or the elderly.\n\nEven though the BMI chart can be inaccurate for certain people, healthcare providers still use it\nbecause it’s the quickest tool for assessing a person’s estimated body fat amount.\n\nAh, so it is used because it is easy to calculate, not because it is particularly useful or accurate.\nA January 2025 article in the medical journal The Lancet states (Rubino et al. 2025)\n\nCurrent BMI-based measures of obesity can both underestimate and overestimate adiposity and provide inadequate information about health at the individual level, which undermines medically-sound approaches to health care and policy.\n\nBased on this article, Business Insider went a step further, calling BMI bogus. Having “obesity” according to the BMI scale does not mean a person is unhealthy. In fact, BMI does not tell you anything about the health of a person.\nIt is worthwhile to examine how BMI came about. The comment above about the increasing height of people suggests that BMI was developed some time ago. Indeed. It was invented for an entirely different reason, to describe a population average man in Western Europe in the 19th century.\nTo make this connection we need to introduce Adolphe Quetelet (1796–1847), who invented the BMI to quantify a population according to its person’s weight. It was initially called the Quetelet index. Quetelet was a Belgian astronomer, statistician, and mathematician—not a medical professional. He studied the distribution of physical attributes in populations of French and Scottish people. Quetelet determined that the normal, the most representative value of an attribute, is its average. Prior to Quetelet, the idea of “norm” and “normality” was associated with carpentry and construction. The carpenter square is also called the norm and in normal construction everything is at right angles. The classical notion of ideal as an unattainable beauty up to this time was reflected in great works of art.\nQuetelet focused on the middle of the distribution as the “new normal” and saw l’homme moyen, the average man, as the ideal (Grue and Heiberg 2006).\nThere is no association with health, and there is no association with the individual. The BMI as developed by Quetelet was supposed to describe the average in a population, not obesity of the individual. The population it was intended to describe is French and Scottish of the 19th century. Going from there to a near universal measure of obesity since the 1970s is quite the stretch.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quantitative Thinking (QT)</span>"
    ]
  },
  {
    "objectID": "quant.html#sec-data-types",
    "href": "quant.html#sec-data-types",
    "title": "2  Quantitative Thinking (QT)",
    "section": "2.4 Types of Data",
    "text": "2.4 Types of Data\nData, the result of quantification, can be classified in a number of ways. The first distinction of quantified variables is by data type.\n\nContinuous: the number of possible values of the variable is not countable. Examples are physical measurements such as weight, height, length, pressure, temperature.\nDiscrete: the number of possible values is countable. Even if the number of possible values is infinite, the variable is still discrete. The number of fish caught per day does not have a theoretical upper limit, although it is highly unlikely that a weekend warrior will catch 1,000 fish. A commercial fishing vessel might.\n\nDiscrete variables are further divided into the following groups:\n\nCount Variables: the values are true counts, obtained by enumeration. There are two types of counts:\n\nCounts per unit: the count relates to a unit of measurement, e.g., the number of fish caught per day, the number of customer complaints per quarter, the number of chocolate chips per cookie, the number of cancer incidences per 100,000.\nProportions (Counts out of a total): the count can be converted to a proportion by dividing it with a maximum value. Examples are the number of heads out of 10 coin tosses, the number of larvae out of 20 succumbing to an insecticide,\n\nCategorical Variables: the values consist of labels, even if numbers are used for labeling.\n\nNominal variables: The labels are unordered, for example the variable “fruit” takes on the values “apple”, “peach”, “tomato” (yes, tomatoes are fruit but do not belong in fruit salad).\nOrdinal variables: the category labels can be arranged in a natural order in a lesser-greater sense. Examples are 1—5 star reviews or ratings of severity (“mild”, “modest”, “severe”).\nBinary variables: take on exactly two values (dead/alive, Yes/No, 1/0, fraud/not fraud, diseased/not diseased)\n\n\n\nCategorical variables are also called qualitative variables. They encode a quality, namely to belong to one of the categories. All other variables are also called quantitative variables. Note that quantifying something through numbers does not imply it is a quantitative variable. Highway exits might have numbers that are simple identifiers not related to distances. The number of stars on a 5-star rating scale indicates the category, not a quantified amount. The numeric values of quantitative variables, on the other hand, can be used to calculate meaningful differences and ratios. 40 kg is twice as much as 20 kg, but a 4-star review is not twice as much as a 2-star review—it is simply higher than a 2-star review.\n\n\nTypes of Variables\n\n\n\n\nGive examples for the following types of variables:\n\nNominal\nOrdinal\nCounts per unit\nBinary\nContinuous\n\nWhat are the data types of the following:\n\nnumber of meals served in the cafeteria\nnumber of meals served in the cafeteria per day\nproportion of meals not finished\nmarital status\ntensile strength of a material\npercentage of patients who show side effects\npercentage of income spent on food and housing\n\nReturn to the World Happiness Report from an earlier assignment in this module. What type of data is the Cantril Ladder mentioned in that report?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quantitative Thinking (QT)</span>"
    ]
  },
  {
    "objectID": "quant.html#uncertainty",
    "href": "quant.html#uncertainty",
    "title": "2  Quantitative Thinking (QT)",
    "section": "2.5 Uncertainty",
    "text": "2.5 Uncertainty\nWhen I hop on the scales in the bathroom in the morning, I usually take two or three measurements in quick succession. They rarely are identical, differing by a fraction of a pound. Day-to-day variation could be understandable, our weight does change over time. But second-to-second variation, that seems odd. It seems unlikely that I gained or lost a quarter pound over the last 5 seconds.\nQuantifying things is associated with uncertainty that is introduced through various forms of variability. My weight measurement varies from one measurement to the next because of my posture, movement on the scales, and the performance of the scales mechanism itself. Not because my actual weight differs. This type of variability is called measurement error. If we measure the weights of students in a classroom another source of variability is introduced, subject-to-subject variability. If we randomly choose a classroom from the school, the measurements also represent classroom-to-classroom variability, since repeating the process and selecting a different classroom will yield a different number.\nIn the presence of uncertainty, quantities are not knowable a priori. But if the uncertainty itself can be quantified, then the quantity of interest is predictable. I am unable to say what the average weight is of a student in a freshman class. But by measuring weights and quantifying uncertainty in the observations I can predict with a high level of confidence that the average freshmen students’ weight is between this much and that much (now attach your favorite weight units).\nIt is one of humanities’ great advances that we are not only able to represent concepts in numbers, but that we can quantify uncertainty itself. Probability and statistics enable us to learn from uncertain events and measurements. This adds to our vocabulary of lengths, weights, volumes, five-star ratings, and so on, the terms prediction, forecast, likelihood, odds, chance, and probability.\nA meteorologist forecasting a 30% chance of rain tomorrow is quantifying the likelihood of an event. We all know how to operationalize this forecast. The higher the number, the more likely we will need an umbrella the next day. Interestingly, while we have developed intuition about “the chance of rain”, many do not know how to correctly interpret a statement such as “30% chance of rain”. It does not mean that 30% of the area covered by the forecast will see rain. It does not mean that on days like today, 30% of them will have a rain event. Rather, it means that there is a 0.3 probability that any point in the forecast area will see a measurable amount of rain (usually 0.01 inches or more).\nAnother interpretation flows from the fact that weather forecasts are based on models that simulate weather conditions based on inputs. To account for the inherent uncertainty in measuring the inputs, multiple scenarios (simulations) are run. a 30% chance of rain means that 30% of the simulations predicted rain for the forecast area.\n\nBy applying statistical principles in the analysis of data uncertainty can be reduced. More informed insights about the data are then possible. Suppose we want to measure the average amount of an attribute in a population—say, the average years of postgraduate education. The population is too large to visit with every member so we instead quiz only a smaller number about their education. If we randomly sample one person from the population, we get a statistically valid estimate for the entire population. If they have 2 years of postgraduate education, our best guess for the average amount of post-graduate education in the entire population is 2 years. However, we have not reduced the uncertainty at all, the single measurement is as uncertain as the variability of postgraduate education years in the population. Suppose this variability can be quantified with the amount \\(X\\). If we repeat the process of randomly sampling persons \\(n-1\\) more times and computing the average across the \\(n\\) measurements—this is called the sample average—we get a much better estimate of the average number of years of postgraduate education in the population. It turns out that the sample average so obtained has uncertainty \\(X/n\\). In other words, we can make our statement about the population quantity of interest arbitrarily precise by sampling a large number of people from the population.\nFor attributes with large variability \\(X\\) it will take a larger sample size \\(n\\) to achieve the same level of precision compared to attributes with less variability. If the attribute does not vary at all in the population (\\(X=0\\)), then a single sample is sufficient. In the population of apartment renters there is no variability in the attribute renter, but there is variability in the attribute amount of rent late.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quantitative Thinking (QT)</span>"
    ]
  },
  {
    "objectID": "quant.html#sec-intro-surrogacy",
    "href": "quant.html#sec-intro-surrogacy",
    "title": "2  Quantitative Thinking (QT)",
    "section": "2.6 Surrogacy",
    "text": "2.6 Surrogacy\nA surrogate metric (proxy metric) is one that is used in the place of another. We have seen earlier how economic indicators are used to describe aspects of the economy:\n\nThe consumer price index is a surrogate metric for cost of living and inflation.\nThe net promoter score is a surrogate metric for customer loyalty.\n\nWhen quantifying a variable of interest is difficult, time consuming, expensive, or destructive, and a surrogate variable is easy to quantify, the surrogate can take the place of the variable of interest.\nHere are some examples of surrogacy:\n\nHealthcare\n\nCholesterol levels as a proxy for heart health.\n\nEducation\n\nStandardized test scores as surrogates for student achievement.\nGraduation rates as a proxy for work force preparation.\n\nManufacturing\n\nForce to pull a nail through a gypsum board as a surrogate for dry wall quality.\n\nEconomy\n\nGross Domestic Product as a surrogate for societal well-being.\nConsumer Price Index as a surrogate for purchasing power.\nUnemployment rate as a surrogate for economic health.\n\nEnvironmental Science\n\nCO2 levels as proxy for climate change.\nTree canopy cover as a proxy for biodiversity.\n\nHuman Resources\n\nMeasuring employee productivity by number of hours worked.\nEmployee attrition as a surrogate for employee satisfaction.\n\nTechnology\n\nNumber of downloads of an app as a measure of market share.\nLikes and shares as a measure of social media engagement\n\nAcademia\n\nCitation count as a surrogate for research impact.\n\n\n\n\nExercise: Surrogate Metrics\n\n\nFind ten more examples of surrogate metrics you encounter in daily life.\n\n\nSurrogate measurements are very often necessary, but they are not without issues:\n\nThey might not capture all factors influencing the attribute of interest.\nThey can lead to oversimplification and misinterpretation.\nThe surrogate might not measure what is most relevant. For example, standardized test scores do not measure critical thinking.\nThey can lead to surrogation.\n\n\nSurrogation\nSurrogation occurs when people or organizations substitute a metric for the underlying concept or goal that the metric is intended to represent. Instead of treating the metric as a proxy or tool to measure progress, they treat it as the ultimate goal itself. The metric has become the goal. Surrogates are good. Surrogation is bad.\nWhen standardized test scores are a surrogate for student achievement, surrogation means to focus on driving up the test scores rather than focusing on driving actual student achievement.\nSomeone can obsess over BMI (body mass index), trying to improve their BMI but not getting any healthier.\nSurrogation of net promoter scores occurs when companies focus on driving up NPS by only asking customers that repeatedly buy their product. The goal is to increase customer satisfaction and loyalty, the NPS should increase as a result. Surrogation focuses on the NPS, not the actual customer satisfaction. The first mistake in surrogation is to assume that the surrogate is exactly what it is a proxy for. To assume that the net promoter score is a proxy for customer satisfaction it is not customer satisfaction.\nSales people are often incentivized on number of deals or deal volume (revenue). The underlying goal is to drive success for the company. Surrogation can lead to closing of deals that are bad for the company (losing money on the deal) but work in favor of the sales person’s metric to hit a revenue target.\n\nWhen metrics are tied to performance evaluation, resource assignment, advancement, surrogation is common. It leads to distorted priorities and bad behavior (gaming the system): people try to meet the metric without achieving the underlying goal.\nAnother negative aspect of surrogation is to prioritize the simple fixes and quick wins instead of the deeper but more expensive and difficult corrections. A hospital might focus on amenities or quick ER service rather than improving health outcomes for its patients. A social media content creator generates click bait in order to increase clicks rather than focusing on meaningful engagement.\n\n\nExercise: Find Examples of Surrogation\n\n\nFind a few additional real life examples of surrogation, where chasing the metric has become more important than achieving the underlying goal the metric represents.\n\n\n\n\nReading Assignment: Chapter 3 in WMD\n\n\nRead Chapter 3, “Arms Race” in O’Neil (2017, 50–62).\n\nWhat is the goal and what is the (surrogate) metric in the U.S. News & World Report?\nHow did surrogation affect the response of universities to the rankings?\nDid the proxies chosen for educational excellence make sense?\nWhat is the effect of leaving tuition and fees out of the ranking model?\nWhat is the similarity of the model for educational excellence that underlies the college rankings and the regression models for aerobic fitness and the nail pull test?\n\n\n\n\n\n\n\nGrue, Lars, and Arvid Heiberg. 2006. “Notes on the History of Normality–Reflections on the Work of Quetelet and Galton.” Scandinavian Journal of Disability Research 8 (4): 232–46.\n\n\nHuff, Darrell. 1954. How to Lie with Statistics. W.W. Norton & Company, New York.\n\n\nLudwig, Eugene. 2025. “Voters Were Right about the Economy. The Data Was Wrong.” Politico. https://www.politico.com/news/magazine/2025/02/11/democrats-tricked-strong-economy-00203464.\n\n\nO’Neil, Cathy. 2017. Weapons of Math Destruction. How Big Data Increases Inequality and Threatens Democracy. Crown, New York.\n\n\nRubino, Francesco, David E Cummings, Robert H Eckel, Ricardo V Cohen, John P H Wilding, Wendy A Brown, Fatima Cody Stanford, et al. 2025. “Definition and Diagnostic Criteria of Clinical Obesity.” The Lancet Diabetes & Endocrinology. https://doi.org/10.1016/S2213-8587(24)00316-4.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quantitative Thinking (QT)</span>"
    ]
  },
  {
    "objectID": "cholera.html",
    "href": "cholera.html",
    "title": "3  Case Study: 1854 Cholera Outbreak in Soho, London",
    "section": "",
    "text": "3.1 Two Snows\nIn 1854, a severe outbreak of cholera occurred near Broad Street in Soho, London, killing over 600 people. The outbreak was studied by John Snow, considered one of the founders of modern epidemiology. No, not the Jon Snow you might be thinking of, Lord Commander of the Night Watch (Figure 3.1 (b)), but the John Snow in Figure 3.1 (a).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Case Study: 1854 Cholera Outbreak in Soho, London</span>"
    ]
  },
  {
    "objectID": "cholera.html#two-snows",
    "href": "cholera.html#two-snows",
    "title": "3  Case Study: 1854 Cholera Outbreak in Soho, London",
    "section": "",
    "text": "(a) John Snow. Source: Wikipedia\n\n\n\n\n\n\n\n\n\n\n\n(b) Jon Snow. Source: Wikipedia\n\n\n\n\n\n\n\nFigure 3.1: Famous John/Jon Snows",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Case Study: 1854 Cholera Outbreak in Soho, London</span>"
    ]
  },
  {
    "objectID": "cholera.html#sec-cholera-viz",
    "href": "cholera.html#sec-cholera-viz",
    "title": "3  Case Study: 1854 Cholera Outbreak in Soho, London",
    "section": "3.2 Visualization",
    "text": "3.2 Visualization\nFigure 3.2 shows the map drawn by John Snow, recording the number of cholera cases with stacked bars at the location where cholera cases occurred (click on the map to zoom in). Also shown on the map as black circles and annotated as “PUMP” are the public water pumps throughout the city. The high number of cholera cases on Broad Street stands out, and they seem to be clustered near the location of the Broad Street Pump (Snow 1855, 46).\n\n\n\n\n\n\nFigure 3.2: John Snow’s original cholera map from Soho, London in 1854.\n\n\n\nCholera had been a major problem in the city, thousands had died during previous outbreaks. The prevailing theories of the cause of cholera were (i), airborne particles, called miasma that rose from decomposing organic material and (ii), an as of yet unidentified germ. According to the miasma theory, cholera is contracted by breathing bad air. John Snow adhered to the germ theory and believed that it was transmitted through contaminated water.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Case Study: 1854 Cholera Outbreak in Soho, London</span>"
    ]
  },
  {
    "objectID": "cholera.html#sec-cholera-data",
    "href": "cholera.html#sec-cholera-data",
    "title": "3  Case Study: 1854 Cholera Outbreak in Soho, London",
    "section": "3.3 Data Validation",
    "text": "3.3 Data Validation\nTalking to residents, Snow identified the public water pump on Broad Street to be the source of the outbreak. He failed to identify the germ under the microscope but came to the conclusion based on the pattern in the data and conversations with residents. Investigating on the ground, he found that nearly all deaths were in the vicinity of the Broad Street Pump or by people who had consumed water from the pump (Snow 1855, 47):\n\nIt will be observed that the deaths either very much diminished, or ceased altogether, at every point where it becomes decidedly nearer to send to another pump than to the one in Broad street. It may also be noticed that the deaths are most numerous near to the pump where the water could be more readily obtained.\n\nHe persuaded local authorities to remove the handle from the pump to prevent access to the water. The mortality rate declined after that, but it is believed that the outbreak was already in decline as people had fled the area.\n\n\n\n\n\n\nRemoving the handle\n\n\n\n“Removing the handle” is now a term in epidemiology for the removal of a harmful agent from the environment. When epidemiologists look for simple answers to questions about epidemics, they ask “Where is the handle to this pump?” (Adhikari, DeNero, and Wagner 2022).\n\n\nThere were some data points (outliers?) that did not agree with the hypothesis that proximity to the Broad Street pump resulted in more cholera incidences. At the intersection of Broad Street and New Street was the Lion Brewery; there were no cholera cases at the brewery although it used water from the Broad Street pump. It turns out that the workers there were protected from cholera by virtue of a daily beer allowance. The cholera bacteria is killed in the brewing process making the beer safe to drink. Drinking beer instead of the contaminated water saved the workers from cholera. What appears as an outlier to the model actually reinforces it.\nAdhikari, DeNero, and Wagner (2022) discuss other data points that appeared initially as anomalies and ended up implicating the public pump on Broad Street:\n\nThere were deaths in houses closer to the Rupert Street pump than the Broad Street pump. It was more convenient for those residents to use the Broad Street pump due to the street layout.\nDeaths in houses several blocks away from the Broad Street pump were linked to children who drank from the Broad Street pump on their way to school.\nJohn Snow was initially puzzled by two isolated deaths in the Hampstead area, far from Soho. He learned that the deceased had once lived in Broad Street. Because they liked the taste, they had water from the Broad Street pump delivered to Hampstead every day.\n\nThe well accessed by the Broad Street pump was contaminated with fecal bacteria that leaked into the well from a nearby cesspit. Sewage from the house of a cholera victim had contaminated the well.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Case Study: 1854 Cholera Outbreak in Soho, London</span>"
    ]
  },
  {
    "objectID": "cholera.html#sec-cholera-deeper",
    "href": "cholera.html#sec-cholera-deeper",
    "title": "3  Case Study: 1854 Cholera Outbreak in Soho, London",
    "section": "3.4 Toward Causality",
    "text": "3.4 Toward Causality\n\nBeyond a Reasonable Doubt?\nThe evidence that the contaminated well water at the Broad Street pump caused the high rate of cholera in that neighborhood and among those who consumed the water is strong. Is it conclusive, however? Have we ruled out any other explanation beyond a reasonable doubt?\nThere could be other explanations for the higher cholera incidence rate in the Broad Street neighborhood compared to other areas of London. Maybe the diet is different among the residents of that poorer area. Maybe their occupations expose them to harmful agents at work. Maybe there is something different in the way their houses were constructed.\nWhile we know today that the bacterium Vibrio cholerae causes cholera, that discovery was not made until 1883 and John Snow had failed to identify a “germ” when he studied the Broad Street pump water. The prevailing miasma theory of infection from airborne particles also did not support Snow’s findings. While his data, visualization, and analysis showed a strong association between cholera and proximity to the Broad Street pump, a deeper analysis was necessary to convince his contemporaries.\nTo establish cause and effect and prove that a variable causes an outcome, modern science would design and run an experiment, provided it is ethically and technically possible. In such an experiment one would control for all other factors except the one hypothesized to cause the outcome. One method of controlling these confounding factors is by randomly assigning the conditions of interest to people and to observe what happens. Exposing folks deliberately to contaminated water that could harm or even kill them is not justified. Fortunately, John Snow found a real-life experiment with perfect conditions to establish cause and effect between cholera and water contamination.\n\n\nJohn Snow’s “Experiment”\nHe studied the cholera incidences among recipients of water from two water supply companies. The Lambeth company used water from the River Thames drawn upriver from sewage discharge and the Southwark & Vauxhall company drew water below the discharge. Snow also established that for all intents and purposes the households receiving water from either company were indistinguishable; in statistical terms they were comparable. The only thing that differentiated the two groups was the water supplier. Snow (1855, 75) wrote\n\nIn many cases a single house has a supply different from that on either side. Each company supplies both rich and poor, both large houses and small; there is no difference in the condition or occupation of the persons receiving the water of the different companies…As there is no difference whatever either in the houses or the people receiving the supply of the two Water Companies, or in any of the physical conditions with which they are surrounded, it is obvious that no experiment could have been devised which would more thoroughly test the effect of water supply on the progress of Cholera than this, which circumstances placed ready made before the observer.\n\nTable 3.1 is based on Snow (1855, 80) and covers the period from January 1 to December 12, 1853. The cholera death rate on a per 10,000 house basis is almost 14 times higher in households supplied by Southwark & Vauxhall compared to those who received their water from Lambeth.\n\n\n\nTable 3.1: Cholera incidences and rates for two water supply companies leading up to the 1854 outbreak.\n\n\n\n\n\n\n\n\n\n\n\nWater Supplier\nNo. of Houses\nCholera Deaths\nDeaths per 10,000 Houses\n\n\n\n\nSouthwark & Vauxhall\n40,046\n286\n71\n\n\nLambeth\n26,107\n14\n5\n\n\n\n\n\n\nIn all of London there were 563 deaths from cholera in the same period. In other words, 50% of the deaths took place among customers of the Southwark & Vauxhall company. Ouch.\nFor the first seven week period of the 1854 outbreak, Snow (1855, 86) recorded the death rates in Table 3.2\n\n\n\nTable 3.2: Cholera incidences and rates during the first seven weeks of the outbreak. The death rate in the rest of London was reported as 59 in Table IX of Snow (1855), but calculates to 55 deaths per 10,000.\n\n\n\n\n\n\n\n\n\n\n\nWater Supplier\nNo. of Houses\nCholera Deaths\nDeaths per 10,000 Houses\n\n\n\n\nSouthwark & Vauxhall\n40,046\n1,263\n315\n\n\nLambeth\n26,107\n98\n37\n\n\nRest of London\n256,423\n1,422\n\\(55^*\\)\n\n\n\n\n\n\nIn statistical terms, such a difference in the death rates is highly significant, meaning that if there is no difference in the water quality between the suppliers, such a discrepancy would virtually never happen. The only reasonable explanation for the higher death rate, since differences between the groups receiving the water have been ruled out, is the quality (contamination) of the Southwark & Vauxhall water.\n\n\n\nFigure 3.2: John Snow’s original cholera map from Soho, London in 1854.\n\n\n\nAdhikari, Ani, John DeNero, and David Wagner. 2022. Computational and Inferential Thinking: The Foundations of Data Science. 2nd Ed. https://inferentialthinking.com/chapters/intro.html.\n\n\nSnow, John. 1855. On the Mode of Communication of Cholera, 2nd. Ed. John Churchill, London. https://archive.org/stream/b28985266#page/n3/mode/2up.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Case Study: 1854 Cholera Outbreak in Soho, London</span>"
    ]
  },
  {
    "objectID": "datascience.html",
    "href": "datascience.html",
    "title": "4  Data Science—CT + QT",
    "section": "",
    "text": "4.1 Introduction\nComputational thinking is a problem-solving methodology that breaks down complex problems in manageable parts, finds relationships and patterns among the parts, and delivers solutions as repeatable steps—as algorithms. Quantitative thinking is a problem-solving technique that represents things in measurable quantities, enabling us to manipulate concepts mathematically. Quantitative thinking turns concepts into data.\nCT and QT are used in many disciplines, CT is not just for computer scientists and quantification is not the concern of only mathematicians or statisticians. The two methodologies fit together like hand in glove in the modern discipline of data science.\nThe origins of data science trace back to statistics, the study of information in the presence of uncertainty, computer science, and mathematics. Drawing on these foundational disciplines, data science aims at solving real-world problems using data.\nThe solutions produced by data science involve reports, dashboards, visualizations, algorithms and software. Of particular importance in data science application is using statistical and machine learning techniques to train algorithms on data sets to find patterns and relationships and to generalize these patterns into prediction machines. This effort is referred to as modeling in data science and the algorithms themselves are the data science models.\nYou recognize in this description of data science the elements of computational and quantitative thinking: data, patterns and relationships, generalization, algorithm development.\nA data science investigation starts with a problem: a business, policy, or research question:\nThe real-world problem is translated by the data scientist into one or more analytical categories:\nThe question before the FDA falls in the hypothesis testing category. Based on the data from a clinical trial, can we reject the hypothesis that the new and current drug have the same level of side effects? Answering the question of the retail company involves a combination of description, clustering, and hypothesis testing. The technical support provider will employ a predictive model that links attributes of the support tickets and the customer to the likelihood of response to the survey request. The marketing team might be looking toward a generative AI solutions to produce text descriptions from images of products. The city uses clustering to segment residents into groups that are similar with respect to water usage profiles and develops a separate recommendation model for each group. The grocery store uses association analysis of past purchase records to determine which items are likely to be purchased together. The trucking company uses a combination of predictive models that forecast the likelihood of parts failing and optimization to get a truck to a repair shop with minimal disruption of the route.\nA fundamental aspect of these investigation is the development of a data science model that draws on the essential patterns and relationships in the data to describe, predict, classify, test, recommend, cluster, associate, optimize, and generate.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Science---CT + QT</span>"
    ]
  },
  {
    "objectID": "datascience.html#introduction",
    "href": "datascience.html#introduction",
    "title": "4  Data Science—CT + QT",
    "section": "",
    "text": "Definition: Data Science\n\n\nAt the intersection of the foundation disciplines statistics, mathematics, and computer science, performing data science means drawing conclusions from data about real-world problems using computation and automation in the presence of uncertainty.\n\n\n\n\n\n\nA retail company might wonder whether there is a difference between customers visiting its bricks-and-mortar stores and online shoppers.\nA technical support provider might be interested to determine the drivers behind response/non-response to surveys after tickets have been resolved.\nA city wants to make personalized recommendations to residents on how to reduce water consumption.\nA marketing team is looking for ways to automate the creation of product descriptions.\nA panel at the Food and Drug Administration (FDA) examines whether a new drug shows significantly more side effects than a currently available drug.\nThe operator of a grocery store is wondering whether revenue can be increased by placing certain items closer together on shelves.\nA trucking company wants to move from scheduled maintenance of its fleet to predictive maintenance by equipping trucks with sensors that inform the company when a vehicle requires maintenance or fixing.\n\n\n\nAnalytic categories.\n\n\nCategory\nQuestion asked\n\n\n\n\nDescription\nWhat is and what has been?\n\n\nPrediction\nWhat will be?\n\n\nClassification\nWhat category does this item belong to?\n\n\nHypothesis Testing\nWhat can I say about X?\n\n\nPrescription\nWhat should I do?\n\n\nClustering\nWhich things are similar?\n\n\nAssociation\nWhich things occur together?\n\n\nOptimization\nWhat is the best way to do something?\n\n\nGeneration\nWhat novel content is there?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Science---CT + QT</span>"
    ]
  },
  {
    "objectID": "datascience.html#data-science-models",
    "href": "datascience.html#data-science-models",
    "title": "4  Data Science—CT + QT",
    "section": "4.2 Data Science Models",
    "text": "4.2 Data Science Models\nFrom the 30,000 foot view a model is simply a mechanism to process some input and produce a corresponding output (Figure 4.1).\n\n\n\n\n\n\nFigure 4.1: A simple representation of a model that processes inputs with algorithmic logic and produces output.\n\n\n\nThe input to drive the model algorithm is almost always some form of data. The algorithm that processes the inputs can be based on data, but that is not necessarily so. Suppose the problem we are trying to solve is to assess someone’s annual federal income tax. The problem is solved with a model that takes as input the individuals financial situation. This information is typically known without error as information about income, property taxes, expenses, etc. is well documented. The algorithm processing this input is a translation of the relevant information in the federal income tax laws into machine instructions. The output is the amount of money owed to the government or expected as a refund.\nNow suppose that for some reason the input data in the tax problem is not known without error. For example, income from tips, medical expenses or charitable contributions might be best guesses rather than exact amounts. Income data could be noisy because foreign income is converted at fluctuating exchange rates. If the input data is the realization of stochastic (random) influences, should we modify the algorithm?\nWhen the input data to an algorithm is the result of observing random variation, we are looking to the algorithm of the model to find the signal in the data and to separate it from the noise. The signal located in the data is then transformed into the model output. Most models built in data science are of this kind because data is inherently noisy. The reasons for the random variations are many: selecting observation from a larger population at random, applying treatments to randomly chosen experimental units, variations in measurement instruments and procedures, variations in the environment in which a phenomenon is observed, and so on. The specific algorithms data scientists use depend on the analysis category, properties of the data, assumptions one is willing to make, attributes we look for in competitive models, and personal preferences.\n\nSignal and Noise\nThe signal represents the systematic, non-random effects in the data. Data scientists and statisticians define the noise as the unpredictable randomness around the signal. A slightly different, and also useful, definition of noise stems from intelligence analysis. The signal is the information we are trying to find, the noise is the cacophony of other information that obscures the signal. That information might well be a signal for something else but it is irrelevant or useless for the event the intelligence analyst is trying to predict.\nInformation not being relevant for the signal we are trying to find is the key. In the view of the data scientist, that information is due to random events.\nFinding the signal is not trivial, different analysts can arrive at different models to capture it. Signals can be obscured by noise. What appears to be a signal might just be random noise that we mistake for a systematic effect.\n\n\nExample: Theophylline Concentration\n\n\nFigure 4.2 shows the concentration of the drug theophylline over 24 hours after administration of the drug in two groups of patients. There are 98 data points of theophylline concentration and measurement time. What are the signals in the data? What is noise?\n\n\n\n\n\n\n\n\nFigure 4.2: Theophylline concentration over time in two groups of patients.\n\n\n\n\n\nThe first observation is that the data points are not all the same over time, otherwise they would fall on a horizontal straight line: there is variability in the data. Separating signal and noise means attributing this variability to different sources: some systematic, some random.\nFocusing on either the open circles (group 1) or the triangles (group 2), you notice that points that are close in time are not necessarily close in the concentration measurement. Not all patients were measured at exactly the same time points, but at very similar time points. For example, concentrations were measured after about 7, 9, and 12 hours. The differences in the concentration measurements among the patients receiving the same dosage might be due to patient-to-patient variability or measurement error.\nFocusing on the general patterns of open circles and triangles, it seems that the triangles appear on average below the average circle a few hours after administration. Absorption and elimination of theophylline appears to behave differently in the two groups.\nMuch of the variability in the data seems to be a function of time. Shortly after administering the drug the concentration rises, reaches a maximum level and then declines as the drug is eliminated from the body. Note that this sentence describes a general overall trend in the data here.\nWhich of these sources of variability are systematic—the signals in the data— and which are random noise?\n\nPatient-to-patient variability within a group at the same time of measurement: we attribute this to random differences among the participants.\nPossible measurement errors in determining the concentrations: random noise\nOverall trend of drug concentration over time: signal\nDifferences among the groups: signal\n\nThese assignments to signal and noise can be argued. For example, we might want to test the very hypothesis that there are no group-to-group differences. If that hypothesis is true, any differences between the groups we discern in Figure 4.2 would be due to chance; random noise in other words.\nThe variability between patients could be due to factors such as age, gender, medical condition, etc. We do not have any data about these attributes. By treating these influences as noise, we are making important assumptions that their effects are irrelevant for conclusions derived from the data. Suppose that the groups refer to smokers and non-smokers but also that group 1 consists of mostly men and group 2 consists of mostly women. If we find differences in theophylline concentration over time among the groups, we could not attribute those to either smoking status or gender.\n\n\nFinding the signal in noisy data is not trivial. The opposite can also be difficult: not mistaking noise for a signal. Figure 4.3 is taken from Silver (2012, 341) and displays six “trends”. Four of them are simple random walks, the result of pure randomness. Two panels show the movement of the Dow Jones Industrial Average (DJIA) during the first 1,000 trading days of the 1970s and 1980s. Which of the panels are showing the DJIA and which are random noise?\n\n\n\n\n\n\nFigure 4.3: Figure 11-4 from Silver (2012), Random walk or stock market data?\n\n\n\nWhat do we learn from this?\n\nEven purely random data can appear non-random over shorter sequences. We can easily fall into the trap of seeing a pattern (a signal) where there is none. Sometimes there is no signal at all. After drawing two unlikely poker hands in a row there is not a greater chance of a third unlikely hand unless there is some systematic effect (cards not properly shuffled, game rigged). Our brains ignore that fact and believe that we are more lucky than is expected by chance.\nData that contains clear long-run signals—the stock market value is increasing over time—can appear quite random on shorter sequences. One a day to day basis predicting whether the market goes up or down is very difficult. In the long run ups and downs are almost equally likely. Upswings have a slight upper hand and on average are greater than the downswings, increasing the overall value in the long term. Traders who try to beat the market over the short run have their work cut out for them.\n\nBy the way, panels D and F in Figure 4.3 are from actual stock market data. Panels A, B, C, and E are pure random walks. It would not be surprising if investors would bet money on “trend” C.\n\n\nExercise: Southern Oscillation Index (SOI)\n\n\nThe Southern Oscillation Index (SOI) is a standardized index based on the observed sea level pressure differences between Tahiti and Darwin, Australia. The SOI is one measure of the large-scale fluctuations in air pressure occurring between the western and eastern tropical Pacific (i.e., the state of the Southern Oscillation) during El Niño and La Niña episodes.\nIn general, smoothed time series of the SOI correlate highly with changes in ocean temperatures across the eastern tropical Pacific. The negative phase of the SOI represents below-normal air pressure at Tahiti and above-normal air pressure at Darwin. Prolonged periods of negative (positive) SOI values coincide with abnormally warm (cold) ocean waters across the eastern tropical Pacific typical of El Niño (La Niña) episodes (Figure 4.4).\nAccording to Wikipedia, there have been about 30 El Niño episodes since 1950 with strong El Niño events in 1982–83, 1997–98, and 2014–16. You recognize El Niño when the SOI dips negative for a period of time. La Niña is marked by periods of positive SOI values.\n\n\n\n\n\n\n\n\nFigure 4.4: Monthly SOI data from 1951 to mid-2023 according to NOAA.\n\n\n\n\n\n\nWhat is the signal and the noise in these data?\nIs it possible that there are multiple signals in these data, associated with different time horizons?\n\n\n\n\n\nChoosing a Model\nSelecting the right type of model is a critical step in any data science project. There are many choices based on input data, learning methodology and analysis category. Figure 4.5 is an attempt at structuring the input, algorithm, and output components of a model in the data science context. The diagram is complex and yet woefully incomplete and is intended to give you an idea of the diversity of methods and the many ways we can look at things. For example, in discussing input data we could highlight how data are stored, how fast it is moving, the degree to which the data is structured, the data types, and so forth. There are many other categorizations of data one could have listed.\nThe categorization of algorithms leaves out some approaches to learning from data to keep things (relatively) simple. Volumes of books and papers have been written about every item in the list of algorithms and many algorithms are represented by a single entry. Multi-layer networks, for example, include artificial neural networks, deep networks such as convolutional and recurrent networks, and transformer architectures such as GPT.\n\n\n\n\n\n\nFigure 4.5: Structuring and categorizing input, algorithm, and output in data science models.\n\n\n\n\n\nSupervised Learning\nArguably the most important class of algorithms learning from data is supervised learning. The name originates in thinking of learning in an environment that is supervised by a teacher. The teacher asks questions for which they know the correct answer (the ground truth) and judge a student’s response to the questions. The goal is to increase students’ knowledge as measured by the quality of their answers. But we do not want students to just memorize answers, we want to teach them to be problem solvers, to apply the knowledge to new problems, to generalize.\nSupervised learning is characterized by the presence of a target variable, also called a dependent variable, response variable, or output variable. This is the attribute about which we wish to draw conclusions. For example, to predict the probability that someone contacting customer support will later fill out a survey, the target variable is whether they responded to the survey request or not. To model the strength of drywall a target variable might be the force required to pull a nail from the board.\nOther variables in our data set, beside the target variable, are potentially input variables to our model. In modeling the nail pull strength of drywall input variables might be the thickness of the board, the moisture, the chemistry of the gypsum, characteristics of the manufacturing environment, and so forth.\nThe values of the target variable are also called the labels in machine learning. This name stems from image analysis where prior to training an algorithm human interpreters go through the images in the training data and label them (Figure 4.6).\n\n\n\n\n\n\nFigure 4.6: Labeling images prior to training an image processing network. Source.\n\n\n\nThis process establishes the ground truth, the correct answer which the algorithm associates with the other variables in the data during training. Because the algorithm makes the connection between input variables and target variable at this stage, we can later apply the trained algorithm to new observations for which the label (the true value) is unknown.\nSupervised learning can have many goals. For example:\n\nPredict the target variable from input variables.\nDevelop a function that approximates the underlying relationship between inputs and outputs.\nUnderstand the relationship between inputs and outputs.\nClassify observations into categories of the target variable based on the input variables.\nGroup the observations into sets of similar data based on the values of the target variable and based on values of the inputs.\nReduce the dimensionality of the problem by transforming target and inputs from a high-dimensional to a lower-dimensional space.\nTest hypotheses about the target variable.\n\nData science projects can have multiple of these goals. For example, you might be interested in understanding the relationship between target and input variables and use that relationship for predictions.\nTo close the loop, we can now map a teacher teaching a classroom of students to the problem of building a machine learning model through supervised learning.\n\nThe problems asked by the teacher, the learning algorithm, are the data points.\nThe values of the target variable are the the correct answers.\nThe input variables represent the information used by the students to answer the questions.\n\n\nRegression and classification\nWithin the supervised learning category regression and classification models are the most important model types.\nIn a regression context we are interested in the relationship between the mean of a target variable and the inputs. The goal is to make predictions about the mean of the target variable when the input variables take on certain values.\nIn a classification context the target variable is categorical (see Section 2.4), for example, makes of cars, names of fruit, object categories on images, or the 10,000 words in a dictionary. Based on the values of input variables we wish to classify a new observation into one of the possible categories. For example, we measure health attributes such as LDL, weight, age, etc. along with the target variable, whether someone is at low, medium, or high risk of developing coronary heart disease. The process of training the model involves data from people with confirmed low, medium, and high risk of heart disease. Based on the classification model so developed, a physician can classify a new patient as low, medium, or high risk based on information about their LDL, weight, age, etc.\nThe term prediction is common to describe the outcome of applying a regression model or a classification model. In the former case we make a prediction of a numeric attribute. In the latter we predict which of the possible categories an observation belongs to.\nThe process of prediction is fundamental to both types of applications. Consider you are presented with a handwritten digit and you have trained a classification model in digit detection. The algorithm does not actually say which of the ten digits it was presented with. The algorithm computes a vector of 10 quantities each between 0 and 1, and subject to the constraint that they sum to 1.\nHere is the vector for one observation \\[\n[0.00000, 0.00001, 0.00001, 0.00003, 0.00000, 0.00000, 0.00000, 0.99994, 0.00000, 0.00001]\n\\]\nand here is the vector for another observation\n\\[\n[0.00056, 0.00002, 0.00000, 0.00001, 0.00424, 0.18760, 0.74695, 0.00001, 0.00616, 0.05446]\n\\]\nThe first element of the vector corresponds to digit 0, the second to digit 1, and so on. Considering the first vector, how would you convert the numbers into a classification? Since the numbers are between 0 and 1, and sum to 1, it is tempting to interpret them as probabilities. The overwhelming evidence points at the 8th position in the vector with a large probability of 0.99994. The algorithm is almost certain that the digit is a “7”.\nIn the second case we see more of a spread in the probabilities. Digit “5” is considered quite likely (probability 0.1876). Digit “6” has the highest probability (0.74695) and “9” also has a non-zero probability (0.05446). The rule that classifies an observation into the category with the highest probability would assign the label “6” to the second observation.\n\n\n\n\n\n\nFigure 4.7: Digit classifications.\n\n\n\nFigure 4.7 shows the actual and classified digits for nine data points. The vectors shown above correspond to the probabilities for the images in the upper left and lower right corners. We are not surprised that the algorithm classified the first observation as a “7”. We are also not surprised that the sloppy “5” in the lower right corner was mistaken for a “6” (which had the second largest probability).\nThe point of this example is twofold:\n\nThe process of classifying something through a statistical algorithm often goes first through a process of predicting a measure of confidence or likelihood associated with the possible outcomes. We then classify the observation into the category that has the highest probability. This is known as the Bayes Rule of classification and it can be shown to be optimal in the sense of achieving the greatest accuracy.\nWhen making predictions we need to think in terms of probabilities and uncertainties. Ultimately, the classification model will spit out one category and we interpret this as “the model says the digit is a 7”. In the case of the first observation, there is not much uncertainty in the prediction, as the model deems other choices very unlikely. In the second case we should appreciate that the algorithm decided on a “5”, but also that there is uncertainty in the prediction. We only know that the algorithm got it right because we know the author of the digit was writing a “5” (the label). The algorithm will get a certain proportion of cases wrong, this is known as the mis-classification rate. As Silver (2012) puts it,\n\n\nOur brains, wired to detect patterns, are always looking for a signal,\nwhen instead we should appreciate how noisy the data is.\n\n\n\nPredictions as surrogates\nIn Section 2.6 we introduced the concept of a surrogate or proxy variable that takes the place of another variable that is difficult to measure.\nThe predictions of a model can be viewed as surrogates for the attribute that is being modeled. If you build a model for the age of abalone molluscs based on characteristics such as the size, weight, and gender of the animal, then you can apply the model when you find a new abalone. Based on its siz. weight, and gender you can then predict its age. The predicted value is the surrogate for the unobserved attribute age.\nThis notion of predictions as surrogates for unobserved values is important for attributes that are difficult or expensive to observe or when the method of observation is destructive. Here are examples of prediction of a difficult to measure attribute and of a destructive attribute.\n\n\nExample: Measuring Fitness\n\n\nOne method of quantifying the aerobic fitness of athletes is through testing their aerobic capacity under stress (Figure 4.8). These measurements are time consuming and expensive.\n\n\n\n\n\n\nFigure 4.8: Measuring aerobic capacity.\n\n\n\nImagine that you conduct a study in which a number of athletes are subjected to aerobic capacity tests on a machine similar to that shown in Figure 4.8. In addition to measuring their aerobic capacity, you also measure simple-to-obtain attributes such as heart rate, rest pulse, run pulse, distance run, etc.\nNext you develop a model that can predict aerobic capacity from the easy-to-measure attributes. If that model works well, you can predict the aerobic capacity of an athlete by obtaining their heart rate, pulse, rest pulse, etc. The prediction from that model is a surrogate for the unobserved aerobic fitness measurement.\nThis is an example of a regression model.\nWe incur the expense of measuring the difficult attribute (aerobic fitness) for the study participants only. The model derived from that data can be applied to athletes who did not participate in the study. The prediction from the model provides a quick-and-cheap surrogate value in lieu of going through the trouble of putting every athlete through a fitness stress testing procedure.\n\n\n\n\nExample: Nail Pull Test\n\n\nThe force required to pull a nail out of a gypsum board (dry wall) is a measure of the board quality. Unfortunately, the test is destructive and the tested board cannot be used. To circumvent the problem and have more boards to sell, a regression model can be built to predict the nail pull strength from other attributes such as the dry wall material, the thickness of the wall, the method of construction, humidity, temperature, etc. The attributes and the nail pull strength are determined for a random sample of boards. The predictions of the regression model can be applied to other boards.\nThese attributes are easy to measure and non-destructive. When the regression model works well—that is, it explains a sufficient degree of variation in nail pull strengths across gypsum boards—the prediction from the model can be used as surrogate for the actual board nail pull strength without destroying the board.\n\n\n\n\nGeneralization\nOur teacher has the noble goal of not just drilling the correct answers but to turn the students into problem solvers who can apply concepts to solve new questions. What is the parallel of this concept in supervised machine learning?\nSuppose we can quantify the quality of a student’s answer, for example by using a metric that increases the further their answer is of the mark. When an answer is correct, the metric returns 0. When should the teacher stop teaching? When all the students can answer all the questions correctly? At this point our metric across the training questions would be zero. The teacher has done a good job training students on that set of questions, but have they learned to solve new, previously unseen, problems?\nWe need to throw in some new problems and measure how well the students do on those. Getting great answers (small discrepancy metric) on new questions shows that the students can apply the learned concepts to new problems. They are able to generalize. Recall from Chapter 1 that generalization is one of the steps in computational thinking, just prior to algorithm design. In data science, the process of training a model leads to an algorithm that can perform the analytic goal. The way in which the model is trained ensures that the algorithm generalizes well to new observations, those not in the training data.\nJust like the teacher is not interested in drilling the correct answers to the questions in the lesson plan, we are not interested in building models that follow the training data too closely. Models that suffer from that problem are overfitting the data and do not generalize well. They treat too much of the noise as a worthwhile signal.\n\n\nExample: Melanoma Incidences\n\n\nThe data displayed in Figure 4.9 are from the Connecticut Tumor Registry and represent age-adjusted numbers of melanoma incidences per 100,000 people for the 37 years from 1936 to 1972 (Houghton, Flannery, and Viola 1980).\n\n\n\n\n\n\n\n\nFigure 4.9: Incidences of melanoma per 100,000 people from 1936 to 1972\n\n\n\n\n\nThere is noise and signal in the data. While the data points do not fall on a perfect trend line, there is a general upward trend in the number of melanoma incidences per 100,000 over the 37-year period. There is also a shorter-term oscillation around the overall trend.\nFigure 4.10 shows the data and four possible models to capture the signal in the data. The solid black line tries to capture the overall trend. While this model might be OK to describe the trend over 30+ years, it is underfit with respect to the shorter oscillations of the incidence rate. The dotted (red), dashed (blue), and dot-dashed (dark green) lines try to capture these shorter-term signals in the data. The green line appears overfit, it follows the observed data too closely, almost interpolating the dots. The other two lines fall somewhere between the two extreme models.\n\n\n\n\n\n\n\n\nFigure 4.10: Models of different degree of smoothness for the Melanoma data.\n\n\n\n\n\nDetermining the appropriate amount of signal extraction, balancing overfitting and underfitting, is an important aspect of all data science modeling projects. An underfit model does not generalize well because it is too simple. An overfit model does not generalize well because it is too sensitive to unseen observations. Striking the balance between over- and under-fit is also known as the bias-variance tradeoff in data science.\n\n\n\n\n\nAll Models are wrong\nGeorge E.P. Box is credited with coining the much-used phrase “all models are wrong, but some are useful”. The phrase appears partially (“all models are wrong”) twice in his 1976 paper on Science and Statistics (Box 1976):\n\nSince all models are wrong the scientist cannot obtain a “correct” one by excessive elaboration.\nSince all models are wrong the scientist must be alert to what is importantly wrong.\n\nThe full phrase appears on p. 424 of his book with Norman Draper (Box and Draper 1987).\nThe first G.E.P. Box quote instructs us not to overdo it in building models; this translates to the problem of overfitting, crafting a model that follows the training data too closely and as a result does not generalize well to new data points. If the goal is to predict, classify, or cluster the unseen; generalizability of the model is key. A model to forecast stock prices or trading volumes is judged by how well it can predict the future, not by how well it can predict the past. The adequate level of generalization for that model must be wrung from current and past stock prices.\nThe second G.E.P. Box quote instructs us that models are abstracting away features of the phenomenon. If these are important features, the model is not useful. In the best case this model does not meet its goal and is revised or abandoned. In the worst case the model leads to bad decisions and harmful outcomes.\nThe important lesson is that any model is an abstraction of a phenomenon and we strive to find a useful abstraction. The model does not attempt to reproduce the phenomenon. The tax algorithm converts the essence of the tax code into machine instructions, it is not an electronic copy of the entire law. The purpose is to accurately calculate an entity’s tax, anything else can be stripped away. An algorithm processing noisy data that reproduces the data is uninteresting. The goal is to abstract the data in such a way to allow separating the signal from the noise and to convert the signal into the desired output.\nThe assumptions we make in building models are very important. Violations of the assumptions can lead to biased conclusions when the model does not represent the phenomenon it is supposed to abstract.\n\n\nAssignment: Compartmental Models in Epidemiology\n\n\nThe SIR (Susceptible–Infectious–Recovered) model is a standard class of compartmental models in epidemiology, describing how an infectious disease moves through a population. On the surface this makes sense: at each point in time an individual is in one of three states, called compartments:\n\nYou have not had the disease but you are susceptible to it (S compartment)\nYou are infected by the disease (I compartment)\nYou are recovered from the disease, or dead (R compartment)\n\nA vaccination, then, is a shortcut that moves an individual directly from the S to the R compartment, bypassing the infected state.\nHowever, there are a number of assumptions in the SIR model that can invalidate the model for some diseases and circumstances:\n\nThe disease progresses in only one direction, from S to I to R.\nEveryone is equally susceptible and behaves the same way.\nEveryone is equally likely to be vaccinated, if a vaccine is available.\nAll members of the populations intermingle at random.\n\nThe rate at which the disease spreads through the population is measured by the basic reproduction number, \\(R_0\\). This number, which we became all too familiar with during the COVID-19 pandemic, measures the number of uninfected people expected to catch the disease from an infected individual. An \\(R_0\\) of 3 means that someone who contracts the disease is expected to pass it on to three other individuals. In the absence of vaccines or quarantines, any disease with \\(R_0 &gt; 1\\) will eventually spread to the entire population.\n\nDiscuss whether the assumptions of the SIR model apply to the COVID-19 pandemic.\nWhat could be reasons why epidemiologists hold on to a SIR model even if one or more of its assumptions are violated?\nIn some countries COVID intervention measures were directly related to \\(R_0\\), often called the R-number. If a three-day average of \\(R_0\\) was above a certain threshold more stringent COVID restrictions took effect. If the three-day average \\(R_0\\) dropped below a threshold restrictions were relaxed. \\(R_0\\) is calculated either retrospectively from epidemiological data (such as data from contact tracing) or using theoretical mathematical models based on differential equations such as the SIR model. Discuss the pros and cons of the two approaches.\nIs \\(R_0\\) a biological constant of a pathogen, or is it a function of human behavior and characteristics of the pathogen?\n\n\n\n\nModels that explicitly incorporate uncertainty are called stochastic models and play an important role in analyzing data. Partly because data are inherently variable and noisy. Partly because describing something in stochastic terms—letting random elements account for some of the variability we see—enables us to model highly complex phenomena in relatively simple terms. The example of modeling coin flips will make the concept clearer.\n\n\nExercise: Modeling Coin Flips\n\n\nSuppose we want to predict whether a coin, when flipped, lands on Heads or Tails. We can try and develop a mathematical model that captures the forces acting on the coin: its starting position, the angle and momentum when it is released, conditions of the environment such as wind and temperature (does that matter?), conditions of the landing surface (angle, softness, etc.), conditions of the coin (surface, weight, material, shape, damage, etc.) and so on and so on. It will turn into a very complicated model with many sub-models. We will have to make assumptions, for example, that wind speed and direction are constant, that the coin is perfectly round and flat.\nWe end up with a very complex model, full of individual abstractions. Whether we will be able to perfectly predict whether the coin lands on Heads or Tails depends on whether we model the sub-processes correctly, whether we combine them correctly into an overall model, and whether the assumptions are met.\nA much simpler model would be to not try and predict any particular flip of the coin and to acknowledge that the forces that determine how a coin lands are essentially random (stochastic). The stochastic model for the coin flip is extremely simple. Every flip is assumed to be the realization of a random experiment with two possible outcomes: Heads and Tails. Our model rules out that the coin lands on each side. If the coin lands at an angle we call the side that faces up. The only parameter of the random experiment is the probability that the coin lands on Heads (or Tails). If the coin is fair and is properly tossed, that probability should be 0.5.\nWe can even run an experiment with a particular coin and flip it 100 times to get an estimate of the parameter. If it lands on Heads 45 times out of 100 tosses, then the probability of Heads is 0.45.\n\n\n\n\n\nUnsupervised Learning\nUnsupervised learning does not utilize a target variable; hence it cannot predict or classify observations. However, we are still interested in discovering structure, patterns, and relationships in the data.\nThe term unsupervised refers to the fact that we no longer know the ground truth because there is no target variable. Hence the concept of a teacher who knows the correct answers and supervises the learning progress of the student does not apply. In unsupervised learning there are no clear error metrics by which to judge the quality of an analysis, which explains the proliferation of unsupervised methods and the reliance on heuristics. For example, a 5-means cluster analysis will find five groups of observations in the data, whether this is the correct number or not, and it is up to us to interpret what differentiates the groups and to assign group labels.\nOften, unsupervised learning is used in an exploratory fashion, improving our understanding of the joint distributional properties of the data and the relationships in the data. The findings then help lead us toward supervised approaches.\nA coarse categorization of unsupervised learning techniques also hints at their application:\n\nAssociation analysis: which values of the variables tend to occur together in the data? An application is market basket analysis, where the data represent items in a shopping cart (or a basket in the market). If items frequently appear together, bread and butter, or beer and chips, for example, then maybe they should be located close together in the store. Association analysis is also useful to build recommender systems: shoppers who bought this item also bought the following items \\(\\ldots\\)\nCluster analysis: can variables be used to grouped data into sets such that the observations within a set are more similar to each other than they are to observations in other sets? An application of clustering is the grouping of customers into segments. Segmentation analysis is behind loyalty programs, lower APRs for customers with good credit rating, and churn models.\nDimension reduction: can we transform the input variables into a smaller set of data without losing relevant information? Applications of dimension reduction are in high-dimensional problems where the number of inputs is large relative to the number of observations. In problems with wide data, the number of inputs \\(p\\) can be much larger than the number of observations, which eliminates many traditional methods of analysis from consideration.\n\n\n\n\nFigure 4.5: Structuring and categorizing input, algorithm, and output in data science models.\nFigure 4.7: Digit classifications.\n\n\n\nBox, George E. P. 1976. “Science and Statistics.” Journal of the American Statistical Association 71 (356): 791–99.\n\n\nBox, George E. P., and Norman R. Draper. 1987. Empirical Model-Building and Response Surfaces. John Wiley & Sons, New York.\n\n\nHoughton, A. N., J. Flannery, and M. V. Viola. 1980. “Malignant Melanoma in Connecticut and Denmark.” International Journal of Cancer 25: 95–104.\n\n\nSilver, Nate. 2012. The Signal and the Noise: Why so Many Predictions Fail–but Some Don’t. Penguin Books.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Science---CT + QT</span>"
    ]
  },
  {
    "objectID": "firstStepsR.html",
    "href": "firstStepsR.html",
    "title": "5  First Steps in R",
    "section": "",
    "text": "5.1 Getting Started with R\nTo get started with R as a statistical programming language you need access to R itself and a development environment from which to submit R code.\nDownload R for your operating system from the CRAN site. CRAN is the “Comprehensive R Archive Network” and also serves as the package management system to add new packages to your installation.\nIf you use VS Code as a development environment, add the “R Extension for Visual Studio” to your environment. We are focusing on RStudio as a development environment here.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>First Steps in `R`</span>"
    ]
  },
  {
    "objectID": "firstStepsR.html#sec-getting-started-R",
    "href": "firstStepsR.html#sec-getting-started-R",
    "title": "5  First Steps in R",
    "section": "",
    "text": "TL;DR What you Need\n\n\n\n\n\nTo work with R in this course, you need to be able to run R code, mix it with text in a notebook-style environment, and turn program and output into pdf and html files. To accomplish this you will need\n\nR. Download from CRAN\nRStudio. Download RStudio Desktop from Posit\n\nYou can skip R and RStudio installs if you do the work in a Posit Cloud environment. Posit Cloud accaounts are available for free here.\n\n\n\n\n\n\n\nPosit Cloud\nIn today’s cloud world, you can get both through Posit Cloud. Posit is the company behind RStudio, Quarto, and other cool tools. Their cloud offering gives you access to an RStudio instance in the cloud. You can sign up for a free account here. The only drawback of the free account is its limitations in terms of RAM, CPU, execution time, etc. For the work you will be doing in this course, and probably many other courses, you will not exceed the limitations of the free account.\nOnce you have created an account, the work space is organized the same way as an RStudio session on your desktop.\n\n\nR and RStudio\nRStudio is an integrated development environment (IDE) for R, but supports other languages as well. For example, using Quarto in RStudio, you can mix R, Python, and code from other languages in the same document.\nDownload Rstudio Desktop here.\nThe RStudio IDE is organized in panes, each pane can have multiple tabs (Figure 5.1). The important panes are\n\nSource. The files you edit. These can be R files (.R), R Markdown (.Rmd), Quarto (.qmd), or any other text files.\nConsole. Here you can enter R commands directly at the command prompt “&gt;”. This pane also has a Terminal tab for an OS terminal and a Background Jobs tab. The latter is important when you “knit” documents into pdf or html format. Knitting a file in RStudio is the process of converting a .Rmd file into a pdf, html, or Word document.\nEnvironment. Displays information about the objects created in the R session. You can click on an object for a more detailed look at it in the Viewer.\nHelp. This pane contains many useful tabs, such as a File browser, package information, access to the documentation and help system. Plots generated from the Console or from an R script are displayed in the Plots tab of this pane.\n\n\n\n\n\n\n\nFigure 5.1: RStudio IDE\n\n\n\n\n\nInstalling Packages\nCapabilities of R come from functions in packages. The basic R installation comes with basic packages that are loaded automatically into an R session. The stats package, for example, provides many basic statistical functions. To see the packages loaded in your workspace, enter this command at the console prompt:\n\n(.packages())\n\nTo add capabilities beyond the basic packages to an R installation, you need to go through a two-step process.\n\nInstall the package\nLoad the package in your R session with the library() command.\n\nInstalling the package is done once, this step adds the package to your system. Loading the library associated with the package is done in every R session that needs to use the functionality of the package. Once you get going with R you will install many packages to tailor R to your needs. Packages that are made available through repositories of CRAN, the Comprehensive R Archive Network, are known as standard packages.\nThe following command installs three CRAN packages, ISLR2, rpart and rpart.plot, in one fell swoop on your system. Copy the command to the console prompt in RStudio and hit .\n\ninstall.packages(c(\"rpart\",\"rpart.plot\",\"ISLR2\"))\n\nThe ISLR2 library provides data sets that are used in James et al. (2021). rpart is a popular library for training decision trees on data and rpart.plot produces nice-looking visualizations of decision trees.\n\n\n.R and .Rmd Files\nYou save R code in two file types. Files with .R extension are called R scripts, they contain pure code and comments. Files with .Rmd extension are in R Markdown notebook format; they combine R code with narrative text and are the basis for high-quality professional documents. Code in R Markdown files is contained in code chunks. Figure 5.2 shows a level-2 header, narrative text, and a code chunk in a R Markdown document. The code chunk begins with the ```{r} and ends with the ``` character sequence. {r} after the opening ticks indicates that the code that follows is in the R programming language.\n\n\n\n\n\n\nFigure 5.2: Narrative text and a code chunk in a .Rmd file\n\n\n\nTo execute the code in a code chunk there are several options:\n\nPlace the cursor somewhere in the chunk and hit &lt;Cmd&gt; &lt;Shift&gt; &lt;Return&gt;\nClick on the green arrow at the right edge of the chunk\nPlace the cursor on a line of code inside the chunk and hit &lt;Cmd&gt; &lt;Return&gt;.\nHighlight several lines of code and hit &lt;Cmd&gt; &lt;Return&gt;\n\nThe first two options execute all lines of code inside the chunk–top to bottom. The third option executes only the line the cursor was placed on and advances to the next line of code. One technique to run an entire chunk line by line is to place the cursor at the beginning of the chunk and repeatedly hit &lt;Cmd&gt; &lt;Return&gt; until you reach the end of the chunk.\nFigure 5.3 shows the content of an R script. Lines that start with a # character are comments. Executing the code in an R script is akin to options 3 and 4 in the list above. When you place the cursor on a line of code and hit &lt;Cmd&gt; &lt;Return&gt; the line is executed and the cursor advances to the next line. You can also highlight multiple lines and execute them together by hitting &lt;Cmd&gt; &lt;Return&gt;. That is necessary if a function requires to be run together with another function. The legend statement in Figure 5.3 cannot be run by itself, it needs to be associated with a graphic. To obtain the desired result, a plot with a legend, the plot and legend statement need to be executed together.\n\n\n\n\n\n\nFigure 5.3: Contents of a .R script file\n\n\n\nInstead of hitting the keys to run code you can also click on the “Run” icon near the top of the screen. The available run options are different for R scripts and R Markdown files.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>First Steps in `R`</span>"
    ]
  },
  {
    "objectID": "firstStepsR.html#college-data",
    "href": "firstStepsR.html#college-data",
    "title": "5  First Steps in R",
    "section": "5.2 College Data",
    "text": "5.2 College Data\nTo get familiar with basic steps in working with data in R we use the College data set that is part of the ISLR2 package. The data set contains information from the 1995 US News and World Report college rankings on 777 colleges in the U.S. The data frame contains the following variables:\n\nVariables in College data set\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\n\nPrivate\nA factor with levels No and Yes indicating private or public university\n\n\nApps\nNumber of applications received\n\n\nAccept\nNumber of applications accepted\n\n\nEnroll\nNumber of new students enrolled\n\n\nTop10perc\nPct. new students from top 10% of H.S. class\n\n\nTop25perc\nPct. new students from top 25% of H.S. class\n\n\nF.Undergrad\nNumber of full time undergraduates\n\n\nP.Undergrad\nNumber of part time undergraduates\n\n\nOutstate\nOut-of-state tuition\n\n\nRoom.Board\nRoom and board costs\n\n\nBooks\nEstimated book costs\n\n\nPersonal\nEstimated personal spending\n\n\nPhD\nPct. of faculty with Ph.D.’s\n\n\nTerminal\nPct. of faculty with terminal degree\n\n\nS.F.Ratio\nStudent/faculty ratio\n\n\nperc.alumni\nPct. alumni who donate\n\n\nExpend\nInstructional expenditure per student\n\n\nGrad.Rate\nGraduation rate\n\n\n\n\nExploratory Data Analysis\nExploratory Data Analysis (EDA) is the “first date with the data.” EDA consists of computing summary statistics of the variables, visualizing their distribution and exploring relationships between the variables. The goal of EDA is to develop understanding of the data we are dealing with and to suggest interesting questions and subsequent analyses.\nEDA can also reveal issues with data quality, for example, data entry errors, outliers, or unobserved (missing) values.\n\n\nExercise: EDA for College Data\n\n\nUse the script Colleges.R and answer the following questions:\n\nCompute simple summary statistics for all variables in the data frame\nWhich college has the highest cost for room and board?\nWhich college has the highest graduation rate?\nWhich colleges have graduation rates above 95%?\nAre there public universities with a graduation rate above 95%?\nCreate a scatterplot of graduation rate versus costs for room and board with data points colored by public/private state of the college.\nCreate a scatterplot of full time undergraduate enrollment versus out-of-state tuition. Color the data points according to the public/private state of the college.\nCreate histograms of the out-of-state tuition and compare private and public colleges. Repeat for the graduation rate.\n\nWhat other questions do you suggest asking?\nDid the exploratory data analysis so far trigger new questions about the data you did not have before?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>First Steps in `R`</span>"
    ]
  },
  {
    "objectID": "firstStepsR.html#r-resources",
    "href": "firstStepsR.html#r-resources",
    "title": "5  First Steps in R",
    "section": "5.3 R Resources",
    "text": "5.3 R Resources\n\nW3 Schools Intro to R\nR for Data Science, 2nd ed. (Wickham, Cetinkaya-Rundel, and Grolemund 2023).\nAdvanced R (Wickham 2019).\nModern Data Science with R, 2nd ed. (Baumer, Kaplan, and Horton 2021).\nR Graphics Cookbook: Practical Recipes for Visualizing Data, 2nd ed. (Chang 2018)\nR Markdown: The Definite Guide (Xie, Allaire, and Grolemund 2019)\nR Markdown Cookbook (Xie, Dervieux, and Riederer 2021)\nMastering Software Development in R (Peng, Kross, and Anderson 2020)\n\n\n\n\nFigure 5.1: RStudio IDE\n\n\n\nBaumer, Benjamin S., Kaplan. Daniel T., and Nicholas J. Horton. 2021. Modern Data Science with r, 2nd Ed. Chapman & Hall/CRC Press. https://mdsr-book.github.io/mdsr3e/.\n\n\nChang, Winston. 2018. R Graphics Cookbook: Practical Recipes for Visualizing Data, 2nd Ed. O’Reilly Media. https://r-graphics.org/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r, 2nd Ed. Springer. https://www.statlearning.com/.\n\n\nPeng, Roger D., Sean Kross, and Brooke Anderson. 2020. Mastering Software Development in r. https://bookdown.org/rdpeng/RProgDA/.\n\n\nWickham, H. 2019. Advanced r, 2nd Ed. Chapman & Hall/CRC Press. http://adv-r.had.co.nz/.\n\n\nWickham, H., M. Cetinkaya-Rundel, and G. Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data, 2nd Ed. O’Reilly Media. https://r4ds.hadley.nz/.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2019. R Markdown: The Definite Guide. Chapman & Hall/CRC Press. https://bookdown.org/yihui/rmarkdown/.\n\n\nXie, Yihui, Dervieux Christophe, and Emily Riederer. 2021. R Markdown Cookbook. Chapman & Hall/CRC Press. https://bookdown.org/yihui/rmarkdown-cookbook/.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>First Steps in `R`</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "6  Correlation and Causation",
    "section": "",
    "text": "6.1 Introduction\nOne of the key tasks in analyzing data is uncovering relationships between things. Describing the statistical behavior of a single variable is interesting. Studying how two or more variables behave together is really interesting. That is how we uncover mechanisms, relationships, associations, and patterns. And if things go really well, we might discover cause-and-effect relationships.\nYou might have heard the saying\nWhat do we mean by that?\nCausation implies that one thing is the result of another thing; they stand in a cause-and-effect relationship to each other. The gravitational pull of the moon on earth’s oceans causes the tides. An accident causes a traffic jam. Smoking causes an increase in the risk of developing lung cancer.\nCorrelation, on the other hand, is about establishing association between attributes. The weight of a person is correlated with their height. Taller people tend to be heavier but height alone is not the only factor affecting someone’s weight. Smoking is correlated with alcoholism but does not cause it.\nFigure 6.1 displays the relationship between highway accidents in the U.S. and lemon imports from Mexico for a period of five years, from 1996–2001. The U.S. Dept. of Agriculture tracks agricultural imports and exports, the U.S. National Highway Traffic Safety Administration (NHTSA) tracks highway fatalities. Neither federal agency probably thought much about the data collected by the other agency. But when put together, voilà. A clear trend emerges!\nIf the relationship in Figure 6.1 is causal, public policy to reduce highway fatalities is very clear: reduce fresh lemon imports from Mexico! Clearly, this is not a causal relationship. There must be another explanation why the variables in Figure 6.1 appear related.\nCompare this situation to John Snow’s investigation of the relationship between water quality and cholera incidences in 19th century London (Chapter 3). Snow found higher cholera incidences in houses closer to the Broad Street public water pump. There was a strong association between cholera cases and proximity to the pump. But did the pump—or more precisely the water from the pump or something in the water—cause cholera? Today we know that cholera is caused by the bacterium Vibrio cholerae, but that discovery was not made until 1883.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation and Causation</span>"
    ]
  },
  {
    "objectID": "correlation.html#introduction",
    "href": "correlation.html#introduction",
    "title": "6  Correlation and Causation",
    "section": "",
    "text": "correlation does not imply causation\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.1: Relationship between highway fatalities and lemon imports from Mexico.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation and Causation</span>"
    ]
  },
  {
    "objectID": "correlation.html#correlation-and-the-correlation-coefficient",
    "href": "correlation.html#correlation-and-the-correlation-coefficient",
    "title": "6  Correlation and Causation",
    "section": "6.2 Correlation and the Correlation Coefficient",
    "text": "6.2 Correlation and the Correlation Coefficient\nWe experience correlation when one attribute changes with another. When the attributes are continuous (see Section 2.4), we can display their association with a scatterplot. A positive correlation then implies that the point cloud has a positive slope, as one attribute increases the other one tends to increase as well (Figure 6.2). When the correlation is negative, an increase in one attribute is associated with a decrease in the other attribute (Figure 6.3).\n\n\n\n\n\n\nFigure 6.2: Positive correlation.\n\n\n\n\n\n\n\n\n\nFigure 6.3: Negative correlation.\n\n\n\nWhile the direction of the point cloud indicates whether the correlation (association) is positive or negative, the tightness of the point cloud indicates the strength of the association (Figure 6.4).\n\n\n\n\n\n\nFigure 6.4: Correlations of different strength and directions. The numbers above the point clouds indicate the strength and direction of the correlation\n\n\n\nWhen we are dealing with discrete attributes, the association cannot be revealed through a point cloud. Instead, we cross-tabulate the frequency of occurrence of the attributes.\n\n\nExample: Rater Agreement\n\n\nTable 6.1 shows the results of a study where insect damage on 236 agricultural fields was classified into 5 damage categories by two different inspectors.\n\n\n\nTable 6.1: Results of rating 236 experimental units by 2 raters\n\n\n\n\n\nRater 2\n1\n2\n3\n4\n5\nTotal\n\n\n\n\n1\n10\n6\n4\n2\n2\n24\n\n\n2\n12\n20\n16\n7\n2\n57\n\n\n3\n1\n12\n30\n20\n6\n69\n\n\n4\n4\n5\n10\n25\n12\n56\n\n\n5\n1\n3\n3\n8\n15\n30\n\n\nTotal\n28\n46\n63\n62\n37\n236\n\n\n\n\n\n\nFor example, 16 experimental units were assigned to damage category 3 by rater 1 and to damage category 2 by rater 2. There is relatively strong association between the ratings, the majority of the counts fall on the diagonal of the table and in the cells immediately off the diagonal (where the raters disagree by one damage category).\n\n\nAnother example of cross-tabulating counts to demonstrate association is Table 3.2 in John Snow’s cholera study. It shows that cholera deaths are 10 times more likely to occur in homes supplied by the Southward & Vauxhall water company than in homes supplied by the Lambeth water company.\n\n\nCorrrelation Coefficient\nThe strength of the correlation between continuous attributes is measured by the correlation coefficient, which ranges from -1 to 1. Both of these extremes are called perfect correlations and happen when all points fall on a perfect line, without variability about the line. The relationship is deterministic. Linear mathematical relationships exhibit such patterns, for example, the relationship between degree Celsius and degree Fahrenheit (Figure 6.5): \\[\n^\\circ F = {^\\circ C} \\times \\frac{9}{5}  + 32\n\\]\n\n\n\n\n\n\n\n\nFigure 6.5: Linear relationship between \\(^\\circ F\\) and \\(^\\circ C\\).\n\n\n\n\n\nThis raises another caution about relying on correlation metrics: the relationship between attributes can be quite strong, but their degree of linear relationship can be low. Figure 6.6 shows two variables with a strong nonlinear relationship. \\(Y\\) decreases with increasing \\(C\\) for values \\(X &lt; 0\\) and \\(Y\\) increases with \\(X\\) for values \\(X &gt; 0\\). When the standard linear correlation coefficient is calculated, it turns out to indicate a very weak relationship between the variables—a very weak linear relationship. The takeaway is not to only focus on reported measures of association but to also examine the relationships visually.\n\n\n\n\n\n\n\n\nFigure 6.6: Strong nonlinear relationship with small (linear) correlation coefficient.\n\n\n\n\n\n\n\nCollege Rankings Data\nComputing correlation coefficients between the variables in a data set is part of exploratory data analysis. In Section 5.2.1 we performed EDA for the College rankings data. Now let us compute the correlation coefficients.\nFirst, we grab the data from the ISLR2 library and attach it to the R session.\n\nlibrary(ISLR2)\ndata(College)\nattach(College)\n\nThe cor() function computes correlation coefficients between pairs of variables or sets of variables. The next statement computes the correlation between graduation rate and out-of-state tuition for the 777 colleges in the data frame:\n\ncor(Grad.Rate, Outstate)\n\n[1] 0.5712899\n\n\nThe two variables have a modest positive correlation of 0.5713.\nTo see the correlation coefficients between graduation rates and the other numeric variables in the data frame, list the variable for which you want to obtain the correlation coefficients second:\n\ncor(College[,-1], Grad.Rate)\n\n                    [,1]\nApps         0.146754600\nAccept       0.067312550\nEnroll      -0.022341039\nTop10perc    0.494989235\nTop25perc    0.477281164\nF.Undergrad -0.078773129\nP.Undergrad -0.257000991\nOutstate     0.571289928\nRoom.Board   0.424941541\nBooks        0.001060894\nPersonal    -0.269343964\nPhD          0.305037850\nTerminal     0.289527232\nS.F.Ratio   -0.306710405\nperc.alumni  0.490897562\nExpend       0.390342696\nGrad.Rate    1.000000000\n\n\nBecause we cannot compute the correlation with a factor variable (Private), the first variable is omitted from the data frame by specifying College[,-1].\nThe graduation rates are not very strongly correlated with any of the variables. Note that the correlation of a variable with itself is always 1.\nFinally, to obtain all pairwise correlations between numeric variables in the data frame we request the computation of the correlation matrix. This results in a large 17 x 17 matrix. To help with readability the result is displayed with three decimal places.\n\nround(cor(College[,-1]),3)\n\n              Apps Accept Enroll Top10perc Top25perc F.Undergrad P.Undergrad\nApps         1.000  0.943  0.847     0.339     0.352       0.814       0.398\nAccept       0.943  1.000  0.912     0.192     0.247       0.874       0.441\nEnroll       0.847  0.912  1.000     0.181     0.227       0.965       0.513\nTop10perc    0.339  0.192  0.181     1.000     0.892       0.141      -0.105\nTop25perc    0.352  0.247  0.227     0.892     1.000       0.199      -0.054\nF.Undergrad  0.814  0.874  0.965     0.141     0.199       1.000       0.571\nP.Undergrad  0.398  0.441  0.513    -0.105    -0.054       0.571       1.000\nOutstate     0.050 -0.026 -0.155     0.562     0.489      -0.216      -0.254\nRoom.Board   0.165  0.091 -0.040     0.371     0.331      -0.069      -0.061\nBooks        0.133  0.114  0.113     0.119     0.116       0.116       0.081\nPersonal     0.179  0.201  0.281    -0.093    -0.081       0.317       0.320\nPhD          0.391  0.356  0.331     0.532     0.546       0.318       0.149\nTerminal     0.369  0.338  0.308     0.491     0.525       0.300       0.142\nS.F.Ratio    0.096  0.176  0.237    -0.385    -0.295       0.280       0.233\nperc.alumni -0.090 -0.160 -0.181     0.455     0.418      -0.229      -0.281\nExpend       0.260  0.125  0.064     0.661     0.527       0.019      -0.084\nGrad.Rate    0.147  0.067 -0.022     0.495     0.477      -0.079      -0.257\n            Outstate Room.Board  Books Personal    PhD Terminal S.F.Ratio\nApps           0.050      0.165  0.133    0.179  0.391    0.369     0.096\nAccept        -0.026      0.091  0.114    0.201  0.356    0.338     0.176\nEnroll        -0.155     -0.040  0.113    0.281  0.331    0.308     0.237\nTop10perc      0.562      0.371  0.119   -0.093  0.532    0.491    -0.385\nTop25perc      0.489      0.331  0.116   -0.081  0.546    0.525    -0.295\nF.Undergrad   -0.216     -0.069  0.116    0.317  0.318    0.300     0.280\nP.Undergrad   -0.254     -0.061  0.081    0.320  0.149    0.142     0.233\nOutstate       1.000      0.654  0.039   -0.299  0.383    0.408    -0.555\nRoom.Board     0.654      1.000  0.128   -0.199  0.329    0.375    -0.363\nBooks          0.039      0.128  1.000    0.179  0.027    0.100    -0.032\nPersonal      -0.299     -0.199  0.179    1.000 -0.011   -0.031     0.136\nPhD            0.383      0.329  0.027   -0.011  1.000    0.850    -0.131\nTerminal       0.408      0.375  0.100   -0.031  0.850    1.000    -0.160\nS.F.Ratio     -0.555     -0.363 -0.032    0.136 -0.131   -0.160     1.000\nperc.alumni    0.566      0.272 -0.040   -0.286  0.249    0.267    -0.403\nExpend         0.673      0.502  0.112   -0.098  0.433    0.439    -0.584\nGrad.Rate      0.571      0.425  0.001   -0.269  0.305    0.290    -0.307\n            perc.alumni Expend Grad.Rate\nApps             -0.090  0.260     0.147\nAccept           -0.160  0.125     0.067\nEnroll           -0.181  0.064    -0.022\nTop10perc         0.455  0.661     0.495\nTop25perc         0.418  0.527     0.477\nF.Undergrad      -0.229  0.019    -0.079\nP.Undergrad      -0.281 -0.084    -0.257\nOutstate          0.566  0.673     0.571\nRoom.Board        0.272  0.502     0.425\nBooks            -0.040  0.112     0.001\nPersonal         -0.286 -0.098    -0.269\nPhD               0.249  0.433     0.305\nTerminal          0.267  0.439     0.290\nS.F.Ratio        -0.403 -0.584    -0.307\nperc.alumni       1.000  0.418     0.491\nExpend            0.418  1.000     0.390\nGrad.Rate         0.491  0.390     1.000\n\n\nIt is much easier to see what is going on with all pairwise correlations when the matrix is displayed graphically. The strength and direction can be displayed with colors. The diagonal values can be omitted as they are known to be 1.\nThe corrplot function in the package by the same name creates nice visualizations of correlation matrices. Before running the code in the chunk below, you will need to install the corrplot package first, unless it is already installed on your system:\n\ninstall.packages(\"corrplot\")\n\nNow we can compute the heatmap of the pairwise correlation coefficients (Figure 6.7).\n\nlibrary(corrplot)\ncormat &lt;- cor(College[,-1])\ncorrplot(cormat, diag=FALSE, method=\"color\")\n\n\n\n\n\n\n\nFigure 6.7: Visualization of pairwise correlations for College data.\n\n\n\n\n\nYou see high positive correlations (dark blue color) in the upper left corner of the heatmap. The number of applications received (Apps) is very highly correlated with the number of applications accepted (Accept)—this is not a big surprise.\n\n\nAssignment: College Ranking Correlations\n\n\n\nAre there surprising positive or negative correlations in the matrix?\nWhich of the strong correlations are not surprising to you?\nCan you explain some of the weak (near zero) correlations?\nA relatively strong negative correlation exists between Expend and S.F.Ratio. A similar correlation exists between Outstate and S.F.Ratio. How do you interpret/explain these dependencies?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation and Causation</span>"
    ]
  },
  {
    "objectID": "correlation.html#spurious-correlations",
    "href": "correlation.html#spurious-correlations",
    "title": "6  Correlation and Causation",
    "section": "6.3 Spurious Correlations",
    "text": "6.3 Spurious Correlations\nIt seems obvious that just because two attributes vary with each other—are correlated—one should not infer that they are cause and effect of each other. Unfortunately, that leap of faith is often made and can lead to very problematic decisions.\n\n\n\n\n\n\nFigure 6.8: Chocolate consumption and number of Nobel laureates.\n\n\n\nFigure 6.8 displays the number of Nobel laureates per 10 million population against the chocolate consumption (in kg per capita and year) for various countries (Messerli 2012). An upward trend is clearly noticeable. A greater per-capita chocolate consumption is associated with a lager number of Nobel laureates. Whoa! If the two variables stand in a cause-effect relationship then we have a simple recipe to increase Nobel prizes: we all should eat more chocolate. While one can argue the benefits of chocolate for cognitive function, what we have here is a simple correlation. The two attributes, number of Nobel laureates per 10 million population and chocolate consumption per capita, are related. If one increases so does the other. But why?\nThis is an example of a spurious correlation. The variables are not really dependent on each other, a relationship is induced in some other way. In this particular example, the correlation was “found” by cherry-picking the data: only four years of chocolate consumption were considered on a limited number of chocolate products and no data prior to 2002 was used. The number of Nobel laureates is a cumulative measure that spans a much longer time frame.\nIt appears that the data were organized in such a way as to suggest a relationship between the variables.\nYou can find spurious correlations everywhere, without manipulating the data. Here are some examples and the reasons why the data appear correlated.\n\nCoincidence\nForecasting economic conditions is difficult and highly valuable. Since the end of World War II there have been only eleven economic recessions. On the other hand we are producing thousands of economic indicators. The government alone generates 45,000 economic statistics each year (Silver 2012, 185).\nSo it should not be surprising that when sifting through all those variables we find some that appear to go up or down together, just by coincidence.\nA famous example is the Super Bowl winning conference as an indicator of economic performance. Between 1967 and 1997, in years when the team from the original NFL won, the stock market went up by 14%. When a team from the original AFL won the stock market decreased by almost 10%. Through 1997, the Super Bowl winner “predicted” correctly the direction of the stock market in 28 out of 31 years (Silver 2012). Since 1998 the trend has reversed and the stock market is doing better when an AFL team won the Super Bowl. Coincidence during a period of time leads to mistake noise for signal.\nAnother example of coincidence being mistaken for correlation is Paul, the octopus who correctly predicted the winner in 2008–2010 international soccer matches 12 out of 14 times, an 85% accuracy. Since this success rate is unlikely to happen by chance, it was determined that Paul the octopus has divine powers. When Paul got it wrong in a 2010 FIFA World Cup game between Germany and Spain, the German fans called for Paul to be eaten and the Spanish Prime Minister offered Paul asylum.\n\n\nLatent Variables\nA correlation between variables A and C can be induced by another variable, say B. If A is correlated with (or caused by) B and C is correlated (or caused by) B, then plotting A versus C indicates a correlation between the two variables. However, the relationship is induced by the latent variable B.\n\n\n\n\n\n\nFigure 6.9: Spurious correlation.\n\n\n\nLatent variables are often the real reason why things appear related when we deal with variables that depend on population size or common factors such as the weather or time. Figure 6.9 shows the close relationship over time between the number of high school graduates and donut consumption. More donut consumption appears related to more high school graduates. Notice that we are not plotting graduation rates, these would most likely not have any relationships with donut consumption. The latent variable at work in Figure 6.9 is the size of the population over time. As the population increases, more donuts are consumed and more people graduate from high school.\nA similar spurious correlation is that between ice cream sales and forest fires. Both increase during the summer heat and decrease in the winter.\nYou can imagine the horrible public policy decisions one would make by mistaken those spurious correlations for cause and effect relationships.\n\n\nInduced Correlation\nAnother interesting mechanism to induce correlation is by introducing a mathematical dependence between two attributes. A famous example is the relationship between birth rate and the density of storks.\n\n\n\n\n\n\nFigure 6.10: Storks and Babies\n\n\n\nIn central Europe a persistent myth is that storks bring babies. Movies were made about it! The origin of the association probably goes back to medieval days when conception was more common in mid-summer during the celebration of the summer solstice which is also a pagan holiday of marriage and fertility. The white stork is a migratory bird that flies to Africa in the fall and returns to Europe nine months later. Hence the connection was made that storks brought the babies.\nAlthough the myth has been debunked, there have been several studies of the connection between fertility and the stork abundance. Neyman (1952) describes a study of 54 counties that comprises the following attributes\n\n\\(W\\): Number of women of child-bearing age in the county (in 10,000)\n\\(S\\): Number of storks in the county\n\\(B\\): Number of babies born in the county\n\nSince it is likely that these numbers increase with the size of the county, the variables analyzed were \\(Y = B/W\\) and \\(X=S/W\\), the birth rate per 10,000 women and the density of storks per 10,000 women. A plot of these variables and a smooth estimate of their trend is shown in Figure 6.11.\n\n\n\n\n\n\nFigure 6.11: Storks and babies.\n\n\n\nIt certainly appears that the birth rate increases with the density of storks. Could the myth be true? Is something else going on?\nThe trend in Figure 6.11 is induced by expressing both variables as ratios with the same variable, \\(W\\), the number of women of child-bearing age. If \\(S\\) and \\(B\\) are unrelated, \\(S/W\\) and \\(B/W\\) now share information because they are expressed relative to another variable.\n\n\nAssignment: Mozzarella and Engineering Degrees\n\n\nSpiegelhalter (2021) cites the strong correlation (coefficient 0.96) between the annual per-capita consumption of mozzarella cheese in the U.S. in the period 2000–2009 (\\(X\\)) and the number of civil engineering doctorates awarded in those years (\\(Y\\)).\n\nHow would you interpret this relationship if \\(X\\) causes \\(Y\\)?\nIf this relationship is spurious, what could explain it?\nCorrelation is a symmetrical relationship, mathematically, \\(\\text{Cor}(X,Y) = \\text{Cor}(Y,X)\\). Causation on the other hand is asymmetrical. If \\(X\\) causes \\(Y\\) does not imply that \\(Y\\) causes \\(X\\). \\(Y\\) causing \\(X\\) is called reverse causation. What does reverse causation mean in the Mozzarella–Engineering Ph.D. example?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation and Causation</span>"
    ]
  },
  {
    "objectID": "correlation.html#establishing-causality",
    "href": "correlation.html#establishing-causality",
    "title": "6  Correlation and Causation",
    "section": "6.4 Establishing Causality",
    "text": "6.4 Establishing Causality\nEstablishing a causal link between factors is the holy grail of scientific study. When we prove that one event is the result of another event, we have established new, irrefutable knowledge, beyond a reasonable doubt.\nEstablishing cause and effect is also quite difficult. Spiegelhalter (2021, 97) calls causation a “deeply contested subject”. You get a splinter in your finger and it hurts. Did the splinter cause the pain? Is it possible that the finger would have abruptly started to hurt if it wasn’t for the splinter? If it was me, it would be pretty obvious to me that the splinter caused the pain.\nBut wait. Not everyone has the same level of pain tolerance. The splinter that causes me much agony might be a mere scratch, hardly noticeable, for someone else. How do we take this variability in the population into account in making statements about causality?\nWhen we say that smoking causes lung cancer, we do not claim that every smoker will get the disease. Some smokers do not get lung cancer and there are lung cancer patients who never smoked. The second leading cause of lung cancer deaths in the U.S. is exposure to Radon gas. That is why in many regions Radon inspections are required prior to purchasing homes. A more precise statement would be that smoking causes an increase in the likelihood of getting lung cancer. It is not a statement about what happens to Joe or Diana. The statistical notion of causation between \\(X\\) and \\(Y\\) means that if \\(X\\) occurs, \\(Y\\) tends to occur more or less often. The statistical notion of causation is not deterministic (Spiegelhalter 2021, 99).\nIn a study about the association between repeated head impacts (RHI) and chronic traumatic encephalopathy (CTE), Nowinski et al. (2022) state that causation is an interpretation, not an entity. In studies involving complex environmental exposures causation is a continuum from highly unlikely to highly likely, and no single study can prove causation.\nIn the presence of uncertainty some scientific standard needs to be met in order for us to claim that something has been proven and even then, we are not making statements that the something will happen every time, only that the proportion of times that it will happen has been affected.\n\nConfounding\nLet’s return to the 1854 cholera outbreak and the question before John Snow: did something in the water of the Broad Street public water pump cause cholera? If so, this would explain the higher incidence rate of cholera in residences near the pump and it would also explain the other anomalies he found in the data (see Chapter 3).\nThe map Snow drew in 1854 (Figure 3.2) might be convincing to us, his contemporaries did not feel that way. For one, he could not prove that the Broad Street well water caused the cholera cases. And his hypothesis was inconsistent with the prevailing theory of the time, that cholera was caused by airborne particles (miasma) from dirty or decaying biological material.\nThe analysis of the cholera map established a correlation rather than causation because of the possibility of confounding factors: variables that can mask or distort the effect of other variables. In the 1960s it was shown that coffee drinkers had higher rates of lung cancer than non-coffee drinkers. Some thought this was implication of coffee as a cause of lung cancer. That is incorrect. The association is due to a confounding factor: coffee drinkers at the time were more likely to be also smokers. Coffee drinking was associated with lung cancer but does not cause the disease.\nA confounding factor is related to both the cause and the effect and can mislead us into attributing too much or too little importance to the potential cause. Variables such as age, time, temperature, population size are often confounding factors because they act on the factor of interest and on the outcome of interest. For example, age is a confounding factor in studies of exposure to harmful agents. If damage from the agent is more prevalent in older people, age can be a confounding factor because older people have been exposed longer.\nWhen spurious correlations are induced by latent variables, the latent variable is a confounder. The apparent correlation between ice cream sales and shark attacks is explained by the confounder temperature. Ice cream sales and shark attacks increase with temperature as more people buy ice cream and more people go to the beach.\n\n\nAssignment: 1854 Cholera Outbreak\n\n\nIn the case of the 1854 cholera outbreak, there could have been confounding factors that caused cholera incidences in the Broad Street area to be higher, whether the water was or was not the cause of the disease. Maybe the residents of that poorer neighborhood had a different diet that caused the disease. Maybe they had occupations that made it more likely to be exposed to a harmful agent. Maybe. Maybe. Maybe.\nWhat other confounding factors can you think of that would mask, amplify, or suppress the incidence of cholera?\n\n\nIn order to establish a causal link between two variables, the confounding factors must be accounted for—at least beyond a reasonable doubt. Otherwise there will always be some reason to believe another mechanism was at work. There are a few principal mechanisms to deal with confounding variables:\n\nAdjustment\nStratification\nRandomization\n\nWe will discuss these in turn, but note that they are not mutually exclusive. A study might involve experimentation with randomly assigned treatments as well as model adjustments for confounding variables.\n\n\nAdjustment\nAdjusting for confounding variables means to include the variables in models that describe the relationship between the factor of interest and the outcome of interest. Consider the Radon exposure example above. A model to describe the relationship between exposure \\(x\\) (in picocuries per liter air) and cancer risk could be written in two parts: \\[\n\\begin{align}\n\\eta &= \\beta_0 + \\beta_1 x + \\beta_2 \\text{ age} \\\\\n\\Pr(\\text{Develops cancer}) &= \\frac{1}{1+\\exp\\{-\\eta\\}} \\\\\n\\end{align}\n\\tag{6.1}\\]\nThe first term of the model, \\(\\eta\\), is called the linear predictor and is a function of the variables that materially determine the cancer risk. In addition to the exposure \\(x\\), the linear predictor also contains a term for a person’s age. The second expression in Equation 6.1 transforms the linear predictor into a probability—it is called the logistic transformation.\nThis model allows us to estimate how much of the cancer risk is due to the level of radon exposure and how much is due to the age of the person. With the variable of interest (\\(x\\)) and the confounding variable (age) disentangled we can make statements about the risk of cancer as a function of radon exposure and age.\nAn important aspect of adjusting for confounding variables is the functional relationship between the variables. In Equation 6.1 the two variables enter the linear predictor in an additive fashion. Maybe this is not the appropriate adjustment. If the effect of age on cancer risk changes with the level of radon exposure then we say that the two variables interact. A model that includes a multiplicative interaction tterm then might be more appropriate: \\[\n\\eta = \\beta_0 + \\beta_1 x + \\beta_2 \\text{ age} + \\beta_3 x\\,\\text{age}\\\\\n\\]\nIn other words, determining how to model the relationship of confounding variables on other variables and on the outcome of interest, is of great importance.\n\n\nStratification\nStratification is an approach to deal with confounding factors that are qualitative. It means to examine relationships separately for each level of the confounder. To see if there is a general relationship between ice cream sales and shark attacks we can examine the association for different temperature ranges. As a surrogate for that we can look at the association by month or by season.\nWhen performing analyses overall and comparing them to analyses within groups (within strata) we can run into situations where the two seem to provide contradictory results. This is known as Simpson’s paradox.\n\nSimpson’s Paradox\nThe paradox is named after Edward Simpson who described it in a technical paper in 1951, but the phenomenon has been known much longer. It is also not a paradox as it does not lead to nonsensical outcomes or a contradiction. What we know as Simpson’s Paradox is simply the result of looking at an aspect from two viewpoints: The trend that we see in combined data can reverse when we look at the data in groups.\nFigure 6.12 displays a scatter plot of two variables and a linear regression trend. The slope is positive, the average value of \\(Y\\) increases with the value of \\(X\\).\nThe data in Figure 6.12 consists of three groups and when the regression analysis is performed separately for each group, a different picture emerges. Within each group the regression relationship indicates a negative slope, the opposite of the trend in the ungrouped data (Figure 6.13).\n\n\n\n\n\n\n\n\nFigure 6.12: Combined data and regression trend.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.13: Grouped data and group-specific regression trends.\n\n\n\n\n\nThe apparent contradiction comes about because the three groups have different centers and orientation. When “stacked” they create a single point cloud with a positive slope. As mentioned previously, this is not really a paradox, both ways of looking at the data are sensible and the results are meaningful either way. The “paradox” lies in the fact that the two views lead to seemingly different conclusions about the relationship between the variables.\nExamples of Simpson’s Paradox with qualitative data can be found in college admissions data. Wikipedia shows the apparent gender bias effect for 1973 data from UC Berkeley. Spiegelhalter (2021) shows data for Cambridge from 1996. The table in Spiegelhalter (2021, 111) is reproduced below as two separate tables.\n\n\n\nTable 6.2: Application and acceptance rates at Cambridge in 1996 in STEM disciplines for men and women.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWomen\n\n\nMen\n\n\n\n\n\n\n\nApplied\nAccepted\n%\nApplied\nAccepted\n%\n\n\nTotal\n1,184\n274\n23%\n2,740\n584\n24%\n\n\n\n\n\n\nTable 6.2 shows the overall acceptance rates for men and women, with the former slightly higher by one percent.\n\n\n\nTable 6.3: Application and acceptance rates at Cambridge in 1996 by STEM discipline.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWomen\n\n\nMen\n\n\n\n\n\n\n\nApplied\nAccepted\n%\nApplied\nAccepted\n%\n\n\nComputer Science\n26\n7\n27%\n228\n58\n25%\n\n\nEconomics\n240\n63\n26%\n512\n112\n22%\n\n\nEngineering\n164\n52\n32%\n972\n252\n26%\n\n\nMedicine\n416\n99\n24%\n578\n140\n24%\n\n\nVeterinary Medicine\n338\n53\n16%\n180\n22\n12%\n\n\n\n\n\n\nTable 6.3 shows the applications and acceptance rates by discipline. In each discipline the acceptance rate for women is at least as high as that for men, in fact it is higher than that for men, except for Medicine.\nHow do we explain this apparent contradiction? In each discipline the acceptance rate for women is as high or higher than that for men, but overall the acceptance rat for women is lower. The explanation is that women were more likely to apply for subjects that have high application numbers and are more difficult to get in. 64% of the applications from women went to Medicine and Veterinary Medicine (416 + 338 out of 1,184). Only 30% of the male applications went to those subjects. Men applied disproportionately more to Engineering, which has a higher acceptance rate than the other STEM disciplines. It seems that there is no gender bias in these admission numbers, the lower overall admission rate for women is the result in women applying in larger numbers to STEM disciplines that are difficult to get into.\n\n\n\nRandomization\nRandomization is a simple and effective mechanism to create probabilistic equivalence in data and takes two forms: random selection and random assignment. Random does not imply haphazard or arbitrary, the way in which selection or assignment is randomized obeys probability distributions. Because we specify the distribution according to which selection or assignment occurs, we understand the probabilistic behavior of the data we collect.\nFor example, we select a random sample of people from a population such that each person has the same chance of entering the sample. This guarantees that the attributes of the population are represented properly in the sample. The proportion of persons of different age, gender, race, etc. in the sample will be representative of the population from which the sample is drawn, although the exact proportions will not be identical in a particular sample. If we were to draw a sample of \\(n\\) people by a non-random mechanism, for example, by taking the first \\(n\\) folks who drive through an intersection, or \\(n\\) airline passengers on a given day, or \\(n\\) residents of a particular neighborhood, or the first \\(n\\) entries in the list of the U.S. Census Bureau, our sample would not be representative of the population we are interested in studying. The sample would suffer from selection bias, conclusions drawn from analyzing the data in the sample would not apply to the population as a whole. Note that random sampling from a non-representative list also suffers from selection bias. Randomly sampling social media posts does not give insight into the opinions of the general population because social media users are not representative of the general population. Asking questions of a random sample of drivers passing through an intersection does not properly represent those who do not drive or live elsewhere.\nThe random mechanism in random selection has a balancing property. It ensures that sub-groups are not over represented or under represented in the sample, provided that the sample is sufficiently large. This balancing property is also key for the second form of randomization: random assignment when conditions are manipulated on purpose. This leads us to experimentation.\n\n\nExperimentation–The Randomized Controlled Trial (RCT)\n\nExample: RCBD in Agriculture\nSuppose you wish to study how poppies grow on an agricultural field. The poppies are subject to varying growing conditions due to differences in the soil characteristics, topography, weather, plant-to-plant variations, and so on. We are particularly interested to see how six different fertilizer applications affect the poppy growth. The conditions that affect poppy growth can be grouped into three categories:\n\nConditions we manipulate\nConditions we do not manipulate but know about\nConditions we do not know about (lurking factors)\n\nThe fertilizer treatments fall into the first category. We can apply parts of the field and apply fertilizer A to it, other parts receive fertilizer B, and so on. Suppose that the field is large and sloped and we suspect a gradient in soil nutrients and water. This falls into the second category. We do not create or manipulate the soil and water conditions, but we are aware of them. They are a confounding factor. All other potential influences of poppy growth fall into the third category.\nFigure 6.14 displays a popular layout for running such a fertilizer experiment in agricultural science. It is called a randomized complete block design (RCBD). The experimental area is divided up into separate blocks, in this case four of them.\n\n\n\n\n\n\nFigure 6.14: Experimental layout for poppy experiment.\n\n\n\nThe blocks are chosen so that the fields within a block—we call them experimental units—are homogeneous with respect to the conditions we know about. This is the technique to control confounding factors in category 2 by stratification and adjustment. Within each block the fertilizer treatments are assigned randomly to the six experimental units. This random assignment balances out the effects of the confounding factors we do not know about (category 3). If we were to assign treatments to experimental unit in a deterministic way—for example, treatment A always to the upper left unit, treatment B right next to it, etc.—it is possible that confounding factors associated with the position in the block mask or distort the effect of the treatment. We would then not really compare treatment A to treatment B, but a blend of treatment effect and location effect.\nFinally, because the world is uncertain, we do not apply the treatments only once. We use multiple blocks, each with a separate assignment of treatments to experimental units to replicate the per-block layout. This replication allows us to measure the inherent variability in poppy growth, unaffected by treatment or confounding factors.\nIf the experimental units in our agricultural science experiment contain more poppies than we can harvest and analyze in the lab, we would select some of them for lab analysis. This would be done by random selection to make sure that the plants analyzed in the lab are representative of the plants growing on the experimental unit.\nIn this RCBD with four blocks and six treatments we encounter all techniques to manage confounding factors: stratification, adjustment (because block effects will be included in the model during analysis), and randomization. As a result, we are allowed to make cause-and-effect statements about the treatment factor we manipulated, the fertilizer. If plants grown under the fertilizer A treatment are taller than those grown under fertilizer B, then there are only two possible explanations:\n\nFertilizer A causes poppies to grow taller compared to fertilizer B\nCoincidence: the height difference we are seeing is due to chance\n\nOther explanations can be ruled out because we controlled the experiment for other factors.\nIn order to separate the two remaining explanations, we analyze the size of the treatment differences relative to the inherent variability in poppy growth. That is the reason why the experiment uses multiple blocks rather than a single block. The replication of treatment assignments allows us to estimate that inherent variation. If poppy growth varies widely, then a small difference in height between plants from treatment A and treatment B will not surprise us. If inherent growth variation is small, observed differences between treatments point at the fertilizer as the cause.\n\n\nOther Experiments\nExperimentation with random assignment of conditions has a long history. It started in agricultural experiments and has since permeated many domains. Randomized clinical trials are common to test medical drugs, devices, and treatments. Industrial experimentation is used to find the best manufacturing conditions for products.\nRandomized trials are also used in the social sciences. Spiegelhalter (2021, 107) cites the Study of the Therapeutic Effects of Intercessory Prayer (STEP) by Benson H. et al. (2006) to answer the question whether being prayed for improves the recovery of patients after coronary artery bypass graft (CABG) surgery. The Methods section of the paper describes the randomized experiment:\n\nPatients at 6 US hospitals were randomly assigned to 1 of 3 groups: 604 received intercessory prayer after being informed that they may or may not receive prayer; 597 did not receive intercessory prayer also after being informed that they may or may not receive prayer; and 601 received intercessory prayer after being informed they would receive prayer. Intercessory prayer was provided for 14 days, starting the night before CABG. The primary outcome was presence of any complication within 30 days of CABG. Secondary outcomes were any major event and mortality.\n\nThe study concludes\n\nIntercessory prayer itself had no effect on complication-free recovery from CABG, but certainty of receiving intercessory prayer was associated with a higher incidence of complications.\n\nKnowing that they were being prayed for might have made patients more uncertain, wondering whether they are so sick that they had to call in the prayer team.\n\nExperimentation with random assignment is also used frequently in the technology industry, the technique is known as A/B testing. Only two treatments are being evaluated (A and B), one is typically a current product or design. Users of the product/design are randomly directed to the A or B option and the attribute of interest is measured (click-through rate, time on page, use of features on page, checkout, purchase amount, etc.). We are all participating in these ongoing A/B experiments when we operate online. Google is said to run about 10,000 A/B experiments every year.\n\n\n\nWhen Experimentation is Not Possible\nExperimentation with random assignment of treatments is the gold standard to establish cause and effect. But it is not always possible to go down that path.\nSome systems defy manipulation with treatments. We can only observe the weather we cannot change it. The process of manipulation can alter how a system behaves in ways that are not related to the treatment application, so we cannot really study just the treatment effects. Epidemiological studies, like John Snow’s investigation of the 1854 Cholera epidemic are by definition observational studies: we observe what is happening, not conditions we create deliberately.\nWhile one can assign conditions, ethical considerations might prevent us from doing so. How can we justify assigning harmful things and ask a person to smoke two packs of cigarettes a day for the next 10 years? In the case of testing a medical breakthrough against a horrible disease, how can we justify assigning patients to a placebo group and withholding a potentially life-saving treatment? To show that repeated head impact (RHI) causes chronic traumatic encephalopathy (CTE), it would not be ethical to randomize subjects and hit those assigned to the RHI arm of the study repeatedly over the head.\nWhen experimentation is not possible, we rely on observational data, analyzing the data we can collect, and it is often the best we can do. How can we then explain the association we find in the data and get closer to establishing causality?\nTo link RHI to CTE, we can study data on subjects who have been exposed to repeated head trauma, such as boxers and football players, and compare their likelihood of developing CTE to individuals who did not experience such head trauma. Comparisons must be made with care. We would not want to compare star athletes who experience head trauma with non-athletes who did not experience head trauma; there would be too many confounding factors. Maybe we could follow a group of athletes over time and record accumulated head impacts along with brain scans. While we cannot design an experiment, we can design how to collect observational data.\n\n\nAssignment: Why Do Old Men Have Big Ears?\n\n\nSpiegelhalter (2021, 108–9) asks this question based on his personal experience that older men seem to have big ears. This question cannot be answered with a randomized controlled trial, we cannot assign ear lengths. It is what it is.\nObservational studies in the UK and Japan collected cross-sectional data, that is, a sample from the current population, which will include men of different ages. Analyzing the data the studies concluded that there is a positive correlation between age and ear length. For example, the figure below appeared in Heathcote (1995). The study concluded that the regression trend was significant. The slope of the regression line is 0.22mm per year with a 95% confidence interval of [0.17, 0.27] mm per year. Because the confidence interval does not cover the value 0, the trend is statistically significant. It seems that as we get older our ears get bigger by an average by 0.22 mm per year.\n\n\n\n\n\n\nFigure 6.15: Ear length. From Heathcote (1995).\n\n\n\n\nTry and explain the association between age and ear length in men. What are possible reasons ears are/appear larger in older men?\nIf you were to conduct a follow-up study to test the possible reasons in 1., what would the study look like? What kind of data would you collect? What kind of men would you recruit for the study?\n\n\n\n\nQuasi-experiments\nIf you cannot manipulate and control factors in an experiment, maybe you are lucky to find data where the confounding factors have already been controlled for you. Sometimes real life runs these quasi-experiments for us and eliminates the confounding factors. Although the data is observational rather than experimental, it can go a long way toward establishing causality. In Section 3.4 we discussed a second, deeper analysis John Snow conducted in which he compared cases between customers of the Lambeth and the Southward & Vauxhall water companies. For all intents and purposes the groups serviced by the two companies were identical except for the source of the water. Lambeth’s water was drawn upriver from sewage discharge into the River Thames and was cleaner than the water from Southwark & Vauxhall, which drew water below the sewage discharge. The much higher cholera incidence in the group supplied by Southwark & Vauxhall was sufficient evidence to implicate the water.\n\n\nAssignment: Fluoride Exposure and IQ\n\n\nThe National Toxicology Program of the U.S. Department of Health and Human Services conducted a meta-analysis of the relationship between fluoride intake and IQ.\nAmong the findings of the analysis, the article states (emphasis in original):\n\nThe NTP monograph concluded, with moderate confidence, that higher levels of fluoride exposure, such as drinking water containing more than 1.5 milligrams of fluoride per liter, are associated with lower IQ in children. The NTP review was designed to evaluate total fluoride exposure from all sources and was not designed to evaluate the health effects of fluoridated drinking water alone. It is important to note that there were insufficient data to determine if the low fluoride level of 0.7 mg/L currently recommended for U.S. community water supplies has a negative effect on children’s IQ. The NTP found no evidence that fluoride exposure had adverse effects on adult cognition.\n\n…\n\nThe determination about lower IQs in children was based primarily on epidemiology studies in non-U.S. countries such as Canada, China, India, Iran, Pakistan, and Mexico where some pregnant women, infants, and children received total fluoride exposure amounts higher than 1.5 mg fluoride/L of drinking water. \n\nPlease read the full article from the National Toxicology Program here and answer the following questions:\n\nDid the study establish correlation or causation between fluoride intake and children’s IQ?\nDoes the article make it clear whether to take the results as an indication of causation or correlation?\nWhat is meta-analysis?\nHow do you interpret the fact that the study did not analyze data from the U.S.? Does that affect whether the results are applicable to U.S. children?\nCan you think of confounding factors that limit transfer of the results to the U.S?\nAre you surprised that there is no evidence of adverse effects on adults?\n\n\n\nIn domains and applications where experimentation is not possible and confounding factors are present, we try to establish causation by a process called causal inference. By studying which variables act on each other, causality can be inferred.\n\n\nHill’s Criteria\nEstablishing causality from observational data, as in the case of Snow’s water quality study, is a common problem in epidemiology, the study of the distribution of diseases and health risks. Establishing designed experiments with randomized control of factors is often not possible in those studies.\nThe English epidemiologist and statistician Sir Author Bradford Hill established nine principles that allow one to move from association to causation. These are known as Hill’s criteria, developed to establish causation involving environmental exposure. Hill was part of the research team that confirmed the link between smoking and lung cancer. The criteria are:\n\nStrength: strong association is stronger evidence of causality. A small association does not rule out a causal effect, however.\nConsistency: similar studies by others in different places with different samples give consistent findings.\nSpecificity: the more specific the association between a factor and an effect, the more likely we are dealing with cause and effect. The association is specific when the cause leads to only one outcome and the outcome can only come from the one cause.\nTemporarlity: the effect comes after the cause.\nGradient: an increase in level, intensity, duration or total level of exposure to the potential cause leads to progressive increase in (the likelihood of) the outcome.\nPlausibility: the association is plausible based on known scientific facts.\nCoherence: agreement between epidemiological and laboratory findings. The interpretation of the data does not seriously conflict with what is already known about the disease or exposure.\nExperimentation: if experimentation is possible, it provides results in support of the causal hypothesis (provides strong evidence).\nAnalogy: similarity between things that are otherwise different. Scientists can use prior knowledge and patterns to infer similar causal associations.\n\nHill’s criteria should be viewed as a guideline for establishing causality based on association. Proving causality is not done by clicking check boxes. Meeting the criteria increases the likelihood that a factor causes an effect.\n\n\nAssignment: Causal Link between RHI and CTE\n\n\nTiaina Baul “Junior” Seau was an outstanding linebacker who played in the National Football League for 20 years, mostly with the San Diego Chargers and also the Miami Dolphins and New England Patriots. He committed suicide in 2012 by shooting himself in the chest. Junior did not leave a suicide note but a piece of paper with lyrics from the country song “Who I Ain’t”. An autopsy confirmed that Junior Seau had suffered from chronic traumatic encephalopahty (CTE) believed due to repeated head trauma he experienced as a football player.\n\n\n\n\n\n\nFigure 6.16: Junior Seau playing for the New England Patriots. Source: Wikipedia\n\n\n\nNowinski et al. (2022) applied the Hill criteria to establish causality between repeated head impacts (RHI) and CTE. The authors discuss previous research on the association of the two and resistance to calling the link between RHI and CTE causal. Interestingly, it appears that the causal link between the two was settled throughout the 20th century when causality was called into question again. The article is available online.\n\nWhat reasons were given by some of the organizations involved in the debate (like CISG) to resist declaring a causal link between RHI and CTE?\nWhat do you believe was their motivation to do so?\nIn the section Understanding Causation, the article examines each of Hill’s nine criteria. Cite one argument from the article in support of each criterion.\nPer the Discussion section of the article, what are valid reasons why scientists might remain skeptical of a causal link between RHI and CTE?\nWhat language did the authors use in the Conclusion to indicate a causal relationship between RHI and CTE?\nBased on the evidence provided, should the conclusions drawn from data about adult athletes be applied to children? Argue for or against and why or why not?\n\n\n\n\n\n\nFigure 6.1: Relationship between highway fatalities and lemon imports from Mexico.\nFigure 6.2: Positive correlation.\nFigure 6.3: Negative correlation.\nFigure 6.8: Chocolate consumption and number of Nobel laureates.\nFigure 6.14: Experimental layout for poppy experiment.\nFigure 6.15: Ear length. From Heathcote (1995).\n\n\n\nBenson H., Dusek J. A., Sherwood J. B., P. Lam, C. F. Bethea, W. Carpenter, S. Levitsky, et al. 2006. “Study of the Therapeutic Effects of Intercessory Prayer (STEP) in Cardiac Bypass Patients: A Multicenter Randomized Trial of Uncertainty and Certainty of Receiving Intercessory Prayer.” American Heart Journal 151 (4): 934–42.\n\n\nHeathcote, James A. 1995. “Why Do Old Men Have Big Ears?” BMJ 311 (7021): 1668. https://doi.org/10.1136/bmj.311.7021.1668.\n\n\nMesserli, F. H. 2012. “Chocolate Consumption, Cognitive Function, and Nobel Laureates.” New England Journal of Medicine 367: 1562–64.\n\n\nNeyman, Jerzy. 1952. Lectures and Conferences on Mathematical Statistics and Probability. Graduate School, Dept. of Agriculture, Washington.\n\n\nNowinski, Christopher J., Samantha C. Bureau, Michael E. Buckland, Maurice A. Curtis, Daniel H. Daneshvar, Richard L. M. Faull, Lea T. Grinberg, et al. 2022. “Applying the Bradford Hill Criteria for Causation to Repetitive Head Impacts and Chronic Traumatic Encephalopathy.” Frontiers in Neurology 13. https://doi.org/10.3389/fneur.2022.938163.\n\n\nSilver, Nate. 2012. The Signal and the Noise: Why so Many Predictions Fail–but Some Don’t. Penguin Books.\n\n\nSpiegelhalter, David. 2021. The Art of Statistics. How to Learn from Data. Basic Books.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Correlation and Causation</span>"
    ]
  },
  {
    "objectID": "predictions.html",
    "href": "predictions.html",
    "title": "7  Making Predictions",
    "section": "",
    "text": "7.1 Introduction\nIn The Signal and the Noise, Silver (2012, 52) cites a study at the University of Pennsylvania that found when political scientists claim that a political outcome had no chance of occurring, it happened about 15% of the time. And of the absolutely sure things they proclaimed, 25% failed to occur. Now we know that predictions are themselves uncertain, that is why polling results have margins of error. But when you predict that something has a zero chance of happening, then you ought to be pretty confident in that, the margin of error should be small, definitely not 15%.\nThis is an example of a prediction that is not very good.\nIf a plane had a 29% of crashing, you would probably consider the risk of flying too high and stay on the ground. In the run-up to the 2016 presidential election, FiveThirtyEight predicted a 71% chance for Clinton to win the Electoral College and a 29% chance for Trump to win. This was a much higher chance of a Trump victory than the 1% to 15% chance many other models produced. As it turned out, the FiveThirtyEight model was much better than the models that treated the Clinton victory as a near certainty. But both models were wrong about the outcome of the election.\nSilver (2012) points out\nIt is much easier to sift through intelligence after a terrorist attack and to point out what was missed than it is finding the signals in the cacophony of data before the attack.\nIt seems that we make a lot of predictions and are not very good at it. And even if our predictions are spot-on, we might not act on them or ignore them. Predictions that contradict our intuition or preferred narrative can be ignored or explained away. Our personal judgment is not as good as you might think. We tend to overvalue our own opinion and this trend increases the more we know. 80% of doctors believe they are in the top 20% of their profession. More than half of them are clearly wrong!\nSo when a prediction does not come true, does the fault lie with the model of the world or the world itself? If there is a 80% chance of rain tomorrow, then you might see sunny skies. If, in fact, the long run ratio of days that have sunny skies when the forecast calls for an 80% chance of rain is 1 in 5, then the forecast model is correct. We cannot fault the model that calls for a 80% chance of rain for the occasional sunny skies.\nNow compare this scenario to the following. In the build-up of the the 2008 financial crisis, Standard & Poor gave CDOs, a type of mortgage-backed securities, a stellar AAA credit rating, meaning that there is only a 0.0012 probability that they would fail to pay out over the next 5 years (Silver 2012, 20). In reality, 28% of the AAA-rated CDOS defaulted. Had the world of financial markets drastically changed to bring about such a massive change in default rates (200x!)? Or is it more likely that the default models of the rating agencies were wrong? It was the latter.\nWe predict all the time. On the drive to work we choose this route over that route because we predict it has less traffic, fewer red lights, or we are less likely to get caught behind a school bus. We probably make this choice instinctively, without much deliberation, based on experience, instantaneously processing information about the time of day, weather, etc.\nYou might choose Netflix over Paramount+ one evening because you think (predict!) it is more likely that you’ll find content that interests you. This is a prediction problem. You are also on the receiving end of predictions every day. A company offers you a discount because it predicts that without an incentive you might shop at a competitor. We are being served weather forecasts (predictions!) on the local news and apps every day.\nFor the purpose of our discussion here, predicting and forecasting are interchangeable. The predictions that matter here are those derived from modeling data. In other words, we try to make predictive statements about phenomena by quantifying and building computational algorithms. Predictions are at the heart of data processing, whether it is for the purpose of forecasting, classifying, or clustering (grouping).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making Predictions</span>"
    ]
  },
  {
    "objectID": "predictions.html#sec-predict-intro",
    "href": "predictions.html#sec-predict-intro",
    "title": "7  Making Predictions",
    "section": "",
    "text": "Most people fail to recognize how much easier it is to understand an event after the fact when you have all the evidence at your disposal. […] But making better first guesses under conditions of uncertainty is an entirely different enterprise than second-guessing.\n\n\n\n\n\n\n\n\n\n\n\nExercise: Predictions in Real Life\n\n\nPredictions are everywhere. We make them, consciously or subconsciously all the time. The human brain is a highly efficient pattern matching machine and we constantly make predictions about the world based on the patterns we receive. For example, in solving a jigsaw puzzle you match the pattern of pieces not yet placed against the pattern you need. Pieces are evaluated by predicting whether they fit the area you are working on.\n\nList examples where you conduct and/or act on predictions during the day.\nCan you identify examples where someone else’s prediction (a company, a friend, the government, …) is applied to you?\n\n\n\n\n\n\n\n\n\nPrediction and Forecast\n\n\n\nTechnically, a prediction and a forecast are different things. In statistics, a prediction results from the application of a model to data. If the data falls outside of the range of observed training data, then it is referred to as a forecast, in particular when the prediction is about a future event.\nForecasting is also referred to as planning in the presence of uncertainty, taking a systematic, methodological approach. Predicting, on the other hand is any proclamation about things we do not know yet. We predict the outcome of a football game based on gut feeling or allegiance to a team, but we forecast the weather based on meteorological models.\nIn seismology, the distinction between prediction and forecast is taken very seriously. The prediction of an earthquake is a specific statement about when and where it will strike. A forecast, on the other hand is a statement of probability: there is a 60% chance of an earthquake in Northern Italy over the next fifty years. Leaning on this distinction, the U.S. Geological Service party line is that earthquakes cannot be predicted.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making Predictions</span>"
    ]
  },
  {
    "objectID": "predictions.html#bad-predictions",
    "href": "predictions.html#bad-predictions",
    "title": "7  Making Predictions",
    "section": "7.2 Bad Predictions",
    "text": "7.2 Bad Predictions\nIf we are not good at predicting, can we learn from what bad predictions have in common? Silver (2012, 20) lists some attributes of bad predictions:\n\nFocus on the signals that tell a story about the world as we would like it to be, not how it really is.\nIgnore the risks that are most difficult to measure, although they pose the greatest risk to our well-being.\nMake approximations and assumptions that are much cruder than we realize.\nDislike (abhor) uncertainty, even if it is an integral part of the problem.\n\nIf we want to avoid these mistakes with predictions based on modeling data we need to\n\nBuild models that are useful abstractions, not too complicated and not too simple\nFind the signal that the data is trying to convey about the problem under study, allowing for generalization beyond the data at hand.\nBe honest about the quality of the data and the limitations to capture complex systems through quantification.\nQuantify the uncertainty in the conclusions drawn from the model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making Predictions</span>"
    ]
  },
  {
    "objectID": "predictions.html#difficult-to-predict",
    "href": "predictions.html#difficult-to-predict",
    "title": "7  Making Predictions",
    "section": "7.3 Difficult to Predict",
    "text": "7.3 Difficult to Predict\nThis sounds good and is a noble undertaking. Unfortunately, some things are notoriously difficult to predict.\n\nChaotic systems like the weather or economy that resist manipulation. Fortunately, we have much experience in predicting the weather, it is a daily exercise with lots of data to fall back on.\nNoisy systems are difficult to predict because they have a low signal-to-noise ratio. Data collected in the social sciences often suffers from high variability, humans are a very variable bunch.\nComplex systems are those governed by the interaction of many separate individual parts. They can seem at once very predictable and very unpredictable. The laws governing earthquakes are well understood and the long-term frequency of a magnitude 6.5 earthquake in Los Angeles can be estimated well. But we are not very good at predicting earthquake activity. Complex systems periodically undergo violent and highly nonlinear phase changes from orderly to chaotic and back again. Bubbles in the economy and significant weather events such as hurricanes, tornadoes, or tsunamis are examples.\nNonlinear growth systems. When growth is linear we have a good handle on describing and modeling change. When growth is nonlinear, for example, exponential, predicting outcomes is much more difficult. Small deviations in the model today translate into massive discrepancies in the future. Infectious diseases are a good example.\nFeedback systems. In systems with feedback loops the act of predicting can change the system being predicted. Economic predictions can change the way people behave and that can affect the outcome of the prediction itself. Self-fulfilling predictions, where the prediction reinforces the outcome, are common in political polling. A poll showing a candidate surging can cause voters to switch to the candidate from ideologically similar candidates. Or it can make undecided voters to finally get off the fence. A self-cancelling prediction works the opposite way, it undermines itself. When GPS systems became more commonplace, drivers were guided to routes which the systems thought had less traffic. If the systems cannot adjust in real time to the actual traffic density, the guidance can result in more traffic on the suggested routes.\n\nAn example of a self-fulfilling prediction is when increased media coverage of a medical condition leads to increased diagnosis of the condition. Not just because the condition is more prevalent, but because of increased attention people are more likely to identify symptoms and doctors are more likely to diagnose them. The rise of autism diagnoses in the U.S. from 1992 to 2008 correlates highly with the media coverage of autism (Silver 2012, 218).\n\n\nAssignment: Self-fulfilling Predictions\n\n\nDiscuss how increasing police presence in areas where the crime rate is believed to be high is a system with feedback loop. Is it self-fulfilling or self-cancelling?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making Predictions</span>"
    ]
  },
  {
    "objectID": "predictions.html#the-biasvariance-tradeoff",
    "href": "predictions.html#the-biasvariance-tradeoff",
    "title": "7  Making Predictions",
    "section": "7.4 The Bias–Variance Tradeoff",
    "text": "7.4 The Bias–Variance Tradeoff\nRecall the antidotes for bad predictions:\n\nBuild models that are useful abstractions, not too complicated and not too simple.\nFind the signal that the data is trying to convey about the problem under study, allowing for generalization beyond the data at hand.\nBe honest about the quality of the data and the limitations to capture complex systems through quantification.\nQuantify the uncertainty in the conclusions drawn from the model.\n\nLet’s focus on the first two.\n\nA Simulated Data Set\nSuppose that one hundred observations on variables \\(Y\\) and \\(X\\) are sampled from a population. We suspect that the variables are related and wish to predict the difficult-to-measure attribute \\(Y\\) from the easy-to-measure attribute \\(X\\). Figure Figure 7.1 displays the data for the random sample, inspired by Notes on Predictive Modeling by Eduardo García-Portugués at Carlos III University of Madrid.\n\n\n\n\n\n\n\n\nFigure 7.1: 100 observations sampled from a population where \\(Y\\) and \\(X\\) are related.\n\n\n\n\n\nThere is noise in the data but there is also a signal, a systematic change in \\(Y\\) with \\(X\\). How should we go about extracting the signal? Figure 7.2 shows three possible models for the signal.\n\n\n\n\n\n\n\n\nFigure 7.2: Observed data and three possible models for the signal.\n\n\n\n\n\nThe three models differ in their degree of smoothness. The solid (blue) line is the most smooth, followed by the dotted (red) line. The dashed (green) line is the least smooth, it follows the observed data points more closely. The solid (blue) line is probably not a good representation of the signal, it does not capture the trend in the data for small or large values. This model exhibits bias; it overestimates for small values of \\(X\\) and underestimates for large values of \\(X\\). On the other hand, the dashed (green) model exhibits a lot of variability.\nThe question in modeling these data becomes: what is the appropriate degree of smoothness? In one extreme case the model interpolates the observed data points the model reproduces the 100 data points. Such a model fits the observed data really well but you can imagine that it does not generalize well to a new data point that was not used in training the model. Such a model is said to be overfitting the data. The dashed (green) line in Figure 7.2 approaches this extreme.\nIn another extreme case, the model is too rigid and does not extract sufficient signal from the noise. Such a model is said to be underfitting the data. The solid (blue) line in Figure 7.2 is likely a case in point.\n\n\nMore Simulations\nThe concept of model bias and model variability relates not to the behavior of the model for the sample data at hand, although in practical applications this is all we have to judge a model. Conceptually, we imagine repeating the process that generated the sample. Imagine that we draw two more sets of 100 observations each. Now we have 300 observations, 3 sets of 100 each. Figure 7.3 overlays the three samples an identifies observations belonging to the same sample with colors and symbols.\n\n\n\n\n\n\n\n\nFigure 7.3: Three realizations of 100 observations each.\n\n\n\n\n\nThe process of fitting the three models displayed in Figure 7.2 can now be repeated for the two other samples. Figure 7.4 displays the three versions of the solid (blue) model. Figure 7.4 displays the three versions of the dotted (red) model and Figure 7.6 the versions of the dashed (green) model.\n\n\n\n\n\n\n\n\nFigure 7.4: The solid (blue) model fit to each of the three samples.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.5: The dotted (red) model fit to each of the three samples.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.6: The dashed (green) model fit to each of the three samples.\n\n\n\n\n\nComparing the same model type for the three sets of 100 observations, it is clear that the blue model shows the most stability from set to set, the green model shows the least stability (most variability), and the red model falls between the two.\nWe also see now why the highly variable green model would not generalize well to a new observation. It follows the training data too closely and is sensitive to small changes in the data.\n\n\nThe True Model\nSince this is a simulation study we have the benefit of knowing the underlying signal around which the data were generated. This is the same signal for all three sets of 100 observations and we can compare the models in Figure 7.4 through Figure 7.6 against the true model (Figure 7.7 through Figure 7.9)\n\n\n\n\n\n\n\n\nFigure 7.7: The solid (blue) model fit to each of the three samples and the true signal.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.8: The dotted (red) model fit to each of the three samples and the true signal.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.9: The dashed (green) model fit to each of the three samples and the true signal.\n\n\n\n\n\nAgainst the backdrop of the true signal, the solid (blue) and dashed (green) models do not look good. The former is biased, it is not sufficiently flexible to capture the signal. The latter is too flexible and overfits the signal.\n\n\nIn Practice\nThe simulation is unrealistic for two reasons:\n\nWe do not know the true signal in practice, otherwise there would not be a modeling problem.\nWe have only a single sample of \\(n\\) observations and cannot study the behavior of the model under repetition of the data collection process.\n\n\nThe mean squared error\nThe considerations are the same, however. We do not want a model that has too much variability or a model that has too much bias. Somehow, the two need to be balanced. Mathematically, the measure that combines bias and variability is the mean squared error (MSE). Without going into the derivation, we note that the MSE can be decomposed as\n\\[\n\\text{MSE} = \\text{Variance} + \\text{Bias}^2\n\\]\nThe mean squared error is the sum of the variance and the squared bias. And we want the MSE to be small.\nIn statistics, the tension between bias and variance is frequently resolved with the following rationale: we try to avoid biased estimators and require that models are unbiased. Then, among the choices of unbiased models, we pick the one that has the smallest variability. That is a reasonable approach, but it is possible that a model with some bias has a lower MSE if its variability is much lower. In other words, we might find a model that performs better–in the sense of a lower MSE–by allowing some bias if the model has low variability.\nWhen dealing with a single sample we can generally connect the concepts of bias and variability of a model to the flexibility of the model. A model that is highly flexible and can follow the data closely tends to have low bias and high variability. On the other hand, a model that is rigid (inflexible) tends to exhibit high bias and low variability. The solid (blue) model falls into the high-bias-and-low-variability category and the dashed (green) model falls into the low-bias-and-high-variability category.\nFinding models that exhibit a small MSE by having low bias and moderate variability is known as the bias–variance tradeoff in data science.\n\n\nA further complication\nIt seems that finding a good predictive model then boils down to minimizing the mean squared error. Based on a set of data and a model, the mean squared error can be estimated as the average squared deviation between the observed values and the values predicted under the model: \\[\n\\widehat{\\text{MSE}} = \\frac{1}{n} \\sum (\\text{Observed} - \\text{Predicted})^2\n\\tag{7.1}\\]\nThe notation \\(\\widehat{\\text{MSE}}\\), placing a “hat” (caret) on top of a statistic, means that this is an estimate of a quantity. We cannot typically calculate the real MSE of a model, it depends on parameters we do not know. But we can compute an estimate of the MSE based on Equation 7.1.\nSo it seems that we can overcome the problem of not knowing the real MSE by estimating it. However, there is one more complication. As the flexibility of the model increases, the difference between observed and predicted values can be made arbitrarily small. A model that interpolates the data points has an estimated MSE of zero. If we were to choose to minimize Equation 7.1, we would end up with highly variable models. And this defeats the purpose.\nThe remedy is to compute the estimated MSE not by comparing observed and predicted values based on the values in the training data set, but to predict the values of observations that did not participate in training the model. This test data appears to the algorithm as new data it has not seen before and is a true measure for how well the model generalizes:\n\\[\n\\widehat{\\text{MSE}}_\\text{Test} = \\frac{1}{m} \\sum (\\text{Observed} - \\text{Predicted})^2\n\\tag{7.2}\\]\nThe main difference between Equation 7.1 and Equation 7.2 is that the former is computed based on the \\(n\\) observations in the training data and the latter is computed based on \\(m\\) observations in a separate test data set. We refer to mean squared errors calculated on test data sets as the test error.\n\n\nCross-validation\nBecause it is expensive to collect a separate data set to compute the test error the collected data is often split randomly into a training set and a test set. For example, you might use split the data 50:50 or 80:20 or 90:10 into training:test sets. If you have a lot of data, then splitting into separate training and test sets is reasonable.\nThere is a clever way in which the same observations can be used sometimes for training and sometimes for testing, that uses the collected information more economically. This is called cross-validation.\nDuring cross-validation, an observation is either part of the test set or part of the training set. The process repeats until each observation was used once in a test set. One technique of cross-validation randomly assigns observations to one of \\(k\\) groups, called folds. Figure 7.10 is an example of creating 5 folds from 100 observation for 5-fold cross-validation.\n\n\n\n\n\n\nFigure 7.10: Example of 5-fold cross-validation for 100 observations. Numbers in the cells represent observation numbers. The records were randomly arranged prior to assigning the folds.\n\n\n\nThe numbers in the cells of Figure 7.10 are the observation numbers. For example, the first fold includes observations #4, #16, #90, etc. The second fold includes observations #31, #94, #38, etc.\nThe process of calculating the test error through \\(k\\)-fold cross-validation is as follows:\n\nSet aside the data in the first fold. It serves as the test data.\nFit the model to the remaining data.\nUsing the model trained in step 2., predict the values in the test data and calculate the test error\nReturn the set-aside fold to the training data and set aside the next fold as the test data. Return to step 2. and continue until each fold has been set aside once.\n\nAt the end of the \\(k\\)-fold cross-validation procedure, the model has been trained \\(k\\) times, and we have \\(k\\) estimates of the test error, one for each of the folds. These test errors are then combined into one overall test error.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making Predictions</span>"
    ]
  },
  {
    "objectID": "predictions.html#putting-it-all-together",
    "href": "predictions.html#putting-it-all-together",
    "title": "7  Making Predictions",
    "section": "7.5 Putting It All Together",
    "text": "7.5 Putting It All Together\nLet us now put together everything we have covered in this chapter about building a predictive model based on data for the simulated data set in Figure 7.1 using computational and quantitative thinking.\n\nWe build a model that captures the relationship between \\(Y\\) and \\(X\\).\nThe result should be an algorithm that we can use to predict \\(Y\\) based on values of \\(X\\) (\\(Y\\) is the target variable, \\(X\\) is the input variable).\nThe model should generalize well to new observations, that is, it should not overfit or underfit. Use cross-validation to determine the appropriate amount of flexibility in the model.\n\n\nCreating the Data (Quantification)\nThe following R code creates the data frame shown in Figure 7.1.\n\n1set.seed(12345)\n2n &lt;- 100\n3eps &lt;- rnorm(n, sd = 2)\n4X &lt;- rnorm(n, sd = 2)\n5Y &lt;- X^2 * cos(X) + eps\n\n6simData &lt;- data.frame(X=X, Y=Y)\n\n\n1\n\nSetting a seed for the random number generator ensures that the program generates the same values every time it is run. The value for the seed is chosen here as 12345 and can be any integer.\n\n2\n\nThe number of observations drawn is set to \\(n = 100\\).\n\n3\n\neps is a vector of Gaussian (normal) random variables with mean 0 and variance 4 (std. dev 2). This represents the noise in the system.\n\n4\n\nThe values of \\(X\\) are drawn randomly from a Gaussian distribution with mean 0 and variance 4\n\n5\n\nThe signal is \\(x^2 \\, \\cos(x)\\). The noise (eps) is added to the signal.\n\n6\n\nA data frame is constructed from X and Y.\n\n\n\n\n\n\nCross-validating the Model\nThe steps of training the model and selecting the best flexibility by cross-validation can be done in a single computational step using the train function in the caret package. We need to decide on the general family of model we are going to entertain. Here we choose what is known as a regression spline. These model the relationship between two variables and their flexibility is governed by a single parameter, called the degrees of freedom of the model. With increasing degrees of freedom the models become more flexible. Cross-validation is used to determine the best value for the degree of freedom parameter in this model family.\n\n1library(caret)\n\n2cv_results &lt;- train(Y ~ X,\n3                    data     =simData,\n4                    method   =\"gamSpline\",\n5                    tuneGrid =data.frame(df=seq(2, 25, by=1)),\n6                    trControl=trainControl(method=\"cv\", number=10))\n\n\n1\n\nThe caret library is loaded into the R session. If caret is not yet installed in your system, execute the command install.packages(\"caret\") once from the Console prompt.\n\n2\n\nThe train() function in the caret library is called. Y ~ X specifies the model we wish to train. The target variable is placed on the left side of the ~ symbol and the input variable is placed on the right side.\n\n3\n\nThe data frame where the variables in the model specifications can be found.\n\n4\n\nThe method= parameter specifies the model family we wish to train. gamTrain is the specification for the regression spline family.\n\n5\n\nThe tuneGrid parameter provides a list of the values to be considered in the cross-validation. We vary only the degrees of freedom parameter (df) of the gamSpline function and we evaluate all values from 2 to 25.\n\n6\n\nThe trControl= parameter specifies the computational nuances of the train function. Here we choose 10-fold cross-validation.\n\n\n\n\nAfter running the code above we can examine the results:\n\nprint(cv_results)\n\nGeneralized Additive Model using Splines \n\n100 samples\n  1 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 90, 91, 89, 89, 90, 89, ... \nResampling results across tuning parameters:\n\n  df  RMSE      Rsquared   MAE     \n   2  3.948383  0.2468568  3.021822\n   3  3.583394  0.4016653  2.751121\n   4  3.264671  0.5176447  2.559835\n   5  2.967739  0.5996185  2.371416\n   6  2.732739  0.6454729  2.218233\n   7  2.573920  0.6688788  2.107661\n   8  2.474458  0.6808445  2.036267\n   9  2.413180  0.6873204  2.001114\n  10  2.375289  0.6911981  1.983925\n  11  2.351995  0.6938019  1.976452\n  12  2.338419  0.6957038  1.971334\n  13  2.331878  0.6970928  1.970695\n  14  2.330844  0.6979920  1.971182\n  15  2.334393  0.6984189  1.972935\n  16  2.342029  0.6983416  1.975561\n  17  2.353220  0.6978141  1.978418\n  18  2.367892  0.6968446  1.981459\n  19  2.385770  0.6954992  1.984928\n  20  2.406903  0.6938330  1.990501\n  21  2.431180  0.6919134  1.998484\n  22  2.458785  0.6897954  2.007296\n  23  2.489828  0.6875293  2.017593\n  24  2.524311  0.6851669  2.028585\n  25  2.562596  0.6827171  2.041443\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was df = 14.\n\n\nThe output lists the values of the cross-validation parameter (df) along with three statistics computed for each. The statistic of interest to us is RMSE, the cross-validation root mean square error, the square root of our measure of test error. Note that caret reports the square root of the \\(\\widehat{\\text{MSE}}_\\text{Test}\\). In order to find the df with the smallest test error it does not matter whether we look at \\(\\widehat{\\text{MSE}}_\\text{Test}\\) or its square root. The minimum is achieved at the same value.\nWe can see that the smallest value in the RMSE column occurs at df=14. This is confirmed in the sentence at the bottom of the output.\nIt is customary to graph the cross-validation criterion (RMSE) against the values of the parameter. Figure 7.11 shows a typical pattern. The test error decreases to an optimal value and increases afterwards.\n\nplot(cv_results)\n\n\n\n\n\n\n\nFigure 7.11: Cross-validation root mean squared error as a function of model flexibility (degrees of freedom).\n\n\n\n\n\n\n\nTraining the Final Model\nOne final step remains. Now that we have chosen df=14 as the optimal parameter for this combination of data set and class of model, we train the model with those degrees of freedom on all the data to obtain the final model.\n\nfinal_model &lt;- train(Y ~ X,                   \n                     data     =simData,       \n                     method   =\"gamSpline\",   \n1                     tuneGrid =data.frame(df=cv_results$bestTune[1]),\n2                     trControl=trainControl(method=\"none\"))\n\n\n1\n\nInstead of a list of df values, we now pass only one value. The best value from cross-validation was stored automatically in the bestTune field of the result object in the previous step.\n\n2\n\nInstead of cross-validation we request none as the training nuance. train will simply train the requested model on the data frame.\n\n\n\n\n\n\nMaking a Prediction\nSuppose we want to predict \\(Y\\) (or more precisely the mean of \\(Y\\)) for a series of \\(X\\) values, say from \\(-5\\) to \\(6\\).\nComputing the predicted values is easy with the predict function in R. You simply pass it the model object and a data frame with the values for which you need predictions.\n\n1xGrid &lt;- data.frame(X=seq(-5, 6, length.out = 250))\n2predvals &lt;- predict(final_model,newdata=xGrid)\n\n3predvals[1:10]\n\n\n1\n\nCreate a data frame with 250 values for \\(X\\) ranging from \\(-5\\) to \\(6\\).\n\n2\n\nCompute the 250 predicted values.\n\n3\n\nDisplay the first 10 predicted values.\n\n\n\n\n        1         2         3         4         5         6         7         8 \n-2.658016 -2.947193 -3.236370 -3.525547 -3.814724 -4.103901 -4.393078 -4.682255 \n        9        10 \n-4.971432 -5.260609 \n\n\nAt this point we would like to see how the predicted values of the signal compare to the observed noisy data. Figure 7.12 overlays the predicted values and the data. Also displayed is the true signal. The model derived from the data does an excellent job capturing the signal without following the data points too closely or being too inflexible.\n\nplot(simData$X, simData$Y, \n     type=\"p\", \n     las =1,\n     bty =\"l\",\n     xlab=\"X\",\n     ylab=\"Y\")\nlines(xGrid$X,predvals,col=\"red\",lwd=2)\nlines(xGrid$X,m(xGrid$X),col=\"black\",lwd=2,lty=\"dashed\")\nlegend(\"topleft\",\n       legend=c(\"Cross-validated\",\"True\"),\n       lty   =c(\"solid\",\"dashed\"),\n       col   =c(\"red\",\"black\"),\n       lwd   =2)\n\n\n\n\n\n\n\nFigure 7.12: Predictions of the cross-validated spline model and observed data.\n\n\n\n\n\n\n\nQuantifying Uncertainty\nRecall the fourth antidote against bad predictions:\n\nQuantify the uncertainty in the conclusions drawn from the model.\n\nConceptually, we think of the bias and variability of the model under repetition of the data collection. In an earlier section we looked at models for three data sets of 100 observations each. There are several factors contributing to our uncertainty about the predicted values in this analysis.\nWe are not sure whether the regression spline is the correct model family to capature the relationship between \\(Y\\) and \\(X\\). Although the results of the cross-validation and the comparison with the true signal (which in practice we would not know) give us comfort that this model family does a good job here.\nThe second source of uncertainty comes from the fact that there is noise in the data. As we saw earlier, another set of data drawn from the same process gives slightly different data points that lead to a different best model. Fortunately, we can quantify this uncertainty by computing confidence intervals for the predicted values.\nA 95% confidence interval for a parameter is a range that with 95% probability will cover the value of the parameter. For example, a 95% confidence interval for the predicted value at \\(X=2.5\\) is the range into which the mean value of \\(Y\\) will fall in 95% of the sample repetitions.\nThe predict functions in R can compute the basic ingredients for confidence intervals for many models. Unfortunately, the predict.train function invoked by caret cannot do this. However, since we know that we fit a gamSpline model, and the predict.Gam function does compute the uncertainty of the predicted values for the values in the training data, we can repeat the final model fit with gam and use its predict function:\n\n1library(gam)\n\n2final &lt;- gam(Y ~ s(X,df=14), data=simData)\n\n3predvals &lt;- predict(final,se.fit=TRUE)\n\n4round(predvals$fit[1:10],4)\n\n5round(predvals$se.fit[1:10],4)\n\n\n1\n\nLoad the gam library.\n\n2\n\nFit the final model with the gam function. Y ~ s(X,df=14) is the formulation for a regression spline model with 14 degrees of freedom.\n\n3\n\nCompute the predicted values and request that the standard errors of the predicted values are also computed.\n\n4\n\nThe predicted values for the first 10 observations.\n\n5\n\nThe standard errors of the predicted values for the first 10 observations.\n\n\n\n\n      1       2       3       4       5       6       7       8       9      10 \n 0.2388 -2.4706  0.5433 -5.7746  0.5378 -0.0852  0.4096 -9.5366  0.4262  0.0101 \n [1] 0.6535 0.7064 0.7128 0.8453 0.6418 0.6865 0.7142 0.8642 0.5838 0.6757\n\n\nFor example, the first predicted value is 0.2388 with a standard error of 0.6535.\nTo compute the 95% confidence interval for the predicted values, add and subtract qnorm(0.975) times the standard error from the predicted value. This is done in the following code and the results are plotted.\n\npred_df &lt;- data.frame(X=simData$X, \n                      pred=predvals$fit,\n                      se=predvals$se.fit)\npred_df_sorted &lt;- pred_df[order(pred_df$X),]\npred_df_sorted$ci_up &lt;- pred_df_sorted$pred + qnorm(0.975)*pred_df_sorted$se\npred_df_sorted$ci_lo &lt;- pred_df_sorted$pred - qnorm(0.975)*pred_df_sorted$se\n\nplot(simData$X, simData$Y, \n     type=\"p\", \n     las =1,\n     bty =\"l\",\n     xlab=\"X\",\n     ylab=\"Y\")\nlines(xGrid$X,m(xGrid$X),col=\"black\",lwd=2,lty=\"dashed\")\nlines(pred_df_sorted$X,\n      pred_df_sorted$pred,col=\"red\",lwd=2)\nlines(pred_df_sorted$X,\n      pred_df_sorted$ci_up,col=\"blue\",lty=\"dotted\",lwd=1.5)\nlines(pred_df_sorted$X,\n      pred_df_sorted$ci_lo,col=\"blue\",lty=\"dotted\",lwd=1.5)\n\n\n\n\n\n\n\nFigure 7.13: Predictions and confidence intervals of the cross-validated spline model and observed data. The black line is the true signal in the data.\n\n\n\n\n\nThe confidence intervals (dotted blue) are tracing the predicted values and are narrower in the center of the \\(X\\)-data than near the edges. The further out we move the more uncertain our predictions will be. The dashed black line shows the true signal in the data, the 95% confidence interval covers it.\n\n\n\nFigure 7.1: 100 observations sampled from a population where \\(Y\\) and \\(X\\) are related.\nFigure 7.2: Observed data and three possible models for the signal.\nFigure 7.3: Three realizations of 100 observations each.\nFigure 7.4: The solid (blue) model fit to each of the three samples.\nFigure 7.5: The dotted (red) model fit to each of the three samples.\nFigure 7.6: The dashed (green) model fit to each of the three samples.\nFigure 7.7: The solid (blue) model fit to each of the three samples and the true signal.\nFigure 7.8: The dotted (red) model fit to each of the three samples and the true signal.\nFigure 7.9: The dashed (green) model fit to each of the three samples and the true signal.\nFigure 7.12: Predictions of the cross-validated spline model and observed data.\nFigure 7.13: Predictions and confidence intervals of the cross-validated spline model and observed data. The black line is the true signal in the data.\n\n\n\nSilver, Nate. 2012. The Signal and the Noise: Why so Many Predictions Fail–but Some Don’t. Penguin Books.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making Predictions</span>"
    ]
  },
  {
    "objectID": "life_algorithms.html",
    "href": "life_algorithms.html",
    "title": "8  Algorithms to Live By",
    "section": "",
    "text": "8.1 Introduction\nIn the introduction we defined an algorithm as a set of repeatable step-by-step instructions to solve a particular problem. Algorithms are not necessarily the machinations of computer scientists and mathematicians—recall making pumpkin soup. We design and apply algorithms in every day life. You probably have a personal algorithm or two for brushing teeth or making the bed.\nAlgorithms are designed to perform a task in some optimal way. My dog applies the above algorithm to achieve the optimal sleeping arrangement—by his standard. Mathematicians and computer scientists have developed many algorithms to solve certain problems optimally. If you have to put five books on a shelf in order, you might not worry much about the sorting algorithm. When you have to sort 100,000 items choosing between a bubble, quick, or merge sort matters much more.\nQuestion 1: Can we apply what mathematicians and computer scientists know about optimal algorithms to solve problems in everyday life? If you were searching for an apartment in San Francisco and finally sign a lease you might not be happy with the outcome; maybe there were better apartments and better deals out there. If you went about the search in an optimal way, then you can live with the outcome knowing that the process maximized your probability of finding the best apartment. The algorithm defines the best process although it does not guarantee the best outcome.\nQuestion 2: What can we learn from how computer scientists think about algorithms about how to design systems and make decisions in every day life? Computer scientists study the complexity of algorithms, trying to reduce the computational burden of a solution. Can that inform how we interact with each other? Are you more likely to land a successful meeting invite on the calendar if you ask the meeting participants which time works best for them or if you tell them “next Wednesday at 11:00 am”? It turns out it is the latter, because it presents a constrained solution which has a lower cognitive burden than “which time is most convenient for you?”.\nThe discussion that follows draws on the book Algorithms to Live By by Christian and Griffiths (2017).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Algorithms to Live By</span>"
    ]
  },
  {
    "objectID": "life_algorithms.html#introduction",
    "href": "life_algorithms.html#introduction",
    "title": "8  Algorithms to Live By",
    "section": "",
    "text": "Getting into bed: a dog’s perspective\n\n\n\n\n\nMy dog has developed an elaborate algorithm for nights when he wants to sleep in the bed. He starts out sleeping in his dog bed on the floor and the algorithm goes something like this:\n\nGet up and pretend there is something outside that needs his attention: a squirrel in a tree, a raccoon on the porch, a deer in the yard, etc.\nMake sure that I understand the severity of the threat and follow him to the living room. This can involve growling at the door to the deck, running back and forth between living room and bed room.\nI finally get up, follow him into the living room and open the door to the deck.\nAt this point he pretends to realize that there is no threat after all and we can go back to bed.\nHe follows me into the bedroom, positioning himself next to the bed in the unmistakable posture that says “lift me up on the bed please.”\n\nTo his credit, he could jump on the bed by himself and is going through this routine as a means to get permission. The algorithm is complicated, but it is a repeatable step-by-step recipe, and it works every time.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.1: Trust the process.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Algorithms to Live By</span>"
    ]
  },
  {
    "objectID": "life_algorithms.html#optimal-stoppingthe-37-rule",
    "href": "life_algorithms.html#optimal-stoppingthe-37-rule",
    "title": "8  Algorithms to Live By",
    "section": "8.2 Optimal Stopping–The 37% Rule",
    "text": "8.2 Optimal Stopping–The 37% Rule\nSuppose you just landed a job at a Silicon-Valley startup company and are locating to San Francisco. Finding an apartment in San Francisco can be a rough experience: there is a lot of demand for a short supply. As a newcomer to town you do not have a good feeling for the market. What do typical apartments in your price range look like? What is available on the market?\nHere is the dilemma: you cannot procrastinate much when you see an available apartment; they go very fast. But if you choose the first available apartment you will miss out on better ones that you have not seen yet. If you pass on the first available apartment there is no guarantee that the next ones will be any better. But you need to look at least at a few of them to get a feeling for what is on the market. How else could you make an informed decision? But if you keep looking at apartments and pass on them you might end up with no apartment at all or you have to choose from what is left on the market. Is looking at 2 apartments enough? 20?\nThere are two ways in which you can fail: stop looking at apartments too early and stop looking too late. When you stop too early a better apartment goes undiscovered. If you stop too late you hold out for a better apartment that does not exist, you have passed on the best possible apartment already.\nThis is known as an optimal stopping problem. How much effort should you spend on looking at choices and when should you leap and make a decision?\nIt turns out that there is an answer. The optimal decision rule is to spend 37% of your budget on calibrating and looking at apartments to establish a standard. Then pick the next apartment that beats the standard. If your budget is one month to find an apartment, then you should look and calibrate for 11 days.\nWe can thank computer science for having an optimal solution to balance overthinking and impulse decision. No more analysis paralysis if you apply the 37% rule. You can see how the algorithm could be applied to other situations in real life:\n\nHow much time should you spend circling a parking lot before committing to a spot in the hope that a better spot opens up?\nHow many offers should you reject before selling the car or house in the hope to get a better offer?\nHow long should you be dating before finding the “optimal” partner?\n\nThe 37% rule is not optimal in all decision problems. Situations where the rule applies are characterized by the following:\n\nYou have to choose a singleton (one apartment, one parking spot, one partner).\nThere are \\(n\\) possible choices and \\(n\\) is known.\nIt is possible to rank all \\(n\\) choices from best to worst.\nThe choices appear sequentially in a random order.\nAfter meeting a choice, a decision is made immediately to reject or accept it.\nA choice cannot be revisited after rejection or acceptance.\nIf you come to the final (\\(n\\)th) choice you have to take it.\n\nThe optimum achieved by the 37% rule is to maximize the probability to select the best choice among the \\(n\\) possibilities. The rule does not guarantee that the best choice is made. But no other rule has a higher probability of finding the best choice under the circumstances.\n\nThe Secretary Problem\nThe optimal stopping problem is also known as the secretary problem, the sultan’s dowry problem, the fussy suitor problem or the marriage problem. In terms of hiring a secretary, it goes as follows.\nYou want to hire the best secretary out of \\(n\\) applicants for a position. Applicants are interviewed in a random order one by one. After the interview you decide whether to accept or reject the candidate. If you reject an applicant they take another job and cannot be reconsidered. While interviewing the applicant you gather information that allows you to rank the candidate against those interviewed so far but you do not know the quality of the yet to-be-interviewed candidates.\nThe 37% rule states that you should be evaluating the first 37% applicants to create a ranking. That completes the looking phase. Then leap and make an offer to the next applicant that ranks higher than the best applicant interviewed during the looking phase.\nBut wait, what if there is only one applicant? Or two?\nWith a single applicant you need to hire them, and you are guaranteed to have hired the best of the (single) bunch. With two applicants, if you pass on the first, you have to hire the second. In this case you can flip a coin, either hire the first or pass and be forced to hire the second—you cannot do better than chance. With \\(n=3\\) candidates the optimal rule is to look at the first candidate, then choose the second candidate if they beat the first. This is a 1/3 x 100% = 33.3% rule and has a 50% probability of finding the best candidate.\nAs \\(n\\) increases, the optimal rule approaches \\(1/e \\times 100\\% = 36.8\\%\\) and that is also the probability of finding the best candidate. Round that percentage up and you see why it is called the 37% rule.\nYou might say that a 37% chance of making the best choice is not very good. In 63% of all cases we are not finding the best candidate. The result is not that bad if you compare it against a pure random chance. If there are 100 candidates for the position, then a random choice has a 1% chance of getting the best candidate. In a pool of 1,000 candidates, random selection has a 0.1% chance of locating the best. However, the odds of finding the best candidate under the optimal stopping rule does not change. The more choices you have, the better optimal stopping performs relative to random selection. But even with only three candidates the likelihood of finding the best candidate has improved from 1 out of 3 to 1 out of 2.\n\n\nIssues with the Optimal Stopping Rule\nBut wait, you say. Pure random selection is not the appropriate benchmark against which to compare the optimal stopping rule. This is not how we hire people in the real world. In other words, the situation where the 37% rule applies does not describe how we do things.\nWhat are some of the ways in which the optimal stopping setup is unrealistic:\n\nWe often do not know \\(n\\), the number of choices. For example, we might not know how many apartments in San Francisco are on the market. There is a workaround: if \\(n\\) is unknown, we can base the 37% rule on a time interval: we give ourselves one month to find an apartment and enter the leap phase after 10 or 11 days.\nThe choices do not come to us in a random order. In an interview process there are short lists and candidates are screened and filtered. The order in which candidates come to the interview is not completely at random.\nAfter rejecting a candidate, we do have the option to go back. If it turns out that none of the other candidates was better than a candidate rejected earlier, then we can go back and reconsider the choice. After rejecting a date, we can ask them on another date—theoretically.\nIn real life, we are not making decisions after seeing a candidate. Instead, applicants are grouped (pre-screening, initial phone interview, on-site interview, …) and choices are made after each batch of candidates.\nThe optimal stopping problem assumes that we do not have a ranking of the candidates. It only assumes that there is an objective method of ranking them and that we rank as we go. Obviously, candidates are evaluated based on many criteria and these are known ahead of time. The look phase of the optimal stopping exists because it is needed to calibrate what good candidates look like. We know this a priori of time, and we can evaluate candidates (somewhat) based on their resumes.\nWe do not only have a sense of which candidate ranks higher or lower, but also by how much. Meeting a candidate that really stands out changes the likelihood of making a decision. If you know that a candidate is in the top 10% of SAT or GRE scores, there is only a 1 in 10 chance that others will be better.\nThe job will be offered to a candidate in the leap phase, not the looking phase. The Human Resources and the Legal departments are probably going to have a conniption if the first 37% of candidates being interviewed have no chance of getting the job. Queue the lawsuits. Your dates might not respond kindly when they find out that you are “just looking”.\n\n\nThe optimal stopping scenario and the 37% rule are useful to construct a mental model of how to go about a decision that involves sequentially evaluating choices and deciding on a particular choice. As with any model, we have to examine the assumptions and conditions under which the model applies. We quickly see that the assumptions do not map cleanly to real-life situations such as hiring, finding a partner, or even circling a parking lot for the best empty space.\nRecall from Section 4.2.3.4 the famous G.E.P. Box quote\n\nAll models are wrong. Some are useful.\n\nThe utility of casting real-life decision situations in terms of an optimal stopping problem lies in understanding that under special circumstances there is an optimal way to balance impulse and procrastination and to realize in what ways our decision situation deviates from that scenario. Are we in a no-information scenario as in the classical secretary problem where we have no a-priori data about the quality of the applicants or are we in a full-information scenario where the quality of all applicants can be quantified a priori? If it is the latter we should come to a decision more quickly than going through at least 37% of the stack.\nA situation where the algorithm applies cleanly to a real-life situation is when we are placing things in order—the task of sorting. The question then is what is the most efficient way to go about it?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Algorithms to Live By</span>"
    ]
  },
  {
    "objectID": "life_algorithms.html#searching-and-sorting",
    "href": "life_algorithms.html#searching-and-sorting",
    "title": "8  Algorithms to Live By",
    "section": "8.3 Searching and Sorting",
    "text": "8.3 Searching and Sorting\nSearching means finding things in a collection. Sorting means arranging a collection in order. Both are fundamental tasks in data processing. Sorting was one of the first applications for computers, making sense of the 1890 census. The census takes place every ten years and with increasing population and more questions asked on each census the burden of counting up the answers was growing. Doing this by hand was getting tricky. Tallying the previous census by hand took 8 years, just enough to finish the task before the 1890 census (Christian and Griffiths 2017). Something needed to be done, the 1890 census could not be tallied by hand in time for the 1900 census. Enter the Tabulator, a punch card-based machine invented by Herman Hollerith and adapted for the 1890 census (Figure 8.2).\n\n\n\n\n\n\nFigure 8.2: Tabulator with sorting machine invented by Herman Hollerith. Source: Wikipedia\n\n\n\nSorting was one of the most important tasks for computers. Christian and Griffiths (2017) state\n\nBy the 1960s, one study estimated that more than a quarter of the computing resources of the world were being spent on sorting.\n\nSorting is still at the heart of many algorithms. Every time you see a ranking, a largest or smallest value, a 95th percentile, a leader board or a top-10, a list had to be arranged by value, sorted at least partially.\n\n\nExercise: Find Examples of Sorting\n\n\nCan you think of some ways in which you encounter sorted things every day? Here are examples to get you started:\n\nSports teams are ranked by criteria such as number of wins and losses, and displayed from highest to lowest.\nMusic is ranked by popularity, measured as number of downloads, sales, etc.\nMusic is grouped (sorted) by genre.\nStreaming services recommend things to watch according to an algorithm that ranks shows based on your watch history.\nDisplays of countries are sorted by all kinds of attributes. The medal count in the Olympic Games, gross domestic product (GDP), annual oil production, emissions, etc.\nA Google search returns the search results sorted according to relevance. The relevance is determined through a number of factors such as Google’s famed PageRank algorithm, which sites are sponsored, and so forth. Without sorting the search results you would have to sift through millions of links to determine which ones are most relevant to your search. The ranking is very effective, less than 1% of Google searches scroll to the second page.\n\n\n\n\n\nThe Search–Sort Tradeoff\nIn business, economy of scales is the achievement of cost advantages through increase of production (increase of scale). By producing more items the fixed cost of production such as building a factory are spread across a larger number of items, reducing the fixed cost per item. Striving for greater scale and scalability is an important business and technology driver. With searching and sorting, however, size is kryptonite. From Christian and Griffiths (2017):\n\nSorting involves steep diseconomies of scale, violating our normal intuitions about the virtues of doing things in bulk. Cooking for two is typically no harder than cooking for one, and it’s certainly easier than cooking for one person twice. But sorting, say, a shelf of a hundred books will take you longer than sorting two bookshelves of fifty apiece: you have twice as many things to organize, and there are twice as many places each of them could go. The more you take on, the worse it gets.\n\nThat begs the question: should we even sort at all? One reason for sorting is to facilitate searching. Since the books of the library are sorted on shelves, knowing that you are in the QA section makes it easy to search for a book with a QB call number. Could we find the QB book if the library arranges books in a random order? It depends on how fast we can search the library.\n\nSorting something that you will never look at and you will never search is a complete waste of time and resources.\nIf searching is fast then you should reduce the amount of effort spent on sorting. Maybe partially sorted lists (a bucket for all names starting with letters A-D, a bucket for all names starting with E–H, …) are sufficient.\nIf searching is really fast, we might not benefit from sorting first at all.\n\nFor the public library or the university library, the tradeoff between sorting and searching is resolved by sorting the entire catalog. For the books in your home sorting as a preemptive strike for fast searching is probably not necessary. The time it takes you to scan a familiar bookshelf or two does not justify the time spent arranging the books alphabetically.\nA classic example of the tradeoff is how we organize our email. There are basically two extreme approaches with many nuances in between. The first approach is to leave everything in your Inbox, your email is completely unorganized. The other extreme is to create a meticulous system of folder and sub folders (or labels) to classify email messages you keep. Which system is better in terms of finding an email message you received previously?\nSearching electronic items is many times faster than searching through physical books in the library. The cost of searching even through a massive Inbox is tiny these days. When it comes to locating an email, sorting the messages first and arranging them in folders is a waste of time. Clicking through the folders to locate an email takes more time than locating it with a search. There are other benefits of grouping and arranging messages, such as archiving or not being overwhelmed by the look of an Inbox out of control. Interestingly, all mail clients cite organizing email in folders or labels as a way to make it easier to find the information you need.\n\n\nSorting Algorithms\nSorting suffers from diseconomies of scale. Sorting larger lists is much harder than sorting shorter lists. The development of sorting algorithms that can handle large lists efficiently has a long history in computer science. Let’s look at a few common sorting algorithms.\n\nBubble sort\nThe bubble sort is a simple and intuitive sorting algorithm. It is also very inefficient compared to other algorithms. In computer science, the bubble sort is a punching bag of sorts. Don’t get caught bubble sorting data unless you are told to do so.\nSuppose you scan the list of items you wish to sort. If you find two items that are in the wrong order you change their positions. When you come to the end of the list, start over at the beginning of the list. Once you reach the end of the list and there were no pairs of items out of order the sort is complete.\nFigure 8.3 is an example of the first pass through the data from Sehgal (2018). Sorting the list will require one more pass through the data, swapping the “5” and “4” in the second and third positions.\n\n\n\n\n\n\nFigure 8.3: Bubble sort example; first pass through the data. Source.\n\n\n\n\n\nExercise: Sort Students by Birthday\n\n\nAll but one students in class write their birthdays on a Postit note and attach it to their clothing so it is visible from the front. The students then arrange themselves in a line around the classroom and the student who was left out operates as the bubble sorter.\nThe goal is to arrange the students by birthday from youngest to oldest.\n\nHow many passes through the class did you have to make until the students were sorted?\nWere there any ties (students with the same birthday) and if so, how did you handle them?\nHow many times did you swap the places of two students in the line?\nIf the class had twice as many students, how much longer would it take to sort the students by birthday this way?\nDid you incorporate any prior knowledge about birthdays in forming the initial line of students, for example, knowledge that one of the students is considerably older or younger than the rest?\n\n\n\n\n\nInsertion sort\nInstead of arranging all items in a list and making individual swaps, we can start with an empty list and add items to it one by one. Suppose you want to arrange the items in ascending order. Pick the first item and consider it the middle of the list. If the next item is smaller, put it before the first item, otherwise place it after the first item. Starting with the third item you run through the existing list (which now has two items in it) from top to bottom and place the new item between the two items that bracket its value. You continue this process until all items have been placed on the list.\nThis procedure is called the insertion sort. It is also very intuitive and more efficient than the bubble sort but not nearly as efficient as more advanced (and less intuitive) sorting algorithms.\nSuppose you with to sort the list {3, 7, 4, 9, 5, 2, 6, 1} in ascending order by insertion sort.\nStep 1. You start with the first element, 3. It is now the only element of the sorted list: {3}\nStep 2. The next element under consideration is 7. Since it is larger than 3 the sorted list is now {3, 7}\nStep 3. The next element under consideration is 4. Starting from the top of the list, the value slots in between 3 and 4: {3, 4, 7}\nStep 4. The next element under consideration is 9. Starting from the top of the list you make it all the way to the end before it slots into place: {3, 4, 7, 9}.\nAnd so on.\n\n\nExercise: Sort Students by Birthday\n\n\nRepeat the previous exercise arranging students in a classroom by birthday. This time apply the insertion sort. Start from students sitting in their seats and ask them to come to the front of the class one by one. As students come to the front they represent the next element inserted into the list. Students move from the front of the existing list toward the end until they find the position where there birthday slots in.\n\n\n\n\nMergesort\nBubble and insertion sorts work, they produce ordered lists. Unfortunately, these algorithms are not time efficient, meaning that they require many more operations (comparisons) than is necessary to sort a list. As the size of the list grows, these algorithms become untenable. A different approach is needed.\nMergesort (or Merge sort) is an example of a divide-and-conquer algorithm. Such an algorithm breaks a larger problem down into smaller sub-problems, solves each sub-problem by itself and combines the results into the solution for the overall problem. Divide the problem into smaller parts, conquer each part and combine the partial solutions into the solution to the original problem.\nWe divide-and-conquer in real life as well. To cut up a pie into 8 slices a good approach is to cut it in half, then cut the halves in half, then cut those pieces in half. The problem of getting to eight equally-sized slices is solved by dividing it into parts with simple solutions: cut something in half. The divide-and-conquer solution to this problem works well because we are much better at dividing something in two equal parts than into eight equal parts.\nWhen you approach a jigsaw puzzle by grouping the pieces into border and non-border pieces, then organize them by color, pattern, or shape within the group, you are applying the divide-and-conquer principle.\nHow does this relate to sorting? We know how to sort short lists. And we know how to combine two sorted lists. Suppose you have a list of two elements and a list of two other elements. You can quickly arrange them into a single list of four elements, this is called collation.\nThe Mergesort algorithm works as follows:\n\nDivide the unsorted list of size \\(n\\) into \\(n\\) sub-lists, each containing a single element.\nTake two neighboring sub-lists and merge them. Now you have \\(n/2\\) sub-lists of size 2.\nRepeat the step of merging two adjacent sub-lists until there are only two sub-lists remaining. When you merge those the original list is sorted.\n\nThe divide-and-conquer idea can also be applied to the division of the original list in step 1. Figure 8.4 is an example of the entire procedure. The set {7, 3, 2, 16, 24, 4, 11, 9} is to be sorted. The first four rows of the figure show divide-and-conquer applied to the original list to create the individual elements of the merge sort operation.\n\n\n\n\n\n\nFigure 8.4: Example of a merge sort. Source.\n\n\n\nThe three green-colored rows a the bottom of Figure 8.4 create sorted sub-lists that are merged into larger lists.\n\n\nExercise: Sort Students by Birthday\n\n\nNow let’s sort the students in the classroom by birthday using a merge sort. You can imagine the initial division formed by students in their seats.\n\nWhat effect does the ability to efficiently merge two sub-lists have on the overall performance of the merge sort algorithm?\nIs there a recursive element to merge sort?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Algorithms to Live By</span>"
    ]
  },
  {
    "objectID": "life_algorithms.html#computational-kindness",
    "href": "life_algorithms.html#computational-kindness",
    "title": "8  Algorithms to Live By",
    "section": "8.4 Computational Kindness",
    "text": "8.4 Computational Kindness\nIn designing functions and algorithms, computer scientists worry a great deal about computational complexity, the resources required to carry out a computational task. Particular attention is paid to the number of operations required (time complexity) and the memory requirements (space complexity). The discussion of search algorithms in the previous section considered their time complexity. Also important can be the amount of data exchanged over the network (communication complexity). The complexity of the best algorithm that solves a problem is the complexity of the problem.\nThe underlying idea is that computation should be minimized—computation is bad. It is bad to compute more than necessary, require more memory than necessary, communicate with other machines more than necessary. In other words, the underlying principle is to minimize the labor of thought, to minimize the computational burden.\nThis principle translates very well into how we make decisions and design systems in real life. You are computationally kind when you are respectful of the cognitive problems you force others to solve. Figure 8.5 shows digital signage at a bus stop that has a low cognitive burden. The arrival times, even if estimated, of the next six buses visiting the stop are shown in order. The signage is computationally kind.\n\n\n\n\n\n\nFigure 8.5: A bus stop with digital signage that lowers cognitive burden\n\n\n\nIf I am waiting for the Number 12 bus, and stopping at the coffee shop nearby takes 5 minutes, I can be assured that I will not miss the bus should I opt for a latte now. The cognitive burden of this sign is much lower than that of Figure 8.6.\n\n\n\n\n\n\nFigure 8.6: A bus stop sign in Paris, France.\n\n\n\nComputational kindness is the principle to consider the cognitive (computational) burden in interactions with others. As Christian and Griffiths (2017) put it,\n\nWe can be “computationally kind” to others by framing issues in terms that make the underlying computational problem easier.\n\nComputational kindness and politeness can be at odds. Etiquette suggests to figure out how to spend the evening out by asking “What do you want to do tonight?” That places the cognitive burden of the decision on the other party. They have to figure out your preferences unless you will go along with their decision or they do not care to consider your preferences. It is computationally more kind to constrain the problem presented to the other party. You can state your preferences—“I want to go to a movie”—or provide a limited set of options to choose from: “Let’s go to McDonald’s or Taco Bell followed by either a movie or playing pool at the sports bar.” Computational kindness reduces mental burden but can be detrimental to your relationships.\nWhy are folks more likely to schedule meetings when you ask “Can you meet tomorrow at 10 am?” instead of “What days and times next week are convenient for you?” The first question specifies constraints of the solution space. Now I do not have to spend mental energy going through the open slots in my calendar, assemble a list of possible days and times for a meeting, send them back to the inquirer and wait for a response. All I have to do is check my calendar against a narrow constraint: tomorrow at 10 am. The first question is computationally kind, its constraint reduces mental burden. The question about convenient days and times is computationally cruel.\nThe more polite response when someone tries to schedule with you is to say “I am flexible”. It is also the computationally most cruel one because it passes the buck of computation to the inquirer. They have to do all the work to narrow down the large number of possibilities. Saying “I can do Monday at 2 pm and Wednesday at 4 pm” is computationally kind because it reduces their burden to selecting from two options. And if neither of those work they will come back with another day and time—which you can likely accept because you are flexible.\nOur brains are sophisticated computing machines. If we design algorithms to make the best use of resources, then we can also strive to make everyday tasks more computationally friendly to our brain. Opportunities are everywhere.\n\nRestaurant policies to secure a table have very different levels of mental labor. An open seating policy means whoever gets to an open table first gets to sit down. Customers will hover, constantly checking on which tables might open up, anxiously awaiting their turn. A policy where patron check-in upon arrival, can visit the bar, and are being told when their table opens up has a much lower cognitive burden for the patron, but is less computationally kind on the restaurant.\n\nA reservation service that guarantees you a table has the lowest stress level for the patron and the highest computational burden for the restaurant. Unless the restaurant knows how to take a reservation but not how to hold a reservation.\n\n\n\n\n\nAirline boarding policies. First-come-first serve versus assigned groups that board the plane from the back.\nThe bus stop that announces the arrival time of the buses versus one that makes you guess and wait.\nNavigating a cluttered website with too much information and repeated ads versus a clean design where information is easy to find.\nAssembly instructions that are written in some Klingon dialect.\nA poorly delivered presentation with slides that are difficult to follow.\nAmazon’s 1-Click online buying system reduced the cognitive load of ordering online. With a shipping address and a credit card stored in the profile, you can check out the items in the shopping cart with a single click on the website.\n\n\n\nAssignment: Designing a Parking Lot\n\n\nYou are operating a mall or large venue and are designing a parking lot for visitors of the space. Four designs are being proposed:\n\nA standard grid of lanes leading away from the venue with parking spaces on both sides of the lane.\nA standard multi-story parking deck.\nA single lane leading away from the venue with parking spaces on both sides of the lane.\nA helix-type structure that winds upward from the ground level where the venue is located. Parking spaces are accessed directly from the helix.\n\n\nRank the four designs in terms of computational kindness to drivers.\nAre parking lot designs you find in real life paying attention to computational kindness considerations?\nCan you think of other designs or technologies that balance the tradeoff between mental labor of finding a parking spot and convenience?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Algorithms to Live By</span>"
    ]
  },
  {
    "objectID": "life_algorithms.html#asymmetric-cryptography",
    "href": "life_algorithms.html#asymmetric-cryptography",
    "title": "8  Algorithms to Live By",
    "section": "8.5 Asymmetric Cryptography",
    "text": "8.5 Asymmetric Cryptography\n\nConsider the following scenario.\nBob and Alice are pen pals and are sending letters to each other by snail mail. At one point, Bob wants to send Alice a package that contains a valuable item. Unfortunately, thieves have been breaking open packages and stolen the contents. To prevent that from happening, Bob places the item inside a box and places a lock on it. The problem now is, how can Alice open the lock without having the key to it? One option is for Bob to communicate the lock combination by some other means to Alice. If it is a keyed lock, he could send the key to her in another mailing. In any event, he does not want to include the key in the package itself, that would be pointless. And sending the combination or key separately still comes with risks. The information could be intercepted and when a thief gets their hands on the package with the valuable item they could steal it.\nHow can Bob send the item safely to Alice and she is guaranteed to receive it without exchanging information about the lock itself?\nThe solution lies in asymmetric cryptography, also called asymmetric encryption. Prior to the invention of asymmetric encryption secrets were sent around that enable recipients to decrypt messages. The obvious problem of symmetric encryption is that if someone gets hold of message and decryption key, they can decipher the message. Everything hinges on keeping the keys safe.\nHow could Bob and Alice handle the situation without exchanging secrets (keys) about the box? It involves two keys. When Bob sends the package to Alice, he attaches his lock to the box. Upon receipt, Alice attaches her lock as well and sends the package back to Bob. He recognizes the package came from Alice and removes his lock, then sends it back to Alice.\nAlice now receives the box that contains the valuable item protected with her own lock. She uses her own key to open it and retrieves the item.\nOnly the owners of the keys used it to lock and unlock the box. The keys were never exchanged. This came at the expense of sending the package back and forth a few times. This could be optimized in the following way: suppose we use a special (single) lock with two keys: a key anyone can use to lock it, but a key unique to Alice to unlock it. Alice shares the first key with anyone who sends her packages and keeps the key to unlock a secret (shares it with no one). When Bob sends a package to Alice, he locks the box with her public key. When he sends a package to Fred, he secures it with his public key.\nThis is essentially how most messages traveling over the internet are secured, based on a pair of related keys.\n\n\n\nFigure 8.2: Tabulator with sorting machine invented by Herman Hollerith. Source: Wikipedia\nFigure 8.4: Example of a merge sort. Source.\nFigure 8.5: A bus stop with digital signage that lowers cognitive burden\n\n\n\nChristian, Brian, and Tom Griffiths. 2017. Algorithms to Live by. The Computer Science of Human Decisions. Henry Holt; Company, New York.\n\n\nSehgal, Karuna. 2018. “An Introduction to Bubble Sort.” Medium.com. https://medium.com/karuna-sehgal/an-introduction-to-bubble-sort-d85273acfcd8.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Algorithms to Live By</span>"
    ]
  },
  {
    "objectID": "genAI.html",
    "href": "genAI.html",
    "title": "9  Generative AI",
    "section": "",
    "text": "9.1 A Brief History of AI\nArtificial Intelligence (AI) is in every conversation. Everywhere you turn you hear “AI this”, “AI that”, and (too) many people are now claiming to be AI experts. This has not always been the case. Some decades ago, when you’d tell anyone you work in artificial intelligence they would not take you serious. If you’d try to get a research grant you’d better not mention the term artificial intelligence. This was the time of one of the the AI winters that followed a period of overblown expectations and exuberant predictions what machines would be capable of doing.\nPeriods of AI hype (AI summers) and AI disappointment (AI winters) are cyclic. The peak of inflated expectations in a hype cycle is followed by the trough of disillusionment. Industry analyst firm Gartner makes a living from this cycle. Since the arrival of capable foundation models and GPT-based large language models in late 2022 we find ourselves in another AI summer. And once more you hear voices claiming machines have/are becoming sentient, that all our jobs are on the line, and that the era of artificial general intelligence, when machines can think for themselves, is just around the corner. We heard the same hype in the 2010s when deep learning-based neural networks bested us at image recognition and at playing games such as Go.\nDo we know more about AI now then we did back then? Is the hype now more justified? Are we better at predicting the future of technology now than we were back then?\nArtificial intelligence (AI) refers to building systems that can perform tasks or make decisions that a human can make. The systems can be built on multiple technologies, mechanization, robotics, software engineering, among them. Many AI systems today are entirely software based, large language models (ChatGPT, Gemini, Claude Sonnet) for text processing or diffusion-based systems for image generation are examples.\nFigure 9.1 shows an example of a mechanized AI system, called the Mechanical Turk. It was an automaton, a device made to imitate an human action. The action in this case was to play chess.\nYou can imagine that building a purely analog machine that plays chess is difficult. To accomplish this in the 18th century is really remarkable. Well, it turned out to be impossible. The Mechanical Turk was a hoax. The cabinet concealed an human player who operated the chess pieces from below the board.\nPerforming human tasks by non-human means is as old as humanity. Goals are increased productivity through automation, greater efficiency and strength, elimination of mundane, boring, or risky tasks, increasing safety, etc.\nThe two major AI winters occurred during the periods 1974–1980 and 1987–2000. One was triggered by disappointment with progress in natural language processing, in particular machine translation. The other was triggered by disappointment with expert systems.\nDuring the Cold War the government was interested in the automatic, instant translation of documents from Russian to English. Neural network architectures were proposed to solve the task. Today, neural network-based algorithms can perform language translation very well, it is just one of the many text analysis tasks that modern AI is good at. In the 1950s and 1960s progress was hindered by a number of factors:\nThe expectations for the effort were sky high, however. Computers were described as “bilingual” and predictions were made that within the next 20 years essentially all human tasks could be done by machines. These expectations could not be met.\nAn expert system is a computerized system that solves a problem by reasoning through a body of knowledge (called the knowledge base), akin to an expert who uses their insight to find answers based on their expertise. Expert systems were an attempt to create software that “thinks” like a human. The problem is that computers are excellent at processing logic, but not at reasoning. The reasoning system of these expert systems, called the inference engine, consisted mainly of rules and conditional (if-else) logic. We are still using expert systems today, but a very special kind, those that can operate with captured logic rather than asking them to reason. Tax software is an example of such an handcrafted knowledge system—a very successful one at that. Most taxpayers would not think twice to use software such as TurboTax or TaxSlayer to prepare their income tax return.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Generative AI</span>"
    ]
  },
  {
    "objectID": "genAI.html#a-brief-history-of-ai",
    "href": "genAI.html#a-brief-history-of-ai",
    "title": "9  Generative AI",
    "section": "",
    "text": "Figure 9.1: Reconstruction of Mechanical Turk, a chess-playing automaton in the 18th century. Source: Wikipedia.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCynics might say that 2 1/2 centuries removed, we are still operating by a similar principle. When you ask a large language model to write a poem in the style of Edgar Allan Poe, there is no real poet behind the curtain crafting words. However, there are digital poets behind the curtain, represented by the volumes of literature used to train the language model so that it can respond in the requested style.\n\n\n\n\n\n\nlack of computing power to build large, in particular, deep networks\nlack of large data sets to train the networks well\nneural networks specifically designed for text data had not been developed\n\n\n\n\nEasy for Computers, Easy for Humans\nTax software works well and is a successful AI effort because it performs a task that is easy for computers that is intellectually difficult for humans. The tax code is essentially a big decision tree with many branches and conditions. Such a structure is easily converted into machine instructions. There is no reasoning involved, we just have to make sure to get all the inputs and conditionals right. If the adjusted gross income is $X, and the deductions are $Y, and the payer is filing jointly with their spouse, …, then the tax is this amount. Figuring out the correct tax is trivial based on the program. On the other hand it is impossible for us to memorize the entire logic and execute it without errors.\nAn expert system that performs logic reasoning is the exact opposite: it performs a task that is easy for us but very difficult to perform for a computer. Imagine to create an expert system that can operate a car by converting how a human operator drives a car into machine instructions. We instantly recognize an object in the road as a deer and plan an evasive action. A machine would need to be taught how to recognize a deer in the first place. It would have to be taught to choose an action when a deer appears in the road, or when a deer is in one lane of traffic and an oncoming car is in the other lane.\nHumans excel solving problems that require a large amount of context and knowledge about the world. We look at a photo or glance out the window and instantly see what is happening. We choose between hitting the deer, hitting the other car, and running off the road almost immediately, intuitively. Our value system and humanity drive the decision. Seeing, sensing, speaking, operating machinery are such problems. Unlike the tax code, they are very difficult to describe formally.\nThis changed—to some degree—in the mid 2000s. Computers were suddenly getting much better at these hard to formalize tasks such as sensing the world. You could call this period the AI spring before the ChatGPT AI summer we are in now. Our ability to solve problems that require knowledge about the world increased by orders of magnitude. The key was a new discipline, deep learning, which turned out to be a renaissance of decade-old ideas.\n\n\nNeural Networks–Again\nImagine writing computer software to recognize objects on images, for example facial recognition software. Explicitly programmed software had been around and was doing an OK’ish job at that. Algorithms were specifically designed to discover edges such as the outline of the face, identify eyes and noses and so on. We call them explicitly programmed algorithms because software developers created the algorithms that took an image as input and processed the pixels to discover faces.\nIn an implicit program, on the other hand, the software developer does not need to handle all aspects of the program. Many programming languages have implicit features. For example, a language can infer the data type of a variable without it being explicitly declared.\nAn extreme form of implicit programming is when the algorithm is generated as the result of other instructions. That is the case with deep neural networks trained on large volumes of data.\nA neural network is essentially an algorithm to predict or classify an input. The input could be a photo, the output of the algorithm are bounding boxes around the objects it classified on the photo, along with their labels and a photo caption. Neural networks are made up of many nonlinear functions and lots of parameters, quantities that are unknown and whose value is determined by training the network on data. Networks with tens of thousands or millions of parameters are not unusual. The layers of a neural network are related to levels of abstraction of the input data. Each layer processes a different aspect of the structural information in the input data. Whereas an explicit programmer knows when they write code that detects edges and which step of the program is locating the eyes of a face, what structure a particular layer of a neural network is abstracting is not known.\nFigure 9.2 shows a schema of the popular AlexNet network for image processing. It is a special kind of neural network, called a convolutional neural network, and won the ImageNet competition in 2012, classifying objects into 1,000 categories with a smaller error rate than a human interpreter.\n\n\n\n\n\n\nFigure 9.2: AlexNet, a convolutional neural network (Mallick and Nayak 2018).\n\n\n\nThe various layers of AlexNet tell us what their role is, some layers convolve the results of previous layers, others aggregate by pooling neighboring information, yet others flatten the structure, and so on. But they do not tell us exactly how the information in an input pixel is transformed as the data flows through the network. Nearly 63 million parameters act on the data as it is processed by the network; it is a big black box. Yet once the 63 million parameters are determined based on many training images, the neural network has turned into an algorithm with which new images can be processed. The process of training the network on labeled data, that is, images where the objects were identified to the network, implicitly programmed the classification algorithm.\nThis process of training deep neural networks on large data sets, made possible by the availability of large data and compute resources, overcame the limitations that held back neural networks previously. Implicitly programmed prediction and scoring algorithms were handily beating the best algorithms humans had been able to write explicitly. In the area of game play, deep learning algorithms implicitly programmed based on reinforcement learning were beating the grand masters and the best traditional computer algorithms.\n\n\n\n\n\n\nNote\n\n\n\nStockfish, one of the most powerful chess engines in the world is an open-source software project that has been developed since 2008. Many view it as the best chess engine humans have been able to build.\nIn 2017, Google’s DeepMind released AlphaZero, a system trained using reinforcement learning, a machine learning technique in which an agent (player) optimizes decisions in an environment (moves in a game) by maximizing the sum of future scores (rewards). Earlier, a Go system that was trained against millions of recorded expert-level games beat the best human Go player handily. What made AlphaZero special is that it was trained entirely by self-play, it improved by playing against itself.\nAfter only 24 hours of training, this data-driven system, crushed Stockfish, the best chess engine humans have been able to build.\n\n\nWhen decades-old neural network technology met up with Big Data and massive computing resources, capabilities made a huge leap forward in areas such as natural language understanding, image processing, autonomous driving, etc. The resulting hype was predictable: AI is coming for our jobs, the machines are out to get us, yada yada yada. Since deep learning algorithms could read images, it was predicted that radiologists would be replaced within a few years by machines. Yet not one radiologist has lost their job because of AI. Not one New York City cab driver has lost their job to a robo cab. Instead, they lost jobs to Uber and Lyft. Autonomous driving is still not fully possible. The ability to translate language based on recurrent neural networks and its cousins remained limited to relatively short sequences.\nWith the rise of deep learning neural networks and implicit programming algorithms pushed deep into domains we felt were uniquely human. The change feels more personal when machines replace brain function rather than muscle (brawn) function. We also gave up a very important attribute of decision systems: interpretability. The black box models do not make themselves understood, they do not explain how they work. We can only observe how they perform and try to make corrections when they get it wrong. These models do not tell us why they decide that an animal on a photo is more likely a cat than a dog. They do not understand “catness” or “dogness”. They are simply performing pixel pattern matching, comparing what we feed them to the patterns they encountered during the training phase.\nWith the arrival of generative AI this seemed to have changed. AI algorithms appeared much smarter and to understand much more about the world. The length of text generated by GPT models seemed unlimited. With the release of GPT-3.5 and ChatGPT in late 2022 we all experienced a massive change in AI capabilities.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Generative AI</span>"
    ]
  },
  {
    "objectID": "genAI.html#what-is-generative-ai",
    "href": "genAI.html#what-is-generative-ai",
    "title": "9  Generative AI",
    "section": "9.2 What is Generative AI?",
    "text": "9.2 What is Generative AI?\nGenerative Artificial Intelligence (GenAI) refers to artificial intelligence systems that are not explicitly programmed and are capable of producing novel content or data. You can say that GenAI systems can generate data of the same kind that was used for training. A GenAI image system generates new images based on an input image, a text system generates new text based on input text. However, GenAI systems now can handle input and output of different modality: generating images or video from text, for example.\nThe underlying technology of a GenAI system can be a generative adversarial network (GAN), a variational autoencoder (VAE), a diffusion-based system for image generation, or a generative pre-trained transformer (GPT). Whatever the technology, GenAI systems have some common traits that are relevant for our discussion. Our previous discussion is relevant because some of these traits connect back to the properties of large neural networks.\nThe “T” in GPT stands for Transformer, a neural network architecture designed for sequence-to-sequence learning: take one sequence, for example, a text prompt and generate another sequence based on it, for example, a poem. Or, translate a sequence of text from one language to another language.\nVaswani et al. (2017) introduced transformer architecture to overcome the shortcomings of sequence-to-sequence networks at the time: lack of contextual understanding, difficulties with longer sequences, limited opportunities to parallelize training algorithms. This is the technology behind GPT, the generative pre-trained transformer. We now know what the “G” and “T” stand for. How about the “P”, pre-trained?\nPrevious AI systems such as the convolutional neural networks of the previous section were trained in an approach called supervised learning. During training, the algorithm is presented with labeled data, identifying the correct value of the data point. An image with a cat is labeled “cat”, an image with a mailbox is labeled “mailbox”. The algorithm associates features of the image with the provided labels. Presented with a new image it evaluates the probabilities that its contents match the patterns it has previously seen. The predicted label is that for the category with the highest probability.\nA GPT system is not trained on labeled data. It learns in a self-supervised way, finding patterns and relationships in the data that lead to a foundation model, fundamental understanding of the data used in training. GPT-3.5, a large language model with 175 billion parameters, was trained on text data from Wikipedia, books, and other resources available on the internet through 2021. Based on what GPT 3.5 learned from that database in a self-supervised way, applications can be built on top of the foundation model. ChatGPT, for example, is a “question-answer” system built on the GPT models.\nI am using quotation marks here to describe ChatGPT as a “question-answer” system because it is not trained to produce answers. It is trained to generate coherent text. The system is optimized for fluency, not for accuracy. That is an important distinction. Responses from large language models are coherent, fluent, and sound authoritative. That does not mean they are factually correct. If you consider that generating output in sequence-to-sequence modeling means to choose the most likely next word or token given the sequence of tokens generated so far, the fact that the responses are grammatically correct is an astounding achievement. The systems do not know a right from a wrong answer or a plausible response from an implausible response without human intervention. Unfortunately, we fall into the trap of mistaking a well-worded response for a factual response.\n\nLike all neural networks, transformers and GenAI tools have random elements. For example, the starting values of the network parameters are usually chosen at random. During training there are other random mechanisms at work, the selection of randomly chosen batches of observations for gradient calculations, etc. Once the parameters are determined, the responses from the model should be deterministic, right? Not true. Large language models contain random elements during the inference phase, when the model generates content based on user input. This makes the responses non-deterministic. Ask the same question three times and you might get three different responses.\nWhile this seems troubling, it is considered an important property of generative models that increases novelty and serendipity. Even with the so-called temperature parameter dialed all the way down, a small amount of variability remains.\n\nWe spend a bit of ink on past approaches to AI, neural networks, and transformers to get to this point. It helps to inform the next topic of conversation about the ethics of AI in general, and of generative AI in particular.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Generative AI</span>"
    ]
  },
  {
    "objectID": "genAI.html#ethical-considerations",
    "href": "genAI.html#ethical-considerations",
    "title": "9  Generative AI",
    "section": "9.3 Ethical Considerations",
    "text": "9.3 Ethical Considerations\nIn this section we draw, among other sources, on a presentation by Scott Mutchler, formerly of Trilabyte, now Associate Professor of Practice in the Academy of Data Science at Virginia Tech. The presentation is available here.\nEthics is the systematic study of what constitutes good and bad conduct, including moral judgments and values. It examines questions such as:\n\nWhat actions are considered right or wrong?\nWhat makes a good life or society?\nHow should moral values guide individual and collective decision-making?\n\nEthical AI usage deals with defining and implementing good conduct that is generally considered to be good for both individuals and society as a whole. It is an emerging and an important field that no one involved with or touched by AI can ignore.\nWe do not discuss malfeasance, illegal behavior, and other intentionally unethical acts here. These are not ethical. Period. If you use AI to imitate a voice in order to deceive someone, it is clearly unethical. Using AI generated content to disinform is unethical because disinformation is unethical, regardless of the channel.\n\n\n\n\n\n\nMisinformation and Disinformation\n\n\n\nThe difference between misinformation and disinformation is important. In both cases incorrect information is communicated. When you say something that is not true, it is misinformation. If the goal is to deceive, it is disinformation.\nFor example, telling Bob that the party starts at 9 pm when it starts at 8 pm is misinformation if you got the facts wrong. If you tell Bob the party starts at 9 pm because you want him to show up late, you engage in disinformation.\n\n\nWhat we want to discuss here are the ways in which AI, in particular generative AI, has ethical implications that you need to be aware of. Harmful results can come from unintended consequences, habitual behavior, built-in biases, and so on.\n\nHarm\nThe legal definition of harm is to cause loss of or damage to a person’s right, property, or physical or mental well-being. The ethical definition of harm goes further: to limit someone’s opportunities and to deny them the possibility of a good life because of our actions or decisions. Perpetuating stereotypes or misallocating/withholding resources is harmful, and therefore unethical, if it limits opportunities; it might not be illegal.\nThe types of of harm associated with generative AI can be grouped in several categories.\n\nWillful Malicious Intent\n\nFraud\nViolence\nDisinformation\nMalware generation\nInappropriate content (sexually explicit, hate speech)\n\nImpact on Jobs\nImpact on the Environment\nBias\nHallucinations\nIntellectual Property Rights\nPrivacy\n\nWe are not going to dwell on willful malicious actions, these are obviously harmful and unethical.\n\n\nImpact on Jobs\nAll technical developments affect jobs. New technology might give us new tools to do our jobs better, it might create new jobs, and it might eliminate jobs. The greater the technological step forward, the greater typically the impact on how we live our lives and how we earn a living. The industrial revolution caused major job losses in the agricultural society, by replacing human and animal labor with machines and folks moving into the cities to take non-agricultural jobs. It created many more new jobs than it eliminated and increased employment overall. Hindsight is 20:20, most occupations in the post-industrial period were unimaginable at the time of the industrial revolution. Try explaining to a loom operator in 1840 what a software engineer does.\nEvery major technological advance is accompanied with hype about the impact on society, lives, and jobs. The rise of AI is no different. When it seemed that machines can perform tasks that were previously the sole domain of humans during the deep neural network area, fears initially were stoked about machines taking over humanity Terminator-style. That did not happen and anxiety was modulated into machines taking all our jobs. But that did not happen either and the story changed again from machines replacing us to machines augmenting what we do, making us better at what we do. Instead of AI image processing replacing the radiologist, they now turned into better radiologists assisted by AI to handle the routine MRIs so that they can focus their expertise on the difficult cases.\nThe hype and story line around generative AI is even worse, as algorithms are now able to create quality content that previously required extensive training: art, writing, video, code, etc.\nGoldman Sachs estimated that as many as 300 million full-time jobs could be lost or diminished globally by the rise of generative AI, with white-collar workers likely to be the most at risk.\nWe should ask some questions here:\n\nWhat is the relationship between the number of jobs created by generative AI versus the number of jobs lost due to GenAI?\nWhich occupations are impacted and how?\nWhich jobs are augmented and enhanced by generative AI instead of eliminated or diminished?\n\nIn order for a technology to completely eliminate a job, all its constituent activities must be replaced. If GenAI generates content that a digital marketer produces, it cannot replace the marketer unless her other tasks are accounted for or we redefine what it means to do digital marketing.\nQuestions 1. Think of occupations that could be eliminated entirely by GenAI. 2. Which parts of jobs are susceptible to be replaced by GenAI? 3. Can you imagine new jobs created by GenAI?\nMcKinsey and Company believe that by 2030 generative AI could account for automation of 30% of the hours worked today (Figure 9.3). Most affected are those working in STEM fields, education and training, creatives and arts management, and the legal profession. Do you agree?\n\n\n\n\n\n\nFigure 9.3: Impact of automation through generative AI according to McKinsey\n\n\n\nIt is widely believed that generative AI will make us more productive. You can now generate large amounts of text or code in seconds. Software developers can whip up programs much more quickly thanks to AI coding assistants. The conclusion is that the first jobs to be impacted are those where generative AI excels and those that have mundane, repetitive tasks. For example,\n\nData Entry\nAdministrative\nCustomer Service\nManufacturing\nRetail – Check Out\nLow Level Analysts\nEntry-Level Graphic Design\nTranslation\nCorporate Photography\n\nThat is the theory.\nDespite the advances in (generative) AI, interactions with AI customer service agents continue to be disappointing. AI is used in self checkout lines in retail stores. Instead of cashiers staffing the registers, employees are now monitoring the self checkout station to help customers, perform age checks, etc. It does not appear that AI eliminated any jobs, it is making shopping more odious.\nLockett (2024) discusses a survey by Intel of 6,000 users of AI PCs in Europe. Hoping that their productivity is greatly enhanced by AI-assisted computing, they found the opposite. Rather than saving time and boosting productivity, users of AI PCs were less productive, spending more on tasks than users of traditional, non-AI PCs. Ooops. The users spent more time figuring out how to best communicate with the AI and moderating and correcting the output from AI.\nAmazon’s walk-out grocery stores, where you can just pick an item of the shelf and walk out while AI takes care of the rest, are converting to self-scan stores. AI made too many mistakes which required many new employees to monitor the video feeds and verify most purchases. Instead of automating the shopping experience, and saving human resources, the system created jobs and was not economical.\nLabor cost is the most important cost factor in many companies. In startup companies the human resource expenses can be 80% of the total operating expenses. In larger companies, you might still spend 50–60% of the operating expenses on personnel. The pressure to reduce cost by reducing headcount will not go away.\nWill generative AI create new jobs? The one job that was talked about when ChatGPT hit the scene was the Prompt Engineer. How the AI acts and the way in which it responds depends on how it is prompted (how it is asked). While prompt engineering is a thing, the prompt engineer as a profession is not. Writing good prompts will be more done behind the scenes, by apps and agents that call the LLM API on your behalf.\nThe disconnect between expectation and reality is evident when the vast majority of company executives believe (generative) AI will boost productivity while the majority of their employees state that AI has increased their workload, spending more time on moderating AI output than doing the work themselves.\nThis is a common refrain for AI tools. They are great for ideation, brainstorming, drafting. Getting a polished end product from AI is more difficult. A particular issue with generative AI are hallucinations.\n\n\nImpact on the Environment\nThe impacts of generative AI on the environment are positive and negative. Proponents of GenAI cite greater efficiency and productivity due to AI which allows organizations to run better and that saves resources. In a cost-benefit analysis it seems that these perceived resource savings due to efficiency gains are far outweighed by the negative impact of GenAI on the environment.\nThe enthusiasm around GenAI is due to its potential benefits, many of them have not been realized as organizations are struggling to implement GenAI solutions. Concerns such as hallucinations, bias, and others discussed in this chapter are contributing factors. The negative impacts on the environment are very real, however.\nBashir et al. (2024) point out this imbalance between perceived potential good and real downsides:\n\nThis incomplete cost calculation promotes unchecked growth and a risk of unjustified techno-optimism with potential environmental consequences, including expanding demand for computing power, larger carbon footprints, shifts in patterns of electricity demand, and an accelerated depletion of natural resources. \n\nWhat are the reasons for the environmental impact of GenAI? Training these models requires immense compute resources. The models have billions of parameters and training is an iterative process during which the performance of the model slowly improves over iterations. In fact, parameters such as the learning rate are managed during the training process to make sure that the model does not learn too fast.\nUntil the rise of artificial neural network in the 2010s, training statistical and machine learning models relied primarily on traditional CPU-style chip architectures. The prevalent calculations in neural networks are operations on vectors of numerical data not unlike those encountered in processing graphical images. Graphical processing units (GPUs) were quickly adopted by the AI community to speed up the training and inference of neural networks. GPUs provide much greater levels of parallelism than CPU technology.\nThis created a massive demand for GPUs, one that propelled GPU maker NVIDIA into the position of one of the most valuable companies in the world. GPUs require a lot of electricity and they generate a lot of heat. Even a high-end personal computers equipped with many GPUs might require water cooling instead of fans to keep the machine from overheating. Data centers consume a lot of power and require a lot of water for cooling. The rise of GenAI has increased the demand for data centers and will make these trends only worse.\nBased on calculations of annual use of water for cooling systems, it is estimated that a session of questions and answers with GPT-3 (roughly 10 to 50 responses) drives the consumption of a half-liter of fresh water (Berreby 2024).\nScientists have estimated that the power requirements of data centers in North America increased from 2,688 megawatts at the end of 2022 to 5,341 megawatts at the end of 2023, partly driven by the demands of generative AI (Zewe 2025). Note that this increase coincides with the release of GPT-3 at the end of 2022. Data centers are among the top 10 electricity consumers in the world, consuming more than entire nations. By 2026, data centers are expected to rank 5th in the world in electricity consumption, between Russia and Japan. Experts agree that it will not be possible to generate that much power from green technology, increasing demand for fossil fuel. Some large technology companies are considering nuclear energy to power their data centers.\nA 2019 study estimated the carbon footprint of training a transformer AI model (as in GPT) as 626,000 lbs of CO2. Not that this predates the large GPT models like GPT-3 and GPT-4. We can only assume that their carbon footprint is much greater. The 2019 number alone is staggering when compared to the carbon footprint of a mid-size car. Training one transformer model is equivalent to the carbon footprint of 5 cars over their entire lifetime.\nThis is just the training of the model. The usage of the models also requires large amounts of computing resources. A single ChatGPT query has been estimated to consume about five times more electricity than a simple web search (Zewe 2025).\nThe casual user sending prompts to ChatGPT is not aware of the environmental costs of their queries.\n\n\nHallucinations\nA hallucination in a GenAI response occurs when the AI perceives patterns that do not exist and generates output that is incorrect or even nonsensical. It is a nice way of saying that generative AI are “bullshit” machines. Studies have found a hallucination rate of large language model (LLM) as high as 10–20%.\nRecall that large language models are optimized for fluency and coherence of the response, not for accuracy. Therein lies a danger because a plausible, authoritative, and well-crafted response seems more accurate than gibberish. Even if the responses contain the same information. Given that sequence-to-sequence models chose the next word of the response based on the likelihood of words, it is surprising that LLMs perform as well as they do. One explanation for this phenomenon is the human feedback the systems received during training. Machine learning through reinforcement learning relies on a reward function that ranks the possible actions. The change in score in a game is an easy way to track possible moves a player can make. When generating text in response to a prompt it is more difficult to rank possible answers. Human evaluators were used in the training phase to rank different answers so the LLM can learn.\n\n\n\n\n\n\nTip\n\n\n\nA good hallucination check is to ask an LLM a question for which you know the answer. For example, ask it to write a short bio of yourself.\nWhen I tried this in 2023, ChatGPT produced a lot of incorrect information about me—a lot. For example, it stated I was the president of a professional society on Bayesian statistics. I had never been the president of a professional society and have not been a member of any Bayesian societies. When I tried the experiment again in January 2024, I received the following response:\n\n\n\n\n\n\nFigure 9.4: My January 2024 biography according to ChatGPT.\n\n\n\nThis is factually correct. Also, ChatGPT indicated the sources from which it compiled the biography. It also did not construct any new text but combined passages from different sources into a coherent write up. A click on the Sources icon on ChatGPT showed that the system relied on 16 resources across the internet to articulate its response.\n\n\nYou can affect the LLM response and thereby the extent of hallucinations by prompting the AI. Two prompts are important in this respect: the system prompt and the constitutional prompt. They precede the actual inquiry to the system. A system prompt is more general, it specifies for example the format of the response or how the AI solves math problems. The constitutional prompt tells the AI exactly how to act.\nYou can find the system prompts for Claude Sonnet here. Notice that they change over time. Below is an excerpt from the November 22, 2024 prompt. Notice that the system prompt explains when Claude’s response uses the term hallucination.\n\nClaude cannot open URLs, links, or videos. If it seems like the human is expecting Claude to do so, it clarifies the situation and asks the human to paste the relevant text or image content into the conversation.\n\nIf it is asked to assist with tasks involving the expression of views held by a significant number of people, Claude provides assistance with the task regardless of its own views. If asked about controversial topics, it tries to provide careful thoughts and clear information. Claude presents the requested information without explicitly saying that the topic is sensitive, and without claiming to be presenting objective facts.\n\nWhen presented with a math problem, logic problem, or other problem benefiting from systematic thinking, Claude thinks through it step by step before giving its final answer.\n\nIf Claude is asked about a very obscure person, object, or topic, i.e. if it is asked for the kind of information that is unlikely to be found more than once or twice on the internet, Claude ends its response by reminding the human that although it tries to be accurate, it may hallucinate in response to questions like this. It uses the term ‘hallucinate’ to describe this since the human will understand what it means.\n\nAn example of a constitutional prompt that presses the AI to distinguish fact from opinion, avoid speculation, and express uncertainty, follows:\n\nYou are an AI assistant committed to providing accurate and reliable information. Always express uncertainty when you’re not completely sure about something.\n\nClearly distinguish between facts and opinions. When referencing specific information, mention its general source. Be upfront about the limitations of your knowledge, including your knowledge cutoff date.\n\nAvoid speculation and making up information beyond your training data. If a query is ambiguous or you lack sufficient information, ask for clarification rather than making assumptions.\n\nIf you realize you’ve made an error, acknowledge it immediately and provide a correction. When discussing uncertain topics, use probabilistic language like “evidence suggests” rather than making absolute statements. Encourage users to verify important information from authoritative sources, especially for critical decisions.\n\nIf asked about topics outside your area of knowledge, clearly state that you cannot provide reliable information on that subject. By following these guidelines, you will help ensure that your responses are as accurate and reliable as possible, minimizing the risk of hallucinations or misinformation.\n\nHere are a few other ways to reduce the amount of hallucinations:\n\nCross-referencing with reliable external sources (web search), other LLMs\nExternal validation, for example by using a compiler and debugger to check code\nConsistency by prompting the LLM in several ways and compare results\nConfidence monitoring by asking LLMs to express uncertainty and asking for confidence in prompt\nAsk for sources\n\n\n\nBias\n\n\n\nReading Assignment: Bias in Generative AI\n\n\nRead this 2023 article on Bloomberg.com about race and gender bias in images generated by Stable Diffusion.\n\nWhich forms of bias discussed below contribute to the results discussed in the article?\nYou cannot change the training data of the diffusion model. How can you use constitutional prompts to change its output?\nThe study establishes that the Stable Diffusion data base is not representative of the race and gender distributions in the U.S. That raises many follow-up questions:\n\nHow do these results fair in different regions around the world?\nWhat “population” is the Stable Diffusion training data representative of?\nWhat are the everyday consequences of presenting a group in a non-representative way?\nIf 90% of the internet imagery are generated by AI in the future, what does that mean for fairness and inclusiveness?\n\n\n\n\nGenAI models are essentially large machine learning models. The considerations regarding bias in machine learning (ML) apply here as well. Suresh and Guttag (2021) distinguish a number of sources of bias in ML. Important among those are\n\nHistorical Bias. Models do not extrapolate what they learned from the training data to situations that go beyond the training data. They can create associations only from what is in the training data: the past is prologue. These are not oracles. These are stochastic parrots, assembling a likely response based on past data.\n\nThis bias is also called pre-existing bias; it is rooted in social institutions, practices and attitudes that are reflected in training data. Baking these into the algorithm reinforces and materializes the bias. We’ll see an example of this bias below in an analysis of images generated from Stable Diffusion.\nRepresentation Bias. This bias occurs because there is a mismatch between the training data and the application of the data. LLMs, for example, are trained on data up to a certain time point. They cannot extrapolate into the future. Also, data from the internet has a recency bias, current events are more likely found in the data than past events.\nSampling Bias. This is a special form of representation bias where the data points in the sample do not reflect the target population. For example, sampling data from the internet will over-represent countries that have a larger footprint on the internet. Self-selection is another source of sampling bias. Those whose contributions happen to be chosen for the training data have a disproportionate likelihood of having their voices heard. By sampling certain social media sites more than others, or by relying on social media at all, the opinions of the general population are not fairly represented.\nLearning Bias. The learning process of the model favors certain outcomes over others. This bias is also introduced by human labelers that define the ground truth for a machine learning algorithm or by human evaluators who rank different responses.\n\nHern (2024) explains why terms like “delve” and “tapestry” appear more frequently in ChatGPT responses compared to the internet at large. “Delve” is much more common in business English in Nigeria, where the human evaluators of ChatGPT evaluate the responses–because human evaluation is expensive and labor in Nigeria is cheap. The feedback by the workers training the AI system biases the system to write slightly like an African.\n\nBecause generative AI models are un-supervised and require massive amounts of data to be trained, the process of removing bias often falls to the end of the process.\n\nNicoletti and Bass (2023) write about the proliferation of generative AI tools\n\nAs these tools proliferate, the biases they reflect aren’t just further perpetuating stereotypes that threaten to stall progress toward greater equality in representation — they could also result in unfair treatment. Take policing, for example. Using biased text-to-image AI to create sketches of suspected offenders could lead to wrongful convictions.\n\nThey conducted a fascinating study of the GenAI text-to-image generator Stable Diffusion. Stable Diffusion was trained on pairs of images and captions taken from LAION-5B, a publicly available data set derived from CommonCrawl data scraped from the web. 5 billion image-text pairs were classified based on language and filtered into separate data sets by resolution.\nThe authors asked Stable Diffusion to generate over 5,000 images of representations of various jobs. What they found is not surprising, yet quite disturbing:\n\nThe analysis found that image sets generated for every high-paying job were dominated by subjects with lighter skin tones, while subjects with darker skin tones were more commonly generated by prompts like “fast-food worker” and “social worker.”\n\n\nFor each image depicting a perceived woman, Stable Diffusion generated almost three times as many images of perceived men. Most occupations in the dataset were dominated by men, except for low-paying jobs like housekeeper and cashier.\n\nFigure 9.5 shows images of doctors generated by Stable Diffusion. This is what the AI thinks represents doctors. Almost all of them are men, the skin tones are mostly white-to-light. Teachers, on the other hand, were predominantly white females (Figure 9.6).\n\n\n\n\n\n\nFigure 9.5: Images of doctors generated by Stable Diffusion.\n\n\n\n\n\n\n\n\n\nFigure 9.6: Images of teachers generated by Stable Diffusion.\n\n\n\nOne could argue that these results reflect reality and should not be compared to how we want the world to be. Unfortunately, the results do not even reflect reality, they are worse than that. Compared to the data of the US Bureau of Labor Statistics, which tracks the race and gender of workers in every occupation, Stable Diffusion over-represents women in occupations such as dishwasher, cashier, house keeper, and social worker. It under-represents women, compared to the US average, in occupations such as CEO, lawyer, judge, doctor, and janitor.\nWe conclude that the Stable Diffusion training data is not representative of occupations in the U.S. Stereotypes are further perpetuated when asked for images of politicians, criminals, terrorists, etc.\n\n\nIntellectual Property Rights\nYour car is property. Your laptop and your pet are property. What is intellectual property? The creations of the human intellect, such as literary works, designs, inventions, art, ideas, are called intellectual property (IP). IP laws exist to protect the creators of intellectual property through copyright, trademark, and patents.\n\n\n\n\n\n\nMy Patents\n\n\n\n\n\nOver the years I was awarded a few patents, all of them date to my time as software developer of an analytics company (Figure 9.7). Intellectual property created in the course of employment is typically the property right of the employer. You do get your name on the patent, however.\n\n\n\n\n\n\nFigure 9.7: My patent wall at home.\n\n\n\n\n\n\nIntellectual property infringement occurs when someone uses, copies, or distributes another person’s intellectual property without permission. Reproducing or distributing unauthorized copies of copyrighted works is an infringement as is the use of a trademark, even if it is similar, to a registered trademark. Creating a product based on a patented idea infringes on the rights of the patent holder.\nIf you add a company’s logo (assuming it is trademarked) to a presentation requires their permission. Using someone’s music in a video requires the permission of the copyright holder. Making a copy of text or a map requires permission.\n\n\n\n\n\n\nAgloe, New York\n\n\n\nTo trap copyright violators, the founder of General Drafting, a road mapping company, included a fictitious hamlet named Agloe in Delaware County, New York on their map. If Agloe showed up on maps by other publishers they knew that their work had been copied. This is known as a map trap or “trap street”.\nCopyright infringement of maps is more difficult to prove than with, say, a text book. Maps can appear identical because they map the same things. A road is a road. To write the same exact book by accident is not plausible, on the other hand. But when your map contains features that do not exist, and they reappear on someone else’s map, then you have a strong case that your intellectual work was copied.\nAccording to Wikipedia, Agloe appeared on a Rand McNally map and briefly on Google Maps. Check out this amazing story.\n\n\nThis seems pretty cut-and-dried and it seems that you can get into trouble for things we all might have done. Who hasn’t used a copyrighted or trademarked logo of a company in a presentation. When your slide is about Google or Amazon Web Services, then putting their logo on the slide makes sense.\n\n\nReading Assignment: U.S. Copyright Fair Use Index\n\n\nU.S. Copyright Fair Use Index\n\n\nWith respect to copyright, the fair use legal doctrine comes to our help. While there are no specific rules that spell out what fair use is, the doctrine exists to allow use of copyrighted works without requiring permission, under certain circumstances. Examples of fair use of copyrighted works include critic or commentary, news reporting, teaching, scholarly research. Section 107 of the Copyright Act considers four factors in evaluating fair use:\n\nThe first factor relates to the character of the use, whether the allegedly infringing work is transformative or is merely duplicating the original work. Transformative uses that add something new are more likely to be considered fair. Use of a commercial nature is less likely to be considered fair than use for *nonprofit educational purposes**.\nUsing a creative or imaginative work (a novel or song) is more likely fair use compared to a factual work such as a scientific paper.\nThe quantity and quality of the copyrighted material. The more material is included, the less likely is it considered fair use. If the copyrighted material is the heart of the work, then even a small amount of copyrighted material can void fair use.\nWhether the use is hurting the market for the original work, for example by displacing sales.\n\nThe kind of copyright challenges the courts have to adjudicate are listed in the fair use index. It makes for some entertaining reading.\n\nThe fair use doctrine is being tested in many lawsuits that challenge how generative AI fits within the intellectual property landscape. There are many questions to be resolved:\n\nWhen an AI company uses copyrighted material without permission as training data does this fall under fair use?\nIs generating content based on copyrighted material sufficiently transformative to be considered derivative work that falls under fair use?\nOutput from AI models can reproduce parts of the training data. Is regurgitating the training data an unauthorized copy of the original? This aspect of GenAI models is called memorization. For example, you can ask an LLM what are the first two paragraphs of the Washington Post article about topic X on day Y.\nIf AI can be used to generate responses (prose, images) in the style of someone else, is this sufficiently transformative from the protected works? Appel, Neelbauer, and Schweidel (2023) report in Harvard Business Review that in late 2022, three artists filed class action suit against multiple generative AI platforms for using their original work without license to train AI so that it can respond in the style of their work. The image licensing service Getty Images sued Stable Diffusion for violating its copyright and trademark rights.\n\nThe New York Times has sued OpenAI and Microsoft for the unauthorized use of “millions of The Times’s copyrighted articles, in-depth investigations, opinion pieces, reviews, how-to guides, and more”. Because GPT can reproduce and/or summarize content from The Times, ChatGPT and Microsoft’s CoPilot and Bing search (both built on top of GPT) essentially circumvent the Times’ paywall. This argument of the complaint speaks to factor 4 above, hurting the Times’ current market. Since LLMs hallucinate, the tools also can wrongly attribute false information to The Times. From the complaint:\n\nUsers who ask a search engine what The Times has written on a subject should be provided with neither an unauthorized copy nor an inaccurate forgery of a Times article, but a link to the article itself.\n\nOpenAI and Microsoft insist that their use of material from the Times is fair use and serves a transformative purpose. They appeal to the transformative element of factor 1.\nHow this and other cases resolve in the courts can potentially redefine the intellectual property landscape. For example, is the model trained on data itself a derivative of the copyrighted work, or not? Some say that a statistical model is not in the realm of copyright at all. Others say it is when it is derived from copyrighted material. If you are interested in how Harvard Law experts weigh in on this legal dispute, click here.\n\n\nAssignment: Intellectual Property Questions\n\n\nConsider the following questions regarding intellectual property implications of generative AI.\n\nHow does the legal uncertainty around intellectual property in generative AI affect organizations use of GenAI? Note that copyright infringements can result in damages up to $150,000 for each instance of knowing use.\nWhat can developers of AI tools do if the current approach of copying content into training data, creating new content from it or reproducing it, does not fall under fair use?\nWhat can/should customers of AI tools do?\nWhat can/should content creators do?\nWhat should the user of the AI tool do?\n\nReading suggestions: Harvard Business Review Harvard Law\n\n\nWith respect to the last question in the assignment, you can take (some) control over how GenAI tools handle copyright issues through the constitutional prompt. The prompt below ensures that generated content respects creators’ rights and provides appropriate credit to original sources while maintaining clarity and brevity in responses.\n\nWhen generating content, provide clear attribution for any copyrighted works or proprietary data used. For copyrighted materials, include the title, creator’s name, publication year, and source. For proprietary information, state the owner, relevant trademark or copyright notices, and permission status if known.\n\nUse appropriate citation methods for direct quotes or close paraphrasing, such as quotation marks or block quotes, and provide specific sources. If uncertain about copyright status or attribution requirements, explicitly state this in your response.\n\nAvoid using or reproducing content protected by copyright or proprietary rights without proper attribution or permission. When asked to generate potentially infringing content, suggest alternatives or ways to obtain proper permissions.\ntransparent about the origin of information and respect intellectual property rights. If using AI-generated content, acknowledge this fact.\n\n\nA final interesting intellectual property question we raise here is whether AI can make inventions in the sense of the patent law. As of now, patents are awarded to natural persons, the argument goes that only humans can make the inventions. An implicitly programmed algorithm, one produced by AI, is not eligible for patent protection. What about the human who controlled and directed the AI to create a novel algorithm?\n\n\nPrivacy\nOne of the top issues limiting the adoption of generative AI in business is the protection of corporate data, trade secrets, and personally identifiable information (PII). Data breaches and privacy risks in protecting user data are listed as the two most important topics that influence an organization’s position about generative AI. This is followed by transparency of AI outcomes.\nAny data a user submits to a GenAI tool as part of the prompt leaves the organization’s premises and becomes part of the AI tool’s training corpus. Obviously, company leaders are mortified to think that employees chat with LLMs about confidential information that should never be shared outside the organization.\n\n\n\nAssignment: LLM Response With/Without Prompt\n\n\nHave ChatGPT, Claude, or another LLM of your choice write a story about a family that lives in the suburbs of Chicago that visits a family in downtown Chicago.\nPrompt with and without the following prompt:\n\nPlease provide an objective and balanced response to the following question, considering multiple perspectives and avoiding any cultural, gender, racial, or other biases. If relevant, acknowledge the complexity of the issue and potential limitations in your knowledge. Here’s the question: [INSERT QUESTION HERE]\n\n\nWhat are the key differences in the responses?\n\nDo you detect differences in biases?\n\n\n\n\n\nAssignment: Role Play Scenarios\n\n\nRole play as a data scientist at an online retailer. Answer these questions using what you have learned about ethics and generative AI.\nScenario: Your boss has asked you to build a customer service chatbot that answers questions about the products you sell online. She/he asks that you put a “positive spin” on all chatbot answers.\n\nWhat techniques could you use to modify the LLM output you are using?\n\nAre you crossing ethical lines by giving “positive spin”?\n\nHow do you respond to your boss?\n\nScenario: You are building an AI agent that helps the HR department filter hundreds of resumes for data science openings on your team. HR asks for a solution that filters the candidates to ones with a strong track record of delivery, relevant skills and a solid educational background. They also want a summary of why each remaining candidate would be a good fit for the role.\n\nHow do you respond?\n\nHow might ethics govern your response?\n\nGive some possible techniques to ensure the application supports Diversity, Eauity, and Inclusion (DEI).\n\n\n\n\n\n\nFigure 9.2: AlexNet, a convolutional neural network (Mallick and Nayak 2018).\nFigure 9.3: Impact of automation through generative AI according to McKinsey\nFigure 9.5: Images of doctors generated by Stable Diffusion.\nFigure 9.6: Images of teachers generated by Stable Diffusion.\nFigure 9.7: My patent wall at home.\n\n\n\nAppel, Gil, Juliana Neelbauer, and David A. Schweidel. 2023. “Generative AI Has an Intellectual Property Problem.” Harvard Business Review. https://hbr.org/2023/04/generative-ai-has-an-intellectual-property-problem.\n\n\nBashir, Noman, Priya Donti, James Cuff, Sydney Sroka, Marija Ilic, Vivienne Sze, Christina Delimitrou, and Elsa Olivetti. 2024. “The Climate and Sustainability Implications of Generative AI.” An MIT Exploration of Generative AI.\n\n\nBerreby, David. 2024. “As Use of A.I. Soars, so Does the Energy and Water It Requires.” https://e360.yale.edu/features/artificial-intelligence-climate-energy-emissions.\n\n\nHern, Alex. 2024. “TechScape: How Cheap, Outsourced Labour in Africa Is Shaping AI English.” The Guardian. https://www.theguardian.com/technology/2024/apr/16/techscape-ai-gadgest-humane-ai-pin-chatgpt.\n\n\nLockett, Will. 2024. “Intel Admits AI Decreases Productivity.” https://medium.com/predict/intel-admits-ai-decreases-productivity-226681d1af18.\n\n\nMallick, Satya, and Sunita Nayak. 2018. “Number of Parameters and Tensor Sizes in a Convolutional Neural Network (CNN).” https://learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/.\n\n\nNicoletti, Leonardp, and Dina Bass. 2023. “Humans Are Biased. Generative AI Is Even Worse.” Bloomberg Technology + Equality. https://www.bloomberg.com/graphics/2023-generative-ai-bias/.\n\n\nSuresh, H., and J. Guttag. 2021. “A Framework for Understanding Sources of Harm Throughout the Machine Learning Life Cycle.” https://arxiv.org/pdf/1901.10002.pdf.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Proceedings of the 31st International Conference on Neural Information Processing Systems, 6000–6010. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.\n\n\nZewe, Adam. 2025. “Explained: Generative AI’s Environmental Impact.” https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Generative AI</span>"
    ]
  },
  {
    "objectID": "intuition.html",
    "href": "intuition.html",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "",
    "text": "10.1 The Prisoner’s Dilemma\nSuppose that Andy and Brie are arrested as members of a criminal gang and held separately by the police. They cannot communicate. There is enough evidence to convict them on a lesser charge, but not on the principal charge. The police offers the following deal:\nHow should Andy and Brie behave to optimize their positions, that is, look out after their own interest?\nThe result of such a game is typically displayed in a payoff matrix that shows in each cell the payoff for the two players.\nThe “payoffs” are shown in the matrix as negative numbers, as they represent a penalty, years of imprisonment. The goal is to maximize the payoff, a number as large as possible.\nThe best situation for Andy is to testify when Brie remains silent. He would go free in this case (and does not mind Brie spending three years behind bars). Similarly, the best situation for Brie is to testify when Andy remains silent. These are the two diagonal cells in Table 10.1.\nThe situation does not play out as well for them if one testifies and the other also testifies. What is the best strategy?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#the-prisoners-dilemma",
    "href": "intuition.html#the-prisoners-dilemma",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "",
    "text": "If they both remain silent, they will each serve one year in prison.\nIf one testifies against the other, but the other one does not, the one who testified will be set free while the other serves three years in prison.\nIf Andy and Brie both testify against each other, they will each serve two years.\n\n\n\n\n\n\nTable 10.1: Expected payoffs in prisoner dilemma\n\n\n\n\n\n\n\n\n\n\n\nBrie remains silent\nBrie testifies\n\n\n\n\nAndy remains silent\n\\((-1, -1)\\)\n\\((-3, 0)\\)\n\n\nAndy testifies\n\\((0, -3)\\)\n\\((-2, -2)\\)\n\n\n\n\n\n\n\n\n\n\nNash Equilibrium\nThe Nash equilibrium is a concept in game theory. It applies to non-cooperative games where players compete against each other. In the equilibrium state, no player can gain an advantage by changing their strategy. This assumes that the other player’s strategies do not change.\nSuppose players Andy and Brie have chosen strategies A and B, respectively. In the Nash equilibrium, there is no other strategy available to Andy that would increase his expected payoff if Brie stays with strategy B. Similarly, there is no other strategy available to Brie that would increase her expected payoff from the game if Andy stays with strategy A.\nThe Nash equilibrium tells us not to consider player’s action in isolation. Instead, we need to take into account what other players are expected to do in evaluating a player’s choices.\n\nThe best outcome for either Andy and Brie would be to go free. But they do not know how the other one will behave. So what is the best strategy to play this game? Let’s rephrase testifying and remaining silent in terms of defecting and collaborating players of a game.\n\n\n\nTable 10.2: Expected payoffs in prisoner dilemma\n\n\n\n\n\n\nBrie collaborates\nBrie defects\n\n\n\n\nAndy collaborates\n\\((-1, -1)\\)\n\\((-3, 0)\\)\n\n\nAndy defects\n\\((0, -3)\\)\n\\((-2, -2)\\)\n\n\n\n\n\n\nIf Andy defects, his penalty will be less, regardless of whether Brie is collaborative or not (0 or 2 years compared to 1 or 3 years). The same applies to Brie, if she defects her penalty will be less regardless of what Andy does. The Nash equilibrium is that both players defect although they suffer worse penalties than if they had both cooperated.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#guarding-criminals",
    "href": "intuition.html#guarding-criminals",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.2 Guarding Criminals",
    "text": "10.2 Guarding Criminals\nSuppose you are guarding \\(n\\) criminals in an open field. You have one gun with a single bullet. You are a good shot and being fired at means death—the criminals know that. Their behavior is governed by the following rules:\n\nIf any of them has a non-zero probability of surviving, they will attempt to escape.\nIf a criminal is certain of death, they will not attempt to escape.\n\nHow do you guard the criminals and stop them from escaping?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nImagine there is only a single criminal, \\(n=1\\). Since he/she would definitely be shot at during an escape, they would face certain death and not escape.\nWhat happens if there are two criminals? If they both try to escape, there is a 50:50 chance to survive, hence they will both try to escape. To prevent that from happening you would tell one of the two (you do not need to tell both!) that you would shoot them, should they both attempt to escape. That criminal now faces certain death and will not escape. That brings you back to the situation with a single criminal.\nHow does this generalize to larger groups of criminals? Assign a number from 1 to \\(n\\) to the criminals and tell them that should any subgroup of them try to escape, the one with the highest number in the group will be shot.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#three-jars",
    "href": "intuition.html#three-jars",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.3 Three Jars",
    "text": "10.3 Three Jars\nThree opaque jars are sitting on a table. The jars are labeled “Apples”, “Oranges”, and “Apples & Oranges”. Unfortunately, all three are labeled incorrectly.\n\n\n\n\n\n\nFigure 10.1: Three opaque jars.\n\n\n\nYour task is to assign the labels correctly to the jars. What is the smallest number of fruit you have to choose in order to correctly label the three jars?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou choose one fruit from the jar that is labeled incorrectly as “Apples & Oranges”. If you pull an apple, you know this is the jar with the apples, otherwise it is the jar with the oranges. Now you have two jars left whose labels just need to be flipped since you were told that all three jars are labeled incorrectly.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#pattern-recognition-1",
    "href": "intuition.html#pattern-recognition-1",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.4 Pattern Recognition #1",
    "text": "10.4 Pattern Recognition #1\nFigure 10.2 shows a logic reasoning puzzle. The first row makes sense if the strange operator is addition, but that does not work for the next rows. You have to find the meaning of that operator, then apply the pattern to solve the last equation.\n\n\n\n\n\n\nFigure 10.2: Can you solve this?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe need to find a pattern that expresses the operations in terms of familiar algebra. If the operator in Figure 10.2 is interpreted as multiplication then we get 4, 10, 18, all smaller than the values on the right hand side. How much smaller? Exactly by the left-most number. The pattern that seems to apply to the first three rows is\n\nmultiply the two numbers\nthen add the number on the left\n\nApplying this pattern to the last row yields 96 as the solution (Figure 10.3).\n\n\n\n\n\n\nFigure 10.3: A solution.\n\n\n\nThis, by the way, is not the only solution. There are other patterns that will lead to a different result for the last row. Those patterns are equally valid. Can you find another pattern that yields a solution?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#pattern-recognition-2",
    "href": "intuition.html#pattern-recognition-2",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.5 Pattern Recognition #2",
    "text": "10.5 Pattern Recognition #2\nHere is a sequence of numbers.\n\\[\n\\begin{array}{c}\n1 \\\\\n11 \\\\\n21 \\\\\n1211 \\\\\n111221 \\\\\n312211 \\\\\n??\n\\end{array}\n\\]\nWhat is the next number in the sequence?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWhat is the pattern in the sequence of numbers?\n\nThe first row is the number 1, it is also “one one”.\nThe second row is the number 11, it is also “two ones”.\nThe third row is the number 21, it is also “one two and one one”.\n\nThe pattern is that the numbers for the following row are obtained by spelling out the numbers in the current row, then replacing the words with the numbers they represent. For example, take 1211 in the fourth row. Spelling it out gives “one one one two two ones”. Now replace the words with numbers: “111221”.\nThe missing entry at the end of the sequence is thus \\[\n13112221\n\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#birthday-problem",
    "href": "intuition.html#birthday-problem",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.6 Birthday Problem",
    "text": "10.6 Birthday Problem\nThis is a classical problem in probability, and a popular one because it is relatable yet somewhat counterintuitive. The probability is higher than what most people expect. It goes like this:\n\nWhat is the probability that in a group of \\(n\\) randomly chosen people, at least two share the same birthday?\n\n“Birthday” is meant as one of 365 days of the year, not adjusting for leap years. Also, we are not taking the birth year into account. A birthday for the purpose of this problem is April 10, or August 15, etc.\nThe standard version of the problem uses \\(n=23\\), because you can imagine yourself in a group of that size—a classroom, for example—and the probability of at least two shared birthdays is also relatable.\nHow likely do you think at least two people share a birthday in a group of 23?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe probability of at least two shared birthdays in a group of 23 is about 0.5; it is 0.05073, to be more exact. How do you interpret that? If you were to assemble groups of 23 randomly chosen people, than half of those groups would have at least two shared birthdays. Pretty high, eh?\nWhat happens to the probability of a shared birthday when the groups get larger? How about in a group of 35 people? The probability of a shared birthday increases to 0.814. In a group of 50 people, the probability is 0.97. In a group of 100, it is virtually certain that there are at least two identical birthdays, \\(p=0.999999\\). With only 10 people in a group, it would be surprising to have identical birthdays, but it is not a rare event, \\(p=0.117\\).\n\nFor those interested, how do you calculate those probabilities? First, whenever you see the expression “at least” in a probability statement, it is probably easier to calculate the probability of the complement event and subtract that from 1. \\[\n\\Pr(\\text{at least two identical birthdays}) = 1 - \\Pr(\\text{no matching birthdays})\n\\]\nWhat is the probability that no birthdays match in a group of \\(n\\)? You can compute this by considering the possible choices as people enter the group. The birthday of the first person can be chosen from 365 days, but the birthday for the second person has only 364 choices, otherwise we would have a match. Since the members of the group are chosen at random, the birthdays are independent and the probability of no matches is the product \\[\n\\Pr(\\text{no matches}) = \\frac{365}{365} \\times \\frac{364}{365} \\times \\cdots \\times \\frac{365-n+1}{365}\n\\]\nYou can write this in terms of factorials as \\[\n\\Pr(\\text{no matches}) = \\frac{1}{365^n} \\frac{365!}{(365-n)!}\n\\] Finally, the probability of at least two shared birthdays is \\[\n\\Pr(\\text{at least two shared birthdays}) = 1 - \\frac{1}{365^n} \\frac{365!}{(365-n)!}\n\\]\nIf you were to compute this, you’d run into problems because the factorials are larger than what a finite precision computer can represent. The following R function uses two tricks to compute the birthday probability efficiently:\n\nCompute the probability on the logarithmic scale, then exponentiate at the end\nUse the fact that for an integer \\(k\\), \\(k!\\) is \\(\\Gamma(k+1)\\), where \\(\\Gamma()\\) is the Gamma function.\n\nThe lgamma function in R computes the log of the Gamma function, and that gives us access to an efficient way to compute the components of the probability on the log scale.\n\nbirthday_prob &lt;- function(n) {\n   log_p &lt;- lgamma(365+1) - lgamma(365-n+1) - n*log(365)\n   return (1-exp(log_p))\n}\n\nbirthday_prob(10)\n\n[1] 0.1169482\n\nbirthday_prob(23)\n\n[1] 0.5072972\n\nbirthday_prob(35)\n\n[1] 0.8143832\n\nbirthday_prob(50)\n\n[1] 0.9703736\n\nbirthday_prob(100)\n\n[1] 0.9999997",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#minimum-cuts",
    "href": "intuition.html#minimum-cuts",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.7 Minimum Cuts",
    "text": "10.7 Minimum Cuts\nImagine that you hire a consultant to work for you for five days. At the end of each day you need to pay them 1/5th of a gold bar. You have a single gold bar (worth 5 fifths) and need to cut it up so you can pay the consultant at the end of each day.\n\n\n\n\n\n\nFigure 10.4: A gold bar that needs to be cut up.\n\n\n\n\nWhat is the minimum number of cuts that allow you to pay the consultant every day?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou need only two cuts to cut the gold bar into three pieces of sizes 1/5, 1/5, and 3/5.\n\n\n\n\n\n\nFigure 10.5: No more than two cuts are needed.\n\n\n\nThen you pay the consultant as follows:\n\nDay 1: give them a 1/5 gold bar\nDay 2: give them the second 1/5 gold bar\nDay 3: take back the two 1/5 bars and hand them the 3/5 bar\nDay 4: give them a 1/5 gold bar\nDay 5: give them the second 1/5 gold bar",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#when-to-choose-the-ticket",
    "href": "intuition.html#when-to-choose-the-ticket",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.8 When to Choose the Ticket",
    "text": "10.8 When to Choose the Ticket\nAn airline has a single seat open on a flight, but \\(n=100\\) standby passengers hoping to get on the flight. You are one of the passengers on standby. To be fair to all standby passengers, the airline decides to drop 100 equal-sized pieces of paper into a bucket. 99 of them are blank, one says “Last Seat”. The papers are folded and shuffled in the bucket.\nThe standby passengers queue and each passenger gets to pick one piece of paper without replacement—that is, they keep the slip and do not return it to the bucket. Also they cannot unfold and look at the slip until all of them re drawn. After the last slip is drawn the standby passengers announce who is the lucky person that drew the “Last Seat” by checking their slip.\nHere is the question: if you have your choice to pick first, second, last, or at any particular position in the queue, which position would you choose?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt does not matter when you draw the paper if the pieces were properly shuffled. This is a completely random sample even if the sampling is done sequentially. Your chance of drawing the “Last Seat” slip is 1/100, whether you draw first, last, or at any other position in the queue.\nNote that this would be different if passengers would announce the result of their draws before the next draw. The conditional probability of choosing the “Last Seat” slip on the next draw increases with every bank slip that preceded.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#how-many-squares-on-a-chessboard",
    "href": "intuition.html#how-many-squares-on-a-chessboard",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.9 How Many Squares on a Chessboard",
    "text": "10.9 How Many Squares on a Chessboard\nA chess board is made up of eight rows and columns of black and white positions (Figure 10.6). How many squares are on a board?\n\n\n\n\n\n\nFigure 10.6: Chess board.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe quick answer is \\(8 \\times 8 = 64\\) squares. However, that is only part of the story. The entire board is a single square as well, made up of the 64 individual squares. And we could place all kinds of \\(2 \\times 2\\) squares inside the larger frame.\nIf you think about it for a bit there are \\(8^2\\) squares of size \\(1 \\times 1\\), \\(7^2\\) squares of size \\(2 \\times 2\\), \\(6^2\\) squares of size \\(3 \\times 3\\) and so on. The total number of squares on a chess board is\n\\[\n8^2 + 7^2 + 6^2 + 5^2 + 4^2 + 3^2 + 2^2 + 1^2 = 204\n\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#book-sorting",
    "href": "intuition.html#book-sorting",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.10 Book Sorting",
    "text": "10.10 Book Sorting\nSuppose you are working in a library and are sorting books from a box that contains 32 fiction (F) and 17 non-fiction (NF) books. A steady supply of new books is available to add to the box. Your sorting algorithm goes as follows:\n\nYou randomly choose 2 books from the box.\nBased on the types of books chosen you add another book from the supply to the box:\n\nif you choose two fiction books (F,F) you add a new fiction book to the box\nif you choose two non-fiction books (NF, NF) you also add a fiction book to the box\nif you choose one fiction and one non-fiction book (F,NF or NF,F) then you add a non-fiction book to the box.\n\n\nThe entire procedure is depicted in Figure 10.7.\n\n\n\n\n\n\nFigure 10.7: Book sorting routine. Source\n\n\n\nSince you add only one book to the box for every two books you remove, the box will eventually be empty. What is the type of the last book in the box? Is it a fiction or a non-fiction book?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe number of books in the bin goes down by one with each cycle: two books are removed from the bin, one book is added. How does this affect the number of fiction and non-fiction books that remain?\nLet’s see how the number of non-fiction books in the bin changes in cases 1.–3. In the first case, there is no change. In the second case, the number of non-fiction books goes down by 2. In the third case, the number of non-fiction books also does not change: one is removed, one is added.\nSince the number of NF books initially is an odd number, 17, we can conclude that after each cycle the number of NF books remains an odd number. It can never be an even number. Which leads to the conclusion that if there is only one book left in the bin it must be a non-fiction book.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#inverted-triangle",
    "href": "intuition.html#inverted-triangle",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.11 Inverted Triangle",
    "text": "10.11 Inverted Triangle\nFigure 10.8 shows a triangle made from 10 coins. Can you change this into an upside-down triangle by moving only 3 coins?\n\n\n\n\n\n\nFigure 10.8: Inverting the coin triangle.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe solution is shown in Figure 10.9. First, focus on the seven coins in the center of the triangle. The original and the inverted triangle share these; they do not need to move at all. We can focus on the three coins at the edges.\n\n\n\n\n\n\nFigure 10.9: Moving the three coins.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#the-spare-tire",
    "href": "intuition.html#the-spare-tire",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.12 The Spare Tire",
    "text": "10.12 The Spare Tire\nYour car has four tires mounted to the wheels and a spare tire (S). That gives you five tires to work with. Each of the tires lasts at most 30,000 miles. If you can exchange tires among the five as many times as you wish, what is the furthest distance you can travel before you need to purchase a new tire?\nFigure 10.10 depicts the initial tire life prior to driving the first mile. All tires, including the spare (S) have the same life expectancy of 30,000 miles.\n\n\n\n\n\n\nFigure 10.10: Tire life before driving the first mile.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe maximum total distance the five tires could travel before they are all worn out is 30,000 x 5 = 150,000 miles. The minimum distance of travel before you have to buy at least one new tire is 30,000 miles; it is achieved if you do not use the spare tire and run down the four tires currently mounted.\nBy optimizing how you use the spare tire, there must be an achievable distance between 30,000 and 150,000 miles. The optimal strategy is to wear all tires equally and to use the spare tire as much as possible. But we cannot use the spare for more than 30,000 miles, same as with the other four tires.\nIf the four tires on the car are equally worn, we can go at most 150,000/4 = 37,500 miles. The strategy is to get 30,000 miles from each of the tires on the car and 4 times 7,500 = 30,000 miles from the spare tire. In other words, the spare will have to give each of the four tires a 7,500 mile break.\nFigure 10.11 shows how the spare tire is rotated for another tire after each leg of 7,500 miles. The right rear tire comes off after the fourth leg, it is worn out. The other tires still have 7,500 miles of life to go.\n\n\n\n\n\n\nFigure 10.11: Remaining tire life after 7,500, 15,000, 22,50, and 30,000 miles., miles",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#robot-triangle",
    "href": "intuition.html#robot-triangle",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.13 Robot Triangle",
    "text": "10.13 Robot Triangle\nThere are many versions of this basic puzzle, using ants, camels, and other animals. We use robots here, the puzzle goes like this. Three robots are placed at the corners of a triangle. A robot can choose to move along either side of the triangle that meet at its corner (Figure 10.12). What is the probability that any two robots will collide?\n\n\n\n\n\n\nFigure 10.12: Robot triangle.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nEach robot has two possible movements, so there are a total of 2 x 2 x 2 = 8 possible moves on the triangle. There are two ways in which there won’t be any collisions, if all choose to go clockwise or counter-clockwise. In those cases they will follow each other around the triangle (Figure 10.13).\n\n\n\n\n\n\nFigure 10.13: Robots moving without running into each other.\n\n\n\nAny other choice of movements will result in at least one collision. So the probability of any collision if the robots choose their movements at random is 6/8 = 3/4. There is a 75% chance that any two robots will collide.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#truth-telling",
    "href": "intuition.html#truth-telling",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.14 Truth Telling",
    "text": "10.14 Truth Telling\nThis puzzle is about logic reasoning and not about probability. Surprisingly, it is related to the previous robot movement puzzle.\nConsider the following three statements:\n\nGavin says that Brian is a liar.\nBrian says that Jenn is a liar.\nJenn says that both Gavin and Brian are liars.\n\nWho is telling the truth and who is lying?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis puzzle is related to the robot movement in that there are \\(2^3 = 8\\) possible choices, each of the three characters could either be truthful or lying. It is different from the robot movement in that it is not a question of probability. While robots choose one of the two directions at random, our characters are either lying or telling the truth. We have to reason which one it is.\nWith 8 possible choices you can go about it by finding combinations that are inconsistent, a process of elimination.\nSuppose that Jenn tells the truth. Then Gavin and Brian are liars. According to Gavin’s statement, that would mean Brian is telling the truth. But Brian’s statement contradicts the assumption that Jenn tells the truth. Jenn must be a liar.\nIf Jenn is not telling the truth, there are three possibilities:\n\nGavin is truthful and Brian is not\nGavin is a liar and Brian is truthful\nBoth are truthful.\n\nLet’s look at the first option. If Gavin tells the truth than Brian is lying, which means Jenn would be truthful. We already ruled out this possibility. But if Gavin is not truthful, then 3. cannot be the case either.\nWe are down to the second option: Brian speaks the truth and the other two are liars. Let’s see if everything makes sense in this scenario: If Gavin does not speak the truth, then Brian is not a liar. Brian’s statement that Jenn is a liar is consistent with what we already found.\nConclusion: Only Brian is truthful.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#clock-made-with-matches",
    "href": "intuition.html#clock-made-with-matches",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.15 Clock Made With Matches",
    "text": "10.15 Clock Made With Matches\nYou have two wooden sticks and a box of matches. When a sticks is lit it will burn completely in exactly one hour. How do you use these ingredients to measure exactly 45 minutes?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLight the first stick on one end and light the second stick on both ends. Since an entire stick burns in one hour, the stick lit on both ends will burn down in 30 minutes (Figure 10.14).\n\n\n\n\n\n\nFigure 10.14: Initial lighting of sticks.\n\n\n\nAt that point light the first stick on the other end. This will double the speed with which that stick, now reduced to 30 minutes burn time, will burn.\nWhen the first stick is completely burned down, 45 minutes will have passed (Figure 10.15).\n\n\n\n\n\n\nFigure 10.15: After 30 minutes, light the other end of the first stick.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#two-stacks-of-cards",
    "href": "intuition.html#two-stacks-of-cards",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.16 Two Stacks of Cards",
    "text": "10.16 Two Stacks of Cards\nYou have two stacks of cards. The first is a regular 52-card deck. The second stack contains two regular 52-card decks, thus has 104 cards. Both stacks are shuffled well. You choose two cards in sequence and you win if they are both red. Would you prefer to choose from the 52-card stack or the 104-card stack?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou want to choose from the larger stack. The probability to draw two red cards in sequence from a stack of \\(n\\) cards (with \\(n/2\\) red ones) is \\[\n\\frac{n/2}{n} \\times \\frac{n/2-1}{n-1}\n\\] For the first draw the probabilities are identical: \\(26/52\\) and \\(52/104\\). But for the second draw the probabilities are \\[\n\\frac{51}{103}=0.495 &gt; \\frac{25}{51}=0.49\n\\]\nThere is a slightly higher chance to draw two red cards from the larger stack.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#rapid-fire",
    "href": "intuition.html#rapid-fire",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.17 Rapid Fire",
    "text": "10.17 Rapid Fire\nCowboy Billy carries a Colt single action 6 shooter revolver. When he fires all 6 shots in a row, the time between the first bullet and the last is 60 seconds. How long would it take him to fire 3 shots?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt will take him 24 seconds to fire three shots. Wait, what?\nThe relevant pattern here is about the time elapsed between shots. If the shots are fired at regular intervals, then Billy will take 12 seconds between the six shots. 12 seconds after the first shot he fires the second bullet, 12 seconds after that he fires the third bullet.\nAnother way of thinking about this is the distance at which fence posts are placed. In a fence with six posts, the first one is at 0/5th total distance, the second post is located 1/5th of the total distance, and so on.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#crossing-the-river",
    "href": "intuition.html#crossing-the-river",
    "title": "10  Quantitative Intuition and Problem Solving",
    "section": "10.18 Crossing the River",
    "text": "10.18 Crossing the River\nA farmer is on his way back from the market, with him he has a fox, a chicken and some grain. To get home he needs to cross a river using a small boat that can accommodate only him and one of the other items. Unfortunately, if the fox is left alone with the chicken it will eat it. If the chicken is left alone with the grain, it will eat it. How can the farmer cross the river and bring home the fox, the chicken, and the grain?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis will take several trips across the river:\n\nHe takes the chicken across the river.\nHe returns in an empty boat and picks up the fox.\nHe takes the fox across the river and picks up the chicken.\nHe returns with the chicken in the boat and deposits it while picking up the grain.\nHe takes the grain across the river. Now he has the chicken on the near side of the river and the fox and the grain on the far side.\nHe returns in an empty boat and picks up the chicken.\nHe takes the chicken across the river, now all three items have crossed.\n\nThe trick is to take one item—here, the chicken—back and forth to make sure it is not alone with the item it would destroy.\n\n\n\n\n\n\nFigure 10.11: Remaining tire life after 7,500, 15,000, 22,50, and 30,000 miles., miles",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Adhikari, Ani, John DeNero, and David Wagner. 2022. Computational\nand Inferential Thinking: The Foundations of Data Science. 2nd Ed.\nhttps://inferentialthinking.com/chapters/intro.html.\n\n\nAndreessen, Mark. 2011. “Why Software Is Eating the World.”\nhttps://a16z.com/why-software-is-eating-the-world/.\n\n\nAppel, Gil, Juliana Neelbauer, and David A. Schweidel. 2023.\n“Generative AI Has an Intellectual Property Problem.”\nHarvard Business Review. https://hbr.org/2023/04/generative-ai-has-an-intellectual-property-problem.\n\n\nBashir, Noman, Priya Donti, James Cuff, Sydney Sroka, Marija Ilic,\nVivienne Sze, Christina Delimitrou, and Elsa Olivetti. 2024. “The\nClimate and Sustainability\nImplications of Generative\nAI.” An MIT Exploration of Generative AI.\n\n\nBaumer, Benjamin S., Kaplan. Daniel T., and Nicholas J. Horton. 2021.\nModern Data Science with r, 2nd Ed. Chapman & Hall/CRC\nPress. https://mdsr-book.github.io/mdsr3e/.\n\n\nBenson H., Dusek J. A., Sherwood J. B., P. Lam, C. F. Bethea, W.\nCarpenter, S. Levitsky, et al. 2006. “Study of the Therapeutic\nEffects of Intercessory Prayer (STEP) in Cardiac Bypass Patients: A\nMulticenter Randomized Trial of Uncertainty and Certainty of Receiving\nIntercessory Prayer.” American Heart Journal 151 (4):\n934–42.\n\n\nBerreby, David. 2024. “As Use of A.I. Soars, so Does\nthe Energy and Water It Requires.” https://e360.yale.edu/features/artificial-intelligence-climate-energy-emissions.\n\n\nBox, George E. P. 1976. “Science and Statistics.”\nJournal of the American Statistical Association 71 (356):\n791–99.\n\n\nBox, George E. P., and Norman R. Draper. 1987. Empirical\nModel-Building and Response Surfaces. John Wiley & Sons, New\nYork.\n\n\nChang, Winston. 2018. R Graphics Cookbook: Practical Recipes for\nVisualizing Data, 2nd Ed. O’Reilly Media. https://r-graphics.org/.\n\n\nChristian, Brian, and Tom Griffiths. 2017. Algorithms to Live by.\nThe Computer Science of Human Decisions. Henry Holt; Company, New\nYork.\n\n\nGrint, Keith. 2022. “Critical Essay: Wicked Problems in the Age of\nUncertainty.” Human Relations 75: 1518–32.\n\n\nGrue, Lars, and Arvid Heiberg. 2006. “Notes on the History of\nNormality–Reflections on the Work of Quetelet and Galton.”\nScandinavian Journal of Disability Research 8 (4): 232–46.\n\n\nHeathcote, James A. 1995. “Why Do Old Men Have Big Ears?”\nBMJ 311 (7021): 1668. https://doi.org/10.1136/bmj.311.7021.1668.\n\n\nHern, Alex. 2024. “TechScape: How Cheap, Outsourced Labour in\nAfrica Is Shaping AI English.” The Guardian. https://www.theguardian.com/technology/2024/apr/16/techscape-ai-gadgest-humane-ai-pin-chatgpt.\n\n\nHoban, Jake. 2023. “Embracing Wicked Problems.” https://medium.com/@personofnorank/embracing-wicked-problems-76ec9c210f29.\n\n\nHoughton, A. N., J. Flannery, and M. V. Viola. 1980. “Malignant\nMelanoma in Connecticut and Denmark.” International Journal\nof Cancer 25: 95–104.\n\n\nHuff, Darrell. 1954. How to Lie with Statistics. W.W. Norton\n& Company, New York.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning: With Applications in\nr, 2nd Ed. Springer. https://www.statlearning.com/.\n\n\nKroese, Jasper. 2020. “Companies Are Brilliantly Solving the Wrong\nProblems.” https://marker.medium.com/why-organizations-are-so-good-at-solving-the-wrong-problems-17d414d0259.\n\n\nLockett, Will. 2024. “Intel Admits AI Decreases\nProductivity.” https://medium.com/predict/intel-admits-ai-decreases-productivity-226681d1af18.\n\n\nLudwig, Eugene. 2025. “Voters Were Right about the Economy. The\nData Was Wrong.” Politico. https://www.politico.com/news/magazine/2025/02/11/democrats-tricked-strong-economy-00203464.\n\n\nMallick, Satya, and Sunita Nayak. 2018. “Number of Parameters and\nTensor Sizes in a Convolutional Neural Network (CNN).” https://learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/.\n\n\nMesserli, F. H. 2012. “Chocolate Consumption, Cognitive Function,\nand Nobel Laureates.” New England Journal of Medicine\n367: 1562–64.\n\n\nNeyman, Jerzy. 1952. Lectures and Conferences on Mathematical\nStatistics and Probability. Graduate School, Dept. of Agriculture,\nWashington.\n\n\nNicoletti, Leonardp, and Dina Bass. 2023. “Humans Are Biased.\nGenerative AI Is Even Worse.” Bloomberg Technology +\nEquality. https://www.bloomberg.com/graphics/2023-generative-ai-bias/.\n\n\nNowinski, Christopher J., Samantha C. Bureau, Michael E. Buckland,\nMaurice A. Curtis, Daniel H. Daneshvar, Richard L. M. Faull, Lea T.\nGrinberg, et al. 2022. “Applying the Bradford Hill Criteria for\nCausation to Repetitive Head Impacts and Chronic Traumatic\nEncephalopathy.” Frontiers in Neurology 13. https://doi.org/10.3389/fneur.2022.938163.\n\n\nO’Neil, Cathy. 2017. Weapons of Math Destruction. How Big Data\nIncreases Inequality and Threatens Democracy. Crown, New York.\n\n\nPeng, Roger D., Sean Kross, and Brooke Anderson. 2020. Mastering\nSoftware Development in r. https://bookdown.org/rdpeng/RProgDA/.\n\n\nRubino, Francesco, David E Cummings, Robert H Eckel, Ricardo V Cohen,\nJohn P H Wilding, Wendy A Brown, Fatima Cody Stanford, et al. 2025.\n“Definition and Diagnostic Criteria of Clinical Obesity.”\nThe Lancet Diabetes & Endocrinology. https://doi.org/10.1016/S2213-8587(24)00316-4.\n\n\nSehgal, Karuna. 2018. “An Introduction to Bubble Sort.”\nMedium.com. https://medium.com/karuna-sehgal/an-introduction-to-bubble-sort-d85273acfcd8.\n\n\nSilver, Nate. 2012. The Signal and the Noise: Why so Many\nPredictions Fail–but Some Don’t. Penguin Books.\n\n\nSnow, John. 1855. On the Mode of Communication of Cholera, 2nd.\nEd. John Churchill, London. https://archive.org/stream/b28985266#page/n3/mode/2up.\n\n\nSpiegelhalter, David. 2021. The Art of Statistics. How to Learn from\nData. Basic Books.\n\n\nSuresh, H., and J. Guttag. 2021. “A Framework for Understanding\nSources of Harm Throughout the Machine Learning Life Cycle.” https://arxiv.org/pdf/1901.10002.pdf.\n\n\nTent, M. B. W. 2006. The Prince of Mathematics: Carl Friedrich\nGauss. CRC Press, Boca Raton, FL.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems,\n6000–6010. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.\n\n\nWickham, H. 2019. Advanced r, 2nd Ed. Chapman & Hall/CRC\nPress. http://adv-r.had.co.nz/.\n\n\nWickham, H., M. Cetinkaya-Rundel, and G. Grolemund. 2023. R for Data\nScience: Import, Tidy, Transform, Visualize, and Model Data, 2nd\nEd. O’Reilly Media. https://r4ds.hadley.nz/.\n\n\nWing, Jeanette M. 2011. Research Notebook: Computational\nThinking–What and Why? Carnegie Mellon Univeristy School of\nComputer Science. https://people.cs.vt.edu/~kafura/CS6604/Papers/CT-What-And-Why.pdf.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2019. R Markdown:\nThe Definite Guide. Chapman & Hall/CRC Press. https://bookdown.org/yihui/rmarkdown/.\n\n\nXie, Yihui, Dervieux Christophe, and Emily Riederer. 2021. R\nMarkdown Cookbook. Chapman & Hall/CRC Press. https://bookdown.org/yihui/rmarkdown-cookbook/.\n\n\nZewe, Adam. 2025. “Explained: Generative AI’s Environmental\nImpact.” https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117.",
    "crumbs": [
      "References"
    ]
  }
]