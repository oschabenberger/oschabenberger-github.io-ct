[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational and Quantitative Thinking",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Computational & Quantitative Thinking",
    "section": "",
    "text": "1.1 Computational Thinking (CT)\nIn the broad sense, computational thinking (CT) is a problem-solving methodology that develops solutions for complex problems by breaking them down into individual steps. Well, are not most problems solved this way? Actually, yes. We all apply computational thinking methodology every day. As we will see in the example below, cooking a pot of soup involves computational thinking.\nIn the narrow sense, computational thinking is problem solving by expressing the solution in such a way that it can be implemented through a computer—using software and hardware. The term computational in CT derives from the fact that the methodology is based on core principles of computer science. It does not mean that all CT problems are solved through coding. It means to solve problems by thinking like a computer scientists.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Computational & Quantitative Thinking</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-intro-ct",
    "href": "intro.html#sec-intro-ct",
    "title": "1  Computational & Quantitative Thinking",
    "section": "",
    "text": "Elements of CT\nWhat is that like? The five elements of computational thinking are\n\nProblem Definition. What problem are we trying to solve.\nDecomposition (Factoring). Break the problem into smaller parts.\nPattern Recognition. Learning connections and relationships between the parts.\nGeneralization (Abstraction). Recognize relevant details.\nAlgorithmic Design. Define the solution as a series of repeatable steps.\n\nLet’s look at the steps in more detail.\n\nProblem Definition\nThis should go without saying, before attempting to build a solution one should know what problem the solution solves. However, we often get things backwards, building solutions first and then looking for what problems to solve with them. The world of technology is littered with solutions looking for a problem.\n\n\n\n\n\n\nSolution looking for a problem\n\n\n\nSolutions looking for problems are commonplace. If someone is trying to sell you a new car when there is nothing wrong with your current car, they (salesman) have a solution (new car) but there is no problem (new car is not needed). So they might try to create a problem by selling you on the benefits of the new model.\nOn the other hand it is good to innovate and create new solutions even if not all problems the innovation solves are known beforehand. When Karl Benz combined in 1885 a one-cylinder internal combustion engine with a carriage (Figure 1.1), he created the first automobile, the Benz Patent-Motorwagen. He probably did not imagine the many future applications of the technology.\n\n\n\n\n\n\nFigure 1.1: Benz Patent-Motorwagen, the first automobile.\n\n\n\n\n\n\n\nDecomposition (Factoring)\nThis element of computational thinking ask to break the complex problem into smaller, manageable parts and by doing so helps to focus the solution on the aspects that matter, eliminating extraneous stuff.\nSmaller problems are easier to solve and can be managed independently. A software developer decomposes a task into several functions, for example, one that takes user input, one that sorts data, one that displays results. These functions can be developed separately and are then combined to produce the solution. Sorting can further be decomposed into subproblems, for example, the choice of data structure (list, tree, etc.), the sort algorithm (heap, quicksort, bubble, …) and so on.\nTo understand how something works we can factor it into its parts and study how the individual parts work by themselves. A better understanding of the whole results when we reassemble the components we now understand. For example, to figure out how a bicycle works, decompose it into the frame, seat, handle bars, chain, pedals, crank, derailleurs, brakes, etc.\n\n\nPattern recognition\nPattern recognition is the process of learning connections and relationships between the parts of the problem. In the bicycle example, once we understand the front and rear derailleurs, we understand how they work together in changing gears. Pattern recognition helps to simplify the problem beyond the decomposition by identifying details that are similar or different.\n\n\n\n\n\n\nCarl Friedrich Gauss\n\n\n\nCarl Friedrich Gauss (1777–1855) was one of the greatest thinkers of his time and widely considered one of the greatest mathematicians and scientists of all time. Many disciplines, from astronomy, geodesy, mathematics, statistics, and physics list Gauss as a foundational and major contributor.\nIn The Prince of Mathematics, Tent (2006) tells the story of an arithmetic assignment at the beginning of Gauss’ third school year in Brunswick, Germany. Carl was ten years old at the time. Herr Büttner, the teacher wanted to keep the kids quiet for a while and asked them to find the sum of the first 100 integers, \\[\\sum_{i=1}^{100}i\n\\] The students were to work the answer out on their slates and place them on Herr Büttner’s desk when done. Carl thought about the problem for a minute, wrote one number on his slate and placed it on the teacher’s desk. He was the first to turn in a solution and it took his classmates much longer. The slates were placed on top of the previous solutions as students finished. Many of them got the answer wrong, messing up an addition somewhere along the way. Herr Büttner, going through the slates one by one found one wrong answer after another and expected Gauss’ answer also to be wrong, since the boy had come up with it almost instantly. To his surprise–or dismay–Gauss’ slate showed no work, Carl had written on it just one number, 5,050, the correct answer.\nCarl explained\n\nWell, sir, I thought about it. I realized that those numbers were all in a row, that they were consecutive, so I figured there must be some pattern. So I added the first and the last number: 1 + 100 = 101. Then I added the second and the next to last numbers: 2 + 99 = 101. […] That meant I would find 50 pairs of numbers that always add up to 101, so the whole sum must be 50 x 101 = 5,050\n\nCarl had recognized a pattern that helped him see the connected parts of the problem: a fixed number of partial sums of the same value.\n\n\n\n\nGeneralization (Abstraction)\nOnce the problem is decomposed and the patterns are recognized, we should be able to see the relevant details of the problem and how we go about solving the problem. This is where we derive the core logic of the solution, the rules. For example, to write a computer program to solve a jigsaw puzzle, you would not want to write code specific to one particular puzzle image. You want code that can solve jigsaw puzzles in general. The specific image someone will use for the jigsaw puzzle is an irrelevant detail.\nA rectangle can be decomposed into a series of squares (Figure 1.2). Calculating the area of a rectangle as width x height is a generalization of the rule to calculate the area of a square as width-squared.\n\n\n\n\n\n\nFigure 1.2: Decomposing a 12 x 8 rectangle into six 4 x 4 squares to generalize computation of the area\n\n\n\n\n\nAlgorithm design\nThe final element of CT involves another form of thinking, algorithmic thinking. Here we define the solution as a series of steps to be executed. Algorithmic thinking does not mean the solution has to be implemented by a computer, although this is the case in the narrow sense of CT. The point of the algorithm is to arrive at a set of repeatable, step-by-step instructions, whether these are implemented by humans, machines, or a computer. Capturing the solution in an algorithm is a step toward automation.\n\nFigure 1.3 shows an algorithm to produce pumpkin soup, repeatable instructions that lay out the ingredients and how to process them in steos to transform them into soup.\n\n\n\n\n\n\nFigure 1.3: A recipe for pumpkin soup is an algorithm.\n\n\n\n\n\n\nMaking Pumpkin Soup\nLet’s apply the elements of computational thinking to the problem of making pumpkin soup.\n\nDecomposition\nDecomposition is the process of breaking down a complex problem into smaller, more manageable parts. In the case of making pumpkin soup, we can break it down into several steps:\n\nIngredients: Identify the key ingredients required for the soup.\n\nPumpkin\nOnion or Leek\nGarlic\nStock (vegetable or chicken)\nCream (optional)\nSalt, pepper, and spices (e.g., nutmeg, cinnamon)\nOlive oil or butter for sautéing\n\nPreparation: Break down the actions involved in preparing the ingredients.\n\nPeel and chop the pumpkin\nChop the onion and garlic\nPrepare spices and seasoning\n\nCooking: Identify the steps in cooking the soup.\n\nSauté the onion and garlic\nAdd the pumpkin and cook it\nAdd stock and bring to a simmer\nPuree the mixture\nAdd cream and season to taste\n\nFinal Steps: Focus on finishing touches.\n\nGarnish (optional)\nServe and taste for seasoning adjustments\n\n\n\n\nPattern recognition\nWhat are the similar elements or repeating steps in the problem?\n\nCommon cooking steps: Many soups follow a similar structure: sautéing vegetables, adding liquid, simmering, and then blending or pureeing.\nIngredient variations: While the exact ingredients for pumpkin soup may vary (e.g., adding coconut milk instead of cream), the basic framework of the recipe remains the same.\nTiming patterns: There’s a pattern to the cooking times: first sautéing for a few minutes, then simmering the soup for about 20-30 minutes, followed by blending.\n\n\n\nGeneralization\nWe can generalize (abstract) the process of making pumpkin soup into a more general recipe for making any pureed vegetable soup, regardless of the specific ingredients.\n\nEssential components:\n\nA base (onions, garlic, or other aromatics)\nA main vegetable (in this case, pumpkin)\nLiquid (stock, broth, or water)\nSeasoning and optional cream\n\nGeneral process:\n\nSauté aromatics.\nAdd the main vegetable and liquid.\nSimmer until the vegetable is tender.\nBlend until smooth.\nAdjust seasoning and add cream if desired.\n\n\n\n\nAlgorithm design\nHere is a simple algorithm for making pumpkin soup:\n\nPrepare ingredients:\n\nPeel and chop the pumpkin into cubes.\nChop the onion and garlic.\n\nSauté aromatics:\n\nIn a pot, heat oil or butter over medium heat.\nAdd chopped onion and garlic, sauté for 5 minutes until softened.\n\nCook pumpkin:\n\nAdd chopped pumpkin to the pot and sauté for 5 minutes.\nAdd stock to cover the pumpkin (about 4 cups) and bring to a boil.\n\nSimmer:\n\nLower the heat, cover, and let the soup simmer for 20-30 minutes until the pumpkin is tender.\n\nBlend the soup:\n\nUse an immersion blender or transfer the soup to a blender. Puree until smooth.\n\nAdd cream and seasoning:\n\nStir in cream (optional) and season with salt, pepper, and spices to taste (e.g., nutmeg or cinnamon).\n\nServe:\n\nPour into bowls and garnish with optional toppings (e.g., a swirl of cream, roasted seeds, or fresh herbs).\n\n\nFigure 1.3 is a specific implementation of the algorithm.\nBy applying computational thinking, we decomposed the task of making pumpkin soup into smaller steps, recognized patterns in the cooking process, abstracted the general process for making soups, and designed an algorithm to efficiently make pumpkin soup. This method helps streamline the cooking process, ensures nothing is overlooked and provides a clear, repeatable procedure.\n\n\n\nImportance of CT\nComputational thinking as a methodology is applied nowadays in almost any domain and to many problems. Decomposing a problem into parts, recognizing patterns, abstracting the problem essentials and capturing them in an algorithm is an effective approach to deal with many problems.\nThe rise of computational thinking is also due to the fact that many more problems are solved through software and computing today than in the past. We have become very good at capturing the essential details of processes and phenomena through models, which by their very nature are abstractions of the prevailing patterns in those processes. And the implementation of the models frequently involves the design and deployment of algorithms.\nFinancial analysts make investment decisions based on mathematical models of market behavior. Banks decide whether to award a loan based on statistical models for the probability of loan default. Insurance premiums are based on risk models. A healthcare provider chooses a treatment plan based on assessment of risk factors and predictions of disease progression, based on medical knowledge and models that describe patient outcomes.\nIn 2011, Mark Andreessen of VC firm Andreessen-Horowitz (known as a16z) declared\n\nSoftware is eating the world.\n\nWhy was this happening? Andreessen (2011) cites several reasons, among them\n\nSix decades into the computer revolution, four decades since the invention of the microprocessor, and two decades into the rise of the modern Internet, all of the technology required to transform industries through software finally works and can be widely delivered at global scale.\n\nThe largest book seller, Amazon, is a software company. Prior to the software revolution, it was brick-and-mortar book stores like Borders. The largest provider of video services, Netflix, is a software company. Previously, you rented physical video cassette tapes at Blockbuster. The music we listen to today is stored digitally as a file, distributed through software, and made audible through software. The best recruiting company, LinkedIn, is a software company. Some of the best movies are created by Pixar, a software company. You get the idea, I’ll stop here.\nWhile software implementations of problem solving have upended many industries, the way we build and use software, and the types of problems we can solve with it, is itself being upended, thanks to the advances in large-language models (LLMs) like ChatGPT, Claude, Gemini, and others.\nThe shift toward computational thinking and the importance of solving problems through software has become most evident in 2024, when Nobel prizes in Physics and Chemistry were awarded not to scientists in those fields, but to computer scientists and artificial intelligence researchers who developed the foundational computational methods that helped to advance Physics and Chemistry (Figure 1.4 and Figure 1.5).\n\n\n\n\n\n\nFigure 1.4: 2024 Nobel Prize winners in Physics\n\n\n\n\n\n\n\n\n\nFigure 1.5: 2024 Nobel Prize winners in Chemistry\n\n\n\nDemis Hassabis, for example, is the CEO of Google DeepMind, the company behind AI projects such as AlphaGo, the reinforcement-learning trained system that accomplished what was thought impossible for a computer to do: beat the best Go player in the world.\nGeoffrey Hinton is a leading figure in research on artificial neural networks and deep learning. He is often considered the “Godfather of AI”. He is co-author of a 1986 paper that popularized back-propagation, an algorithm that efficiently computes the gradients in a neural network with many layers.\nBreaking with tradition, the Nobel committee awarded these prizes not to scientists who developed grand new theories of how the world works, but to scientist who develop the computational tools that help us develop grand new theories of how the world works.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Computational & Quantitative Thinking</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-intro-qt",
    "href": "intro.html#sec-intro-qt",
    "title": "1  Computational & Quantitative Thinking",
    "section": "1.2 Quantitative Thinking (QT)",
    "text": "1.2 Quantitative Thinking (QT)\nQuantitative thinking (QT) is a problem-solving technique, like computational thinking. It views the world through measurable events, and approaches problems by analyzing quantitative evidence. At the heart of QT lies quantification, representing things in measurable quantities. The word quantification is rooted in the Latin word quantus, meaning “how much”. The purpose of quantification is to express things in numbers. The result is data in its many forms.\n\nBenefits of Quantification\nThe first obvious benefit of quantification is to make information amenable to mathematical and statistical operations. When mathematical or statistical calculations are concerned, any type of data will eventually be represented as numbers. A photograph turns into row and column indices of pixels and three-number triplets of red, green, and blue intensities. A sophisticated large language model that processes textual information converts the text into numeric representations, a process called encoding.\nHuff (1954), in one of the most widely published statistics texts, How to Lie with Statistics states\n\nMany a statistic is false on its face. It gets by only because the magic of numbers brings about a suspension of common sense.\n\nAccording to one view, the de-contextualized and value-free mathematical symbols used in statistical analyses assist in achieving objectivity, stability, and fairness in decisions. Quantification is a method of standardization that summarizes and reduces concepts to their essence and allows us to make better decisions.\nObjective metrics by which to measure allows us to establish rank and status. Status of nations can be measured by GDP (Gross Domestic Product). A countries’ portion of global GPD is one of the criteria for G20 membership. Quantified status is a much better solution than determining status by sending soldiers across borders.\nQuantification allows us to express relationships: ranking, ordering, and measuring proximity. As we will see in a later chapter, sorting and searching are fundamental. As an exercise observe how many times a day you encounter sorted lists, rankings, top-ten results, best-of lists, most-preferred lists and other forms of arranging information by a relevance metric. Google search would not be so successful if searching would not be combined with ranking results by relevance.\n\n\n\nEasy and Difficult to Quantify\nWhen dealing with inherently measurable attributes such as height or weight, quantification is simple. We need an agreed-upon method of measuring and a system to express the measurement in. The former might be an electronic scale or an analog scale with counterweights for weight measurement, a ruler, yardstick, or laser device for height measurements. It seems obvious to report weights in metric units of milligrams, grams, pounds (500 grams), kilograms, and tons or in U.S. Customary units of ounces, pounds (16 ounces), and tons. As long as we know which measurement units apply, we can all agree on how heavy something is. And we need to keep in mind that the same word can represent different things: a metric pound is 500 grams, a U.S. pound is 453.6 grams. But wait, there is more: Apothecaries’ weights are slightly different, a pound a.p. is 12 ounces. And in some fields, weights are measured entirely differently, diamonds are measured in carats (0.2 grams). In the International System of Units (SI), weight is measured in Newtons, which is gravitational force on a mass, equivalent to kg * m /s2. As long as we know what units are used to report a weight, we can convert it into whatever system we want to use. So we could say that this attribute is easily quantifiable.\nOther attributes are difficult to quantify. How do you measure happiness? Finland has been declared the happiest country on earth for seven years running. This must involve some form of quantification otherwise we could not rank countries and declare one as “best”. How did they come up with that? The purpose of the World Happiness Report is to review the science of measuring well-being and to use survey measures of life satisfaction across 150 countries. Happiness according to the World Happiness Report is a combination of many other measurements. For example, a rating of one’s overall life satisfaction, the ability to own a home, the availability of public transportation, etc. Clearly, there is a subjective element in choosing the measurable attributes that are supposed to allow inference about the difficult to measure attribute happiness. Not all attributes weigh equally in the determination of happiness, the weighing scheme itself is part of the quantification. Norms and values also must play a role. The availability of public transportation affects quality of life differently in rural Arkansas and in downtown London. In short, the process of how we quantify a difficult-to-measure attribute should be part of the conversation.\n\n\nAssignment: World Happiness Report\n\n\nRead the section Measuring and Explaining National Differences in Life Evaluations in the 2024 World Happiness Report\n\nWhich variables is the ranking of happiness based on?\nHow many citizens of each country participate in the survey?\nDoes WHR collect its own data or does it rely on someone else’s survey?\nThe data includes three indicators for well-being. Are they all used in determining the happiness rankings?\nIn the discussion of the methods, can you determine whether the happiness rankings involve some form of modeling, where survey responses are tied to other variables? If so, what are the variables? Are these reflected in Table 2.1?\n\n\n\nVariables are difficult to quantify for various reasons:\n\nInconsistent views about what they mean—how do you measure freedom?\nDifficult to define—what is trust?\nSubjectivity—introversion\nAbstract concepts—creativity, natural beauty, etc.\n\nHere is a short list of difficult-to-quantify concepts from different areas:\n\nPersonal and Psychological Attributes\n\n\nEmotions: Happiness, sadness, or anxiety levels can be challenging to measure as they are subjective and context-dependent.\nPersonality traits: Attributes like openness, conscientiousness, or introversion often rely on self-reporting and are hard to measure precisely.\nSelf-esteem: The internal sense of self-worth varies across situations and over time.\nEmpathy: Understanding the emotions of others is influenced by individual perception.\nIntelligence: there are different forms of intelligence (emotional, spatial, linguistic, musical, etc.). IQ tests provide incomplete pictures.\n\n\nSocial and Cultural Variables\n\n\nSocial cohesion: The sense of community and connection within a group.\nCultural values: Abstract beliefs such as collectivism or individualism differ across societies.\nTrust: The degree of confidence people have in others or institutions can be highly context-sensitive.\nSocial norms: Expectations about behavior that are implicit and vary among groups.\n\n\nEthical and Moral Attributes\n\n\nFairness: What is considered “fair” depends on personal, cultural, and contextual factors.\nIntegrity\nJustice: Varied interpretations of what constitutes equitable treatment or outcomes.\n\n\nCreative and Artistic Variables\n\n\nCreativity: Measuring originality and innovation is inherently subjective.\nAesthetic appeal: People’s appreciation of beauty is highly personal and culturally influenced.\nTalent\n\n\nEnvironmental and Ecological Factors\n\n\nBiodiversity value: The intrinsic worth of maintaining species diversity is challenging to calculate in monetary or ecological terms.\nEcosystem health: Assessing overall resilience, productivity, or stability of ecosystems involves many interdependent variables.\nNatural beauty\n\n\nEconomic and Market Variables\n\n\nBrand loyalty is an emotional attachment to a product or service.\nConsumer satisfaction: Highly subjective and influenced by expectations and individual preferences. Innovation potential: The likelihood that a new product or idea will succeed in the market.\n\n\nAbstract Concepts\n\n\nFreedom: The degree to which individuals or groups are free can depend on legal, social, and personal dimensions.\nHappiness: The overall well-being of a population is a composite of subjective factors.\nPotential: The latent ability for growth or success in individuals or systems.\n\nYou can easily grow the list and find examples of difficult-to-quantify variables from many fields. It is a common problem in many sciences: what we are interested in measuring is difficult to quantify and the process of quantification is full of assumptions. Instead of getting at the phenomenon directly, we use other quantities to inform about all or parts of what we are really interested in. These surrogates are known by different names: we call them an indicator, an index (such as the consumer price index), a metric, a score, and so on.\n\n\nExample: Net Promoter Score (NPS)\n\n\nBuilding on the theme of “happiness”, a frequent question asked by companies that sell products or services is “are my customers satisfied and are they loyal?”\nRather than an extensive survey with many questions as in the World Happiness Report, the question of customer satisfaction and loyalty is often distilled into a single metric in the business world, the Net Promoter Score (NPS). NPS is considered by many the gold standard customer experience metric. It rests on a single question: “How likely are you to recommend the companies products or services?”\nThe calculation of NPS is as follows (Figure 1.6):\n\nCustomers answer the question on a scale of 0–10, with 0 being not at all likely to recommend and 10 being extremely likely to recommend.\nBased on their response, customers are categorized as promoters, passives, or detractors. A detractor is someone whose answer was between 0 and 6. A promoter is someone whose answer is 9 or 10. The others, which responded with a 7 or 8 are passives.\nThe NPS is calculated by subtracting the percentage of detractors from the percentage of promoters.\n\nThe NPS ranges from -100 to 100, higher scores imply more promoters. A NPS of 100 is achieved if everyone scores 9 or 10. A score of -100 is achieved if everyone scores 6 or below.\n\n\n\n\n\n\nFigure 1.6: Net promoter score\n\n\n\n\n\n\n\nExercise: Net Promoter Score\n\n\n\nWhat are the assumptions in the NPS calculation?\nCompany Foo improved its NPS from 30 to 40 over the last year. Explain how that can happen.\nWhat does NPS tell you about a company that has many products and/or services?\nWhat impact could cultural differences and societal norms and traditions have on NPS values around the world?\nWhat do you think is a great net promoter score? Does it depend on the industry?\nCompanies are applying NPS in other contexts, not just to measure customer satisfaction. For example, the employee NPS (eNPS) uses NPS methodology and the question “How likely are you to recommend company X as a place of work?” What do you think about that?\nIf you plot NPS by age, what would that look like? In other words, do you expect younger or older consumers to have higher/lower NPS?\nList reasons why NPS is (might be) a troubling indicator. \n\n\n\nThe NPS has many detractors, pun intended. Some describe it as “management snake oil”. Management touts it when the NPS goes up, nobody reports it when it goes down. It continues to be widely used. Forbes reported that in 50 earnings calls of S&P 500 companies NPS was mentioned 150 times in 2018.\n\n\nIndicator and Index\nAn indicator is a quantitative or qualitative factor or variable that offers a direct, simple, unique and reliable signal or means to measure achievements.\nThe economy is a complex system for the distribution of goods and services. Such a complex system defies quantification by a single number. Instead, we use thousands of indicators to give us insight into a particular aspect of the economy: inflation rates, consumer price indices, unemployment numbers, gross domestic product, economic activity, etc.\n\n\nExercise: Quantifying the Economy\n\n\nFind at least two indicators in each of the following aspects:\n\nInternational trade\nHousing and construction\nConsumer spending\nManufacturing\nClimate\nLabor markets\n\nWhat are the indicators used for–that is, what do they indicate?\n\n\nAn indicator is called leading if it is predictive, informing us about what might happen in the future. The job satisfaction in an employee survey is a leading indicator of employee attrition in the future. Unhappy employees are more likely to quit and to move on to other jobs. A lagging indicator is descriptive, it looks at what has happened in the past. Last month’s resignation rate is a lagging indicator for the human resources department.\n\nWhen multiple indicators are combined, we sometimes call it an index, although the distinction is not sharp. Indicators can also be the result of aggregation so that the distinction between indicator and index becomes one of degree of combining information (with an index being more aggregated or combining more individual pieces of information). In some domains, the word index is simply used more frequently than indicators, and vice versa.\n\n\nExample: Body Mass Index (BMI)\n\n\nThe Body Mass Index (BMI) is a medical screening tool for certain health conditions. Colloquially, it is understood as a measure of “fatness”. That is not quite correct, in most people, BMI only correlates with body fat. We will learn more about the concept of correlation (association) later. BMI has become the standard indicator for obesity.\nThe calculation of BMI involves two variables (two indicators): a person’s weight in kilograms (kg) and their height in meters (m): \\[\n\\text{BMI} = \\frac{\\text{Weight in kg}}{(\\text{Height in m})^2}\n\\]\nNotice that the height is squared in the denominator. If you prefer to work in U.S. pounds and inches, the calculation is \\[\n\\text{BMI} = \\frac{\\text{Weight in lbs} \\times 703}{(\\text{Height in inches})^2}\n  \\]\nIn my case (6’3” tall, 208 lbs), the BMI is (208 )/(75^2) = 25.9. According to BMI charts such as this one at the Cleveland Clinic, I am in the overweight range.\nBMI is used widely by medical professionals. People with low values might be at risk for developing anemia, osteoporosis, infertility, malnutrition, and a weakened immune system. High values can indicate a higher risk for heart disease, high blood pressure, type 2 diabetes, gallstones, osteoarthritis, sleep apenea, depression, and certain cancers.\nGeez. It seems that unless you are in the optimal BMI range you are either bound for osteoporosis or osteoarthritis. No wonder folks are obsessing over their BMI.\nThe Cleveland Clinic is quick to point out:\n\nIt’s important to remember that you could have any of the above health conditions without having a high BMI. Similarly, you could have a high BMI without having any of these conditions.\n…\nIt’s important to remember that body fatness isn’t the only determiner of overall health. Several other factors, such as genetics, activity level, smoking cigarettes or using tobacco, drinking alcohol and mental health conditions all affect your overall health and your likelihood of developing certain medical conditions. \n\nAnd\n\nThe standard BMI chart has limitations for various reasons. Because of this, it’s important to not put too much emphasis on your BMI.\n\nPeople who are muscular can have a high BMI and still have very low fat mass. The BMI does not distinguish between lean body mass and fat body mass. BMI charts do not distinguish between males and females, although females tend to have more body fat (says the Cleveland Clinic!). People today are taller than when the BMI was developed. The BMI charts do not apply to athletes, children, pregnant people, or the elderly.\n\nEven though the BMI chart can be inaccurate for certain people, healthcare providers still use it because it’s the quickest tool for assessing a person’s estimated body fat amount.\n\nAh, so it is used because it is easy to calculate, not because it is particularly useful or accurate.\nA January 2025 article in the medical journal The Lancet states (Rubino et al. 2025)\n\nCurrent BMI-based measures of obesity can both underestimate and overestimate adiposity and provide inadequate information about health at the individual level, which undermines medically-sound approaches to health care and policy.\n\nBased on this article, Business Insider went a step further, calling BMI bogus. Having “obesity” according to the BMI scale does not mean a person is unhealthy. In fact, BMI does not tell you anything about the health of a person.\nIt is worthwhile to examine how BMI came about. The comment above about the increasing height of people suggests that BMI was developed some time ago. Indeed. It was invented for an entirely different reason, to describe a population average man in Western Europe in the 19th century.\nTo make this connection we need to introduce Adolphe Quetelet (1796–1847), who invented the BMI to quantify a population according to its person’s weight. It was initially called the Quetelet index. Quetelet was a Belgian astronomer, statistician, and mathematician—not a medical professional. He studied the distribution of physical attributes in populations of French and Scottish people. Quetelet determined that the normal, the most representative value of an attribute, is its average. Prior to Quetelet, the idea of “norm” and “normality” was associated with carpentry and construction. The carpenter square is also called the norm and in normal construction everything is at right angles. The classical notion of ideal as an unattainable beauty up to this time was reflected in great works of art.\nQuetelet focused on the middle of the distribution as the “new normal” and saw l’homme moyen, the average man, as the ideal (Grue and Heiberg 2006).\nThere is no association with health, and there is no association with the individual. The BMI as developed by Quetelet was supposed to describe the average in a population, not obesity of the individual. The population it was intended to describe is French and Scottish of the 19th century. Going from there to a near universal measure of obesity since the 1970s is quite the stretch.\n\n\n\n\nTypes of Data\nData, the result of quantification, can be classified in a number of ways. The first distinction of quantified variables is by data type.\n\nContinuous: the number of possible values of the variable is not countable. Examples are physical measurements such as weight, height, length, pressure, temperature.\nDiscrete: the number of possible values is countable. Even if the number of possible values is infinite, the variable is still discrete. The number of fish caught per day does not have a theoretical upper limit, although it is highly unlikely that a weekend warrior will catch 1,000 fish. A commercial fishing vessel might.\n\nDiscrete variables are further divided into the following groups:\n\nCount Variables: the values are true counts, obtained by enumeration. There are two types of counts:\n\nCounts per unit: the count relates to a unit of measurement, e.g., the number of fish caught per day, the number of customer complaints per quarter, the number of chocolate chips per cookie, the number of cancer incidences per 100,000.\nProportions (Counts out of a total): the count can be converted to a proportion by dividing it with a maximum value. Examples are the number of heads out of 10 coin tosses, the number of larvae out of 20 succumbing to an insecticide,\n\nCategorical Variables: the values consist of labels, even if numbers are used for labeling.\n\nNominal variables: The labels are unordered, for example the variable “fruit” takes on the values “apple”, “peach”, “tomato” (yes, tomatoes are fruit but do not belong in fruit salad).\nOrdinal variables: the category labels can be arranged in a natural order in a lesser-greater sense. Examples are 1—5 star reviews or ratings of severity (“mild”, “modest”, “severe”).\nBinary variables: take on exactly two values (dead/alive, Yes/No, 1/0, fraud/not fraud, diseased/not diseased)\n\n\n\nCategorical variables are also called qualitative variables. They encode a quality, namely to belong to one of the categories. All other variables are also called quantitative variables. Note that quantifying something through numbers does not imply it is a quantitative variable. Highway exits might have numbers that are simple identifiers not related to distances. The number of stars on a 5-star rating scale indicates the category, not a quantified amount. The numeric values of quantitative variables, on the other hand, can be used to calculate meaningful differences and ratios. 40 kg is twice as much as 20 kg, but a 4-star review is not twice as much as a 2-star review—it is simply higher than a 2-star review.\n\n\nTypes of Variables\n\n\n\n\nGive examples for the following types of variables:\n\nNominal\nOrdinal\nCounts per unit\nBinary\nContinuous\n\nWhat are the data types of the following:\n\nnumber of meals served in the cafeteria\nnumber of meals served in the cafeteria per day\nproportion of meals not finished\nmarital status\ntensile strength of a material\npercentage of patients who show side effects\npercentage of income spent on food and housing\n\nReturn to the World Happiness Report from an earlier assignment in this module. What type of data is the Cantril Ladder mentioned in that report?\n\n\n\n\n\n\nUncertainty\nWhen I hop on the scales in the bathroom in the morning, I usually take two or three measurements in quick succession. They rarely are identical, differing by a fraction of a pound. Day-to-day variation could be understandable, our weight does change over time. But second-to-second variation, that seems odd. It seems unlikely that I gained or lost a quarter pound over the last 5 seconds.\nQuantifying things is associated with uncertainty that is introduced through various forms of variability. My weight measurement varies from one measurement to the next because of my posture, movement on the scales, and the performance of the scales mechanism itself. Not because my actual weight differs. This type of variability is called measurement error. If we measure the weights of students in a classroom another source of variability is introduced, subject-to-subject variability. If we randomly choose a classroom from the school, the measurements also represent classroom-to-classroom variability, since repeating the process and selecting a different classroom will yield a different number.\nIn the presence of uncertainty, quantities are not knowable a priori. But if the uncertainty itself can be quantified, then the quantity of interest is predictable. I am unable to say what the average weight is of a student in a freshman class. But by measuring weights and quantifying uncertainty in the observations I can predict with a high level of confidence that the average freshmen students’ weight is between this much and that much (now attach your favorite weight units).\nIt is one of humanities’ great advances that we are not only able to represent concepts in numbers, but that we can quantify uncertainty itself. Probability and statistics enable us to learn from uncertain events and measurements. This adds to our vocabulary of lengths, weights, volumes, five-star ratings, and so on, the terms prediction, forecast, likelihood, odds, chance, and probability.\nA meteorologist forecasting a 30% chance of rain tomorrow is quantifying the likelihood of an event. We all know how to operationalize this forecast. The higher the number, the more likely we will need an umbrella the next day. Interestingly, while we have developed intuition about “the chance of rain”, many do not know how to correctly interpret a statement such as “30% chance of rain”. It does not mean that 30% of the area covered by the forecast will see rain. It does not mean that on days like today, 30% of them will have a rain event. Rather, it means that there is a 0.3 probability that any point in the forecast area will see a measurable amount of rain (usually 0.01 inches or more).\nAnother interpretation flows from the fact that weather forecasts are based on models that simulate weather conditions based on inputs. To account for the inherent uncertainty in measuring the inputs, multiple scenarios (simulations) are run. a 30% chance of rain means that 30% of the simulations predicted rain for the forecast area.\n\nBy applying statistical principles in the analysis of data uncertainty can be reduced. More informed insights about the data are then possible. Suppose we want to measure the average amount of an attribute in a population—say, the average years of postgraduate education. The population is too large to visit with every member so we instead quiz only a smaller number about their education. If we randomly sample one person from the population, we get a statistically valid estimate for the entire population. If they have 2 years of postgraduate education, our best guess for the average amount of post-graduate education in the entire population is 2 years. However, we have not reduced the uncertainty at all, the single measurement is as uncertain as the variability of postgraduate education years in the population. Suppose this variability can be quantified with the amount \\(X\\). If we repeat the process of randomly sampling persons \\(n-1\\) more times and computing the average across the \\(n\\) measurements—this is called the sample average—we get a much better estimate of the average number of years of postgraduate education in the population. It turns out that the sample average so obtained has uncertainty \\(X/n\\). In other words, we can make our statement about the population quantity of interest arbitrarily precise by sampling a large number of people from the population.\nFor attributes with large variability \\(X\\) it will take a larger sample size \\(n\\) to achieve the same level of precision compared to attributes with less variability. If the attribute does not vary at all in the population (\\(X=0\\)), then a single sample is sufficient. In the population of apartment renters there is no variability in the attribute renter, but there is variability in the attribute amount of rent late.\n\n\nSurrogacy\nA surrogate metric is one that is used in the place of another. We have seen earlier how economic indicators are used to describe aspects of the economy:\n\nThe consumer price index is a surrogate metric for consumer behavior, for example.\nThe net promoter score is a surrogate metric for customer loyalty.\n\nWhen quantifying a variable of interest is difficult, time consuming, expensive, or destructive, and a surrogate variable is easy to quantify, the surrogate can take the place of the variable of interest.\nHere are some examples of surrogacy:\n\nHealthcare\n\nCholesterol levels as a proxy for heart health.\n\nEducation\n\nStandardized test scores as surrogates for student achievement.\nGraduation rates as a proxy for work force preparation.\n\nEconomy Gross domestic product as a surrogate for societal well-being.\nEnvironmental Science\n\nCO2 levels as proxy for climate change.\nTree canopy cover as a proxy for biodiversity.\n\nHuman Resources\n\nMeasuring employee productivity by number of hours worked.\nEmployee attrition as a surrogate for employee satisfaction.\n\nTechnology\n\nNumber of downloads of an app as a measure of market share.\nLikes and shares as a measure of social media engagement\n\nAcademia\n\nCitation count as a surrogate for research impact.\n\n\n\n\nExercise: Surrogate Metrics\n\n\nFind ten more examples of surrogate metrics you encounter in daily life.\n\n\nSurrogate measurements are very often necessary, but they are not without issues:\n\nThey might not capture all factors influencing the attribute of interest.\nThey can lead to oversimplification and misinterpretation.\nThe surrogate might not measure what is most relevant. For example, standardized test scores do not measure critical thinking.\nThey can lead to surrogation.\n\n\nSurrogation\nSurrogation occurs when people or organizations substitute a metric for the underlying concept or goal that the metric is intended to represent. Instead of treating the metric as a proxy or tool to measure progress, they treat it as the ultimate goal itself. The metric has become the goal.\nWhen standardized test scores are a surrogate for student achievement, surrogation means to focus on driving up the test scores rather than focusing on driving actual student achievement.\nSomeone can obsess over BMI (body mass index), trying to improve their BMI but not getting any healthier.\nSurrogation of net promoter scores occurs when companies focus on driving up NPS by only asking customers that repeatedly buy their product. The goal is to increase customer satisfaction and loyalty, the NPS should increase as a result. Surrogation focuses on the NPS, not the actual customer satisfaction. The first mistake in surrogation is to assume that the surrogate is exactly what it is a proxy for. To assume that the net promoter score is a proxy for customer satisfaction it is not customer satisfaction.\nSales people are often incentivized on number of deals or deal volume (revenue). The underlying goal is to drive success for the company. Surrogation can lead to closing of deals that are bad for the company (losing money on the deal) but work in favor of the sales person’s metric to hit a revenue target.\n\nWhen metrics are tied to performance evaluation, resource assignment, advancement, surrogation is common. It leads to distorted priorities and bad behavior (gaming the system): people try to meet the metric without achieving the underlying goal.\nAnother aspect of surrogation is to prioritize the simple fixes and quick wins instead of the deeper but more expensive and difficult corrections. A hospital might focus on amenities or quick ER service rather than improving health outcomes for its patients. A social media content creator generates click bait in order to increase clicks rather than focusing on meaningful engagement.\n\n\nExercise: Find Examples of Surrogation\n\n\nFind a few additional real life examples of surrogation, where chasing the metric has become more important than achieving the underlying goal the metric represents.\n\n\n\n\nPredictions as surrogates\nThe predictions of a statistical model are often used as surrogates for values that are difficult or impossible to observe or when the method of observing the value destroys the object itself.\n\n\nExample: Measuring Fitness\n\n\nOne method for quantifying the aerobic fitness of athletes is through testing their aerobic capacity under stress (Figure 1.7). These measurements are time consuming and expensive.\n\n\n\n\n\n\nFigure 1.7: Measuring aerobic capacity.\n\n\n\nImagine that you conduct a study in which a number of athletes are subjected to aerobic capacity tests on a machine similar to that shown in Figure 1.7. In addition to measuring their aerobic capacity, you also measure simple-to-obtain attributes such as heart rate, rest pulse, run pulse, distance run, etc.\nThen, you develop a model that can predict aerobic capacity from the easy-to-measure attributes. If that model works well, you can predict the aerobic capacity of an athlete by obtaining their heart rate, pulse, rest pulse, etc. The prediction from that model is a surrogate for the unobserved aerobic fitness measurement.\nThis is an example of a regression model.\n\n\n\n\nExample: Nail Pull Test\n\n\nThe force required to pull a nail out of a gypsum board (dry wall) is a measure of the board quality. Unfortunately, the test is destructive and the tested board cannot be used. To circumvent the problem and have more boards to sell, a regression model can be built to predict the nail pull strength from other attributes such as the dry wall material, the thickness of the wall, the method of construction, humidity, temperature, etc.\nThese attributes are easy to measure and non-destructive. When the regression model works well—that is, it explains a sufficient degree of variation in nail pull strengths across gypsum boards—the prediction from the model can be used as surrogate for the actual board nail pull strength.\n\n\n\n\nReading Assignment: Chapter 3 in WMD\n\n\nRead Chapter 3, “Arms Race” in O’Neil (2017, 50–62).\n\nWhat is the goal and what is the (surrogate) metric in the U.S. News & World Report?\nHow did surrogation affect the response of universities to the rankings?\nDid the proxies chosen for educational excellence make sense?\nWhat is the effect of leaving tuition and fees out of the ranking model?\nWhat is the similarity of the model for educational excellence that underlies the college rankings and the regression models for aerobic fitness and the nail pull test?\n\n\n\n\n\n\nFigure 1.1: Benz Patent-Motorwagen, the first automobile.\nFigure 1.3: A recipe for pumpkin soup is an algorithm.\nFigure 1.4: 2024 Nobel Prize winners in Physics\nFigure 1.5: 2024 Nobel Prize winners in Chemistry\n\n\n\nAndreessen, Mark. 2011. “Why Software Is Eating the World.” https://a16z.com/why-software-is-eating-the-world/.\n\n\nGrue, Lars, and Arvid Heiberg. 2006. “Notes on the History of Normality–Reflections on the Work of Quetelet and Galton.” Scandinavian Journal of Disability Research 8 (4): 232–46.\n\n\nHuff, Darrell. 1954. How to Lie with Statistics. W.W. Norton & Company, New York.\n\n\nO’Neil, Cathy. 2017. Weapons of Math Destruction. How Big Data Increases Inequality and Threatens Democracy. Crown, New York.\n\n\nRubino, Francesco, David E Cummings, Robert H Eckel, Ricardo V Cohen, John P H Wilding, Wendy A Brown, Fatima Cody Stanford, et al. 2025. “Definition and Diagnostic Criteria of Clinical Obesity.” The Lancet Diabetes & Endocrinology. https://doi.org/10.1016/S2213-8587(24)00316-4.\n\n\nTent, M. B. W. 2006. The Prince of Mathematics: Carl Friedrich Gauss. CRC Press, Boca Raton, FL.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Computational & Quantitative Thinking</span>"
    ]
  },
  {
    "objectID": "cholera.html",
    "href": "cholera.html",
    "title": "2  Case Study: 1854 Cholera Outbreak in Soho, London",
    "section": "",
    "text": "2.1 Two Snows\nIn 1854, a severe outbreak of cholera occurred near Broad Street in Soho, London, killing over 600 people. The outbreak was studied by John Snow, considered one of the founders of modern epidemiology. No, not the Jon Snow you might be thinking of, Lord Commander of the Night Watch (Figure 2.1 (b)), but the John Snow in Figure 2.1 (a).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Case Study: 1854 Cholera Outbreak in Soho, London</span>"
    ]
  },
  {
    "objectID": "cholera.html#two-snows",
    "href": "cholera.html#two-snows",
    "title": "2  Case Study: 1854 Cholera Outbreak in Soho, London",
    "section": "",
    "text": "(a) John Snow. Source: Wikipedia\n\n\n\n\n\n\n\n\n\n\n\n(b) Jon Snow. Source: Wikipedia\n\n\n\n\n\n\n\nFigure 2.1: Famous John/Jon Snows",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Case Study: 1854 Cholera Outbreak in Soho, London</span>"
    ]
  },
  {
    "objectID": "cholera.html#sec-cholera-viz",
    "href": "cholera.html#sec-cholera-viz",
    "title": "2  Case Study: 1854 Cholera Outbreak in Soho, London",
    "section": "2.2 Visualization",
    "text": "2.2 Visualization\nFigure 2.2 shows the map drawn by John Snow, recording the number of cholera cases with stacked bars at the location where cholera cases occurred (click on the map to zoom in). Also shown on the map as black circles and annotated as “PUMP” are the public water pumps throughout the city. The high number of cholera cases on Broad Street stands out, and they seem to be clustered near the location of the Broad Street Pump (Snow 1855, 46).\n\n\n\n\n\n\nFigure 2.2: John Snow’s original cholera map from Soho, London in 1854.\n\n\n\nCholera had been a major problem in the city, thousands had died during previous outbreaks. The prevailing theories of the cause of cholera were (i), airborne particles, called miasma that rose from decomposing organic material and (ii), an as of yet unidentified germ. According to the miasma theory, cholera is contracted by breathing bad air. John Snow adhered to the germ theory and believed that it was transmitted through contaminated water.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Case Study: 1854 Cholera Outbreak in Soho, London</span>"
    ]
  },
  {
    "objectID": "cholera.html#sec-cholera-data",
    "href": "cholera.html#sec-cholera-data",
    "title": "2  Case Study: 1854 Cholera Outbreak in Soho, London",
    "section": "2.3 Data Validation",
    "text": "2.3 Data Validation\nTalking to residents, Snow identified the public water pump on Broad Street to be the source of the outbreak. He failed to identify the germ under the microscope but came to the conclusion based on the pattern in the data and conversations with residents. Investigating on the ground, he found that nearly all deaths were in the vicinity of the Broad Street Pump or by people who had consumed water from the pump (Snow 1855, 47):\n\nIt will be observed that the deaths either very much diminished, or ceased altogether, at every point where it becomes decidedly nearer to send to another pump than to the one in Broad street. It may also be noticed that the deaths are most numerous near to the pump where the water could be more readily obtained.\n\nHe persuaded local authorities to remove the handle from the pump to prevent access to the water. The mortality rate declined after that, but it is believed that the outbreak was already in decline as people had fled the area.\n\n\n\n\n\n\nRemoving the handle\n\n\n\n“Removing the handle” is now a term in epidemiology for the removal of a harmful agent from the environment. When epidemiologists look for simple answers to questions about epidemics, they ask “Where is the handle to this pump?” (Adhikari, DeNero, and Wagner 2022).\n\n\nThere were some data points (outliers?) that did not agree with the hypothesis that proximity to the Broad Street pump resulted in more cholera incidences. At the intersection of Broad Street and New Street was the Lion Brewery; there were no cholera cases at the brewery although it used water from the Broad Street pump. It turns out that the workers there were protected from cholera by virtue of a daily beer allowance. The cholera bacteria is killed in the brewing process making the beer safe to drink. Drinking beer instead of the contaminated water saved the workers from cholera. What appears as an outlier to the model actually reinforces it.\nAdhikari, DeNero, and Wagner (2022) discuss other data points that appeared initially as anomalies and ended up implicating the public pump on Broad Street:\n\nThere were deaths in houses closer to the Rupert Street pump than the Broad Street pump. It was more convenient for those residents to use the Broad Street pump due to the street layout.\nDeaths in houses several blocks away from the Broad Street pump were linked to children who drank from the Broad Street pump on their way to school.\nJohn Snow was initially puzzled by two isolated deaths in the Hampstead area, far from Soho. He learned that the deceased had once lived in Broad Street. Because they liked the taste, they had water from the Broad Street pump delivered to Hampstead every day.\n\nThe well accessed by the Broad Street pump was contaminated with fecal bacteria that leaked into the well from a nearby cesspit. Sewage from the house of a cholera victim had contaminated the well.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Case Study: 1854 Cholera Outbreak in Soho, London</span>"
    ]
  },
  {
    "objectID": "cholera.html#sec-cholera-deeper",
    "href": "cholera.html#sec-cholera-deeper",
    "title": "2  Case Study: 1854 Cholera Outbreak in Soho, London",
    "section": "2.4 Toward Causality",
    "text": "2.4 Toward Causality\n\nBeyond a Reasonable Doubt?\nThe evidence that the contaminated well water at the Broad Street pump caused the high rate of cholera in that neighborhood and among those who consumed the water is strong. Is it conclusive, however? Have we ruled out any other explanation beyond a reasonable doubt?\nThere could be other explanations for the higher cholera incidence rate in the Broad Street neighborhood compared to other areas of London. Maybe the diet is different among the residents of that poorer area. Maybe their occupations expose them to harmful agents at work. Maybe there is something different in the way their houses were constructed.\nWhile we know today that the bacterium Vibrio cholerae causes cholera, that discovery was not made until 1883 and John Snow had failed to identify a “germ” when he studied the Broad Street pump water. The prevailing miasma theory of infection from airborne particles also did not support Snow’s findings. While his data, visualization, and analysis showed a strong association between cholera and proximity to the Broad Street pump, a deeper analysis was necessary to convince his contemporaries.\nTo establish cause and effect and prove that a variable causes an outcome, modern science would design and run an experiment, provided it is ethically and technically possible. In such an experiment one would control for all other factors except the one hypothesized to cause the outcome. One method of controlling these confounding factors is by randomly assigning the conditions of interest to people and to observe what happens. Exposing folks deliberately to contaminated water that could harm or even kill them is not justified. Fortunately, John Snow found a real-life experiment with perfect conditions to establish cause and effect between cholera and water contamination.\n\n\nJohn Snow’s “Experiment”\nHe studied the cholera incidences among recipients of water from two water supply companies. The Lambeth company used water from the River Thames drawn upriver from sewage discharge and the Southwark & Vauxhall company drew water below the discharge. Snow also established that for all intents and purposes the households receiving water from either company were indistinguishable; in statistical terms they were comparable. The only thing that differentiated the two groups was the water supplier. Snow (1855, 75) wrote\n\nIn many cases a single house has a supply different from that on either side. Each company supplies both rich and poor, both large houses and small; there is no difference in the condition or occupation of the persons receiving the water of the different companies…As there is no difference whatever either in the houses or the people receiving the supply of the two Water Companies, or in any of the physical conditions with which they are surrounded, it is obvious that no experiment could have been devised which would more thoroughly test the effect of water supply on the progress of Cholera than this, which circumstances placed ready made before the observer.\n\nTable 2.1 is based on Snow (1855, 80) and covers the period from January 1 to December 12, 1853. The cholera death rate on a per 10,000 house basis is almost 14 times higher in households supplied by Southwark & Vauxhall compared to those who received their water from Lambeth.\n\n\n\nTable 2.1: Cholera incidences and rates for two water supply companies leading up to the 1854 outbreak.\n\n\n\n\n\n\n\n\n\n\n\nWater Supplier\nNo. of Houses\nCholera Deaths\nDeaths per 10,000 Houses\n\n\n\n\nSouthwark & Vauxhall\n40,046\n286\n71\n\n\nLambeth\n26,107\n14\n5\n\n\n\n\n\n\nIn all of London there were 563 deaths from cholera in the same period. In other words, 50% of the deaths took place among customers of the Southwark & Vauxhall company. Ouch.\nFor the first seven week period of the 1854 outbreak, Snow (1855, 86) recorded the death rates in Table 2.2\n\n\n\nTable 2.2: Cholera incidences and rates during the first seven weeks of the outbreak. The death rate in the rest of London was reported as 59 in Table IX of Snow (1855), but calculates to 55 deaths per 10,000.\n\n\n\n\n\n\n\n\n\n\n\nWater Supplier\nNo. of Houses\nCholera Deaths\nDeaths per 10,000 Houses\n\n\n\n\nSouthwark & Vauxhall\n40,046\n1,263\n315\n\n\nLambeth\n26,107\n98\n37\n\n\nRest of London\n256,423\n1,422\n\\(55^*\\)\n\n\n\n\n\n\nIn statistical terms, such a difference in the death rates is highly significant, meaning that if there is no difference in the water quality between the suppliers, such a discrepancy would virtually never happen. The only reasonable explanation for the higher death rate, since differences between the groups receiving the water have been ruled out, is the quality (contamination) of the Southwark & Vauxhall water.\n\n\n\nFigure 2.2: John Snow’s original cholera map from Soho, London in 1854.\n\n\n\nAdhikari, Ani, John DeNero, and David Wagner. 2022. Computational and Inferential Thinking: The Foundations of Data Science. 2nd Ed. https://inferentialthinking.com/chapters/intro.html.\n\n\nSnow, John. 1855. On the Mode of Communication of Cholera, 2nd. Ed. John Churchill, London. https://archive.org/stream/b28985266#page/n3/mode/2up.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Case Study: 1854 Cholera Outbreak in Soho, London</span>"
    ]
  },
  {
    "objectID": "datascience.html",
    "href": "datascience.html",
    "title": "3  Data Science—CT + QT",
    "section": "",
    "text": "3.1 Introduction\nComputational thinking is a problem-solving methodology that breaks down complex problems in manageable parts, finds relationships and patterns among the parts, and delivers solutions as repeatable steps—as algorithms. Quantitative thinking is a problem-solving technique that represents things in measurable quantities, enabling us to manipulate concepts mathematically. Quantitative thinking turns concepts into data.\nCT and QT are used in many disciplines, CT is not just for computer scientists and quantification is not the concern of only mathematicians or statisticians. The two methodologies fit together like hand in glove in the modern discipline of data science.\nThe origins of data science trace back to statistics, the study of information in the presence of uncertainty, computer science, and mathematics. Drawing on these foundational disciplines, data science aims at solving real-world problems using data.\nThe solutions produced by data science involve reports, dashboards, visualizations, algorithms and software. Of particular importance in data science application is using statistical and machine learning techniques to train algorithms on data sets to find patterns and relationships and to generalize these patterns into prediction machines. This effort is referred to as modeling in data science and the algorithms themselves are the data science models.\nYou recognize in this description of data science the elements of computational and quantitative thinking: data, patterns and relationships, generalization, algorithm development.\nA data science investigation starts with a problem: a business, policy, or research question:\nThe real-world problem is translated by the data scientist into one or more analytical categories:\nThe question before the FDA falls in the hypothesis testing category. Based on the data from a clinical trial, can we reject the hypothesis that the new and current drug have the same level of side effects? Answering the question of the retail company involves a combination of description, clustering, and hypothesis testing. The technical support provider will employ a predictive model that links attributes of the support tickets and the customer to the likelihood of response to the survey request. The marketing team might be looking toward a generative AI solutions to produce text descriptions from images of products. The city uses clustering to segment residents into groups that are similar with respect to water usage profiles and develops a separate recommendation model for each group. The grocery store uses association analysis of past purchase records to determine which items are likely to be purchased together. The trucking company uses a combination of predictive models that forecast the likelihood of parts failing and optimization to get a truck to a repair shop with minimal disruption of the route.\nA fundamental aspect of these investigation is the development of a data science model that draws on the essential patterns and relationships in the data to describe, predict, classify, test, recommend, cluster, associate, optimize, and generate.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Science---CT + QT</span>"
    ]
  },
  {
    "objectID": "datascience.html#introduction",
    "href": "datascience.html#introduction",
    "title": "3  Data Science—CT + QT",
    "section": "",
    "text": "Definition: Data Science\n\n\nAt the intersection of the foundation disciplines statistics, mathematics, and computer science, performing data science means drawing conclusions from data about real-world problems using computation and automation in the presence of uncertainty.\n\n\n\n\n\n\nA retail company might wonder whether there is a difference between customers visiting its bricks-and-mortar stores and online shoppers.\nA technical support provider might be interested to determine the drivers behind response/non-response to surveys after tickets have been resolved.\nA city wants to make personalized recommendations to residents on how to reduce water consumption.\nA marketing team is looking for ways to automate the creation of product descriptions.\nA panel at the Food and Drug Administration (FDA) examines whether a new drug shows significantly more side effects than a currently available drug.\nThe operator of a grocery store is wondering whether revenue can be increased by placing certain items closer together on shelves.\nA trucking company wants to move from scheduled maintenance of its fleet to predictive maintenance by equipping trucks with sensors that inform the company when a vehicle requires maintenance or fixing.\n\n\n\nAnalytic categories.\n\n\nCategory\nQuestion asked\n\n\n\n\nDescription\nWhat is and what has been?\n\n\nPrediction\nWhat will be?\n\n\nClassification\nWhat category does this item belong to?\n\n\nHypothesis Testing\nWhat can I say about X?\n\n\nPrescription\nWhat should I do?\n\n\nClustering\nWhich things are similar?\n\n\nAssociation\nWhich things occur together?\n\n\nOptimization\nWhat is the best way to do something?\n\n\nGeneration\nWhat novel content is there?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Science---CT + QT</span>"
    ]
  },
  {
    "objectID": "datascience.html#data-science-models",
    "href": "datascience.html#data-science-models",
    "title": "3  Data Science—CT + QT",
    "section": "3.2 Data Science Models",
    "text": "3.2 Data Science Models\nFrom the 30,000 foot view a model is simply a mechanism to process some input and produce a corresponding output (Figure 3.1).\n\n\n\n\n\n\nFigure 3.1: A simple representation of a model that processes inputs with algorithmic logic and produces output.\n\n\n\nThe input to drive the model algorithm is almost always some form of data. The algorithm that processes the inputs can be based on data, but that is not necessarily so. Suppose the problem we are trying to solve is to assess someone’s annual federal income tax. The problem is solved with a model that takes as input the individuals financial situation. This information is typically known without error as information about income, property taxes, expenses, etc. is well documented. The algorithm processing this input is a translation of the relevant information in the federal income tax laws into machine instructions. The output is the amount of money owed to the government or expected as a refund.\nNow suppose that for some reason the input data in the tax problem is not known without error. For example, income from tips, medical expenses or charitable contributions might be best guesses rather than exact amounts. Income data could be noisy because foreign income is converted at fluctuating exchange rates. If the input data is the realization of stochastic (random) influences, should we modify the algorithm?\nWhen the input data to an algorithm is the result of observing random variation, we are looking to the algorithm of the model to find the signal in the data and to separate it from the noise. The signal located in the data is then transformed into the model output. Most models built in data science are of this kind because data is inherently noisy. The reasons for the random variations are many: selecting observation from a larger population at random, applying treatments to randomly chosen experimental units, variations in measurement instruments and procedures, variations in the environment in which a phenomenon is observed, and so on. The specific algorithms data scientists use depend on the analysis category, properties of the data, assumptions one is willing to make, attributes we look for in competitive models, and personal preferences.\n\nSignal and Noise\nThe signal represents the systematic, non-random effects in the data. Data scientists and statisticians define the noise as the unpredictable randomness around the signal. A slightly different, and also useful, definition of noise stems from intelligence analysis. The signal is the information we are trying to find, the noise is the cacophony of other information that obscures the signal. That information might well be a signal for something else but it is irrelevant or useless for the event the intelligence analyst is trying to predict.\nInformation not being relevant for the signal we are trying to find is the key. In the view of the data scientist, that information is due to random events.\nFinding the signal is not trivial, different analysts can arrive at different models to capture it. Signals can be obscured by noise. What appears to be a signal might just be random noise that we mistake for a systematic effect.\n\n\nExample: Theophylline Concentration\n\n\nFigure 3.2 shows the concentration of the drug theophylline over 24 hours after administration of the drug in two groups of patients. There are 98 data points of theophylline concentration and measurement time. What are the signals in the data? What is noise?\n\n\n\n\n\n\n\n\nFigure 3.2: Theophylline concentration over time in two groups of patients.\n\n\n\n\n\nThe first observation is that the data points are not all the same over time, otherwise they would fall on a horizontal straight line: there is variability in the data. Separating signal and noise means attributing this variability to different sources: some systematic, some random.\nFocusing on either the open circles (group 1) or the triangles (group 2), you notice that points that are close in time are not necessarily close in the concentration measurement. Not all patients were measured at exactly the same time points, but at very similar time points. For example, concentrations were measured after about 7, 9, and 12 hours. The differences in the concentration measurements among the patients receiving the same dosage might be due to patient-to-patient variability or measurement error.\nFocusing on the general patterns of open circles and triangles, it seems that the triangles appear on average below the average circle a few hours after administration. Absorption and elimination of theophylline appears to behave differently in the two groups.\nMuch of the variability in the data seems to be a function of time. Shortly after administering the drug the concentration rises, reaches a maximum level and then declines as the drug is eliminated from the body. Note that this sentence describes a general overall trend in the data here.\nWhich of these sources of variability are systematic—the signals in the data— and which are random noise?\n\nPatient-to-patient variability within a group at the same time of measurement: we attribute this to random differences among the participants.\nPossible measurement errors in determining the concentrations: random noise\nOverall trend of drug concentration over time: signal\nDifferences among the groups: signal\n\nThese assignments to signal and noise can be argued. For example, we might want to test the very hypothesis that there are no group-to-group differences. If that hypothesis is true, any differences between the groups we discern in Figure 3.2 would be due to chance; random noise in other words.\nThe variability between patients could be due to factors such as age, gender, medical condition, etc. We do not have any data about these attributes. By treating these influences as noise, we are making important assumptions that their effects are irrelevant for conclusions derived from the data. Suppose that the groups refer to smokers and non-smokers but also that group 1 consists of mostly men and group 2 consists of mostly women. If we find differences in theophylline concentration over time among the groups, we could not attribute those to either smoking status or gender.\n\n\nFinding the signal in noisy data is not trivial. The opposite can also be difficult: not mistaking noise for a signal. Figure 3.3 is taken from Silver (2012, 341) and displays six “trends”. Four of them are simple random walks, the result of pure randomness. Two panels show the movement of the Dow Jones Industrial Average (DJIA) during the first 1,000 trading days of the 1970s and 1980s. Which of the panels are showing the DJIA and which are random noise?\n\n\n\n\n\n\nFigure 3.3: Figure 11-4 from Silver (2012), Random walk or stock market data?\n\n\n\nWhat do we learn from this?\n\nEven purely random data can appear non-random over shorter sequences. We can easily fall into the trap of seeing a pattern (a signal) where there is none. Sometimes there is no signal at all. After drawing two unlikely poker hands in a row there is not a greater chance of a third unlikely hand unless there is some systematic effect (cards not properly shuffled, game rigged). Our brains ignore that fact and believe that we are more lucky than is expected by chance.\nData that contains clear long-run signals—the stock market value is increasing over time—can appear quite random on shorter sequences. One a day to day basis predicting whether the market goes up or down is very difficult. In the long run ups and downs are almost equally likely. Upswings have a slight upper hand and on average are greater than the downswings, increasing the overall value in the long term. Traders who try to beat the market over the short run have their work cut out for them.\n\nBy the way, panels D and F in Figure 3.3 are from actual stock market data. Panels A, B, C, and E are pure random walks. It would not be surprising if investors would bet money on “trend” C.\n\n\nExercise: Southern Oscillation Index (SOI)\n\n\nThe Southern Oscillation Index (SOI) is a standardized index based on the observed sea level pressure differences between Tahiti and Darwin, Australia. The SOI is one measure of the large-scale fluctuations in air pressure occurring between the western and eastern tropical Pacific (i.e., the state of the Southern Oscillation) during El Niño and La Niña episodes.\nIn general, smoothed time series of the SOI correlate highly with changes in ocean temperatures across the eastern tropical Pacific. The negative phase of the SOI represents below-normal air pressure at Tahiti and above-normal air pressure at Darwin. Prolonged periods of negative (positive) SOI values coincide with abnormally warm (cold) ocean waters across the eastern tropical Pacific typical of El Niño (La Niña) episodes (Figure 3.4).\nAccording to Wikipedia, there have been about 30 El Niño episodes since 1950 with strong El Niño events in 1982–83, 1997–98, and 2014–16. You recognize El Niño when the SOI dips negative for a period of time. La Niña is marked by periods of positive SOI values.\n\n\n\n\n\n\n\n\nFigure 3.4: Monthly SOI data from 1951 to mid-2023 according to NOAA.\n\n\n\n\n\n\nWhat is the signal and the noise in these data?\nIs it possible that there are multiple signals in these data, associated with different time horizons?\n\n\n\n\n\nChoosing a Model\nSelecting the right type of model is a critical step in any data science project. There are many choices based on input data, learning methodology and analysis category. Figure 3.5 is an attempt at structuring the input, algorithm, and output components of a model in the data science context. The diagram is complex and yet woefully incomplete and is intended to give you an idea of the diversity of methods and the many ways we can look at things. For example, in discussing input data we could highlight how data are stored, how fast it is moving, the degree to which the data is structured, the data types, and so forth. There are many other categorizations of data one could have listed.\nThe categorization of algorithms leaves out some approaches to learning from data to keep things (relatively) simple. Volumes of books and papers have been written about every item in the list of algorithms and many algorithms are represented by a single entry. Multi-layer networks, for example, include artificial neural networks, deep networks such as convolutional and recurrent networks, and transformer architectures such as GPT.\n\n\n\n\n\n\nFigure 3.5: Structuring and categorizing input, algorithm, and output in data science models.\n\n\n\nArguably the most important class of algorithms learning from data is supervised learning. The name originates in thinking of learning in an environment that is supervised by a teacher. The teacher asks questions for which they know the correct answer (the ground truth) and judge a student’s response to the questions. The goal is to increase students’ knowledge as measured by the quality of their answers. But we do not want students to just memorize answers, we want to teach them to be problem solvers, to apply the knowledge to new problems, to generalize.\nSupervised learning is characterized by the presence of a target variable, also called a dependent variable, response variable, or output variable. This is the attribute about which we wish to draw conclusions. For example, to predict the probability that someone contacting customer support will later fill out a survey, the target variable is whether they responded to the survey request or not. To model the strength of drywall a target variable might be the force required to pull a nail from the board.\nOther variables in our data set, beside the target variable, are potentially input variables to our model. In modeling the nail pull strength of drywall input variables might be the thickness of the board, the moisture, the chemistry of the gypsum, characteristics of the manufacturing environment, and so forth.\nThe values of the target variable are also called the labels in machine learning. This name stems from image analysis where prior to training an algorithm human interpreters go through the images in the training data and label them (Figure 3.6).\n\n\n\n\n\n\nFigure 3.6: Labeling images prior to training an image processing network. Source.\n\n\n\nThis process establishes the ground truth, the correct answer which the algorithm associates with the other variables in the data during training. Because the algorithm makes the connection between input variables and target variable at this stage, we can later apply the trained algorithm to new observations for which the label (the true value) is unknown.\nSupervised learning can have many goals. For example:\n\nPredict the target variable from input variables.\nDevelop a function that approximates the underlying relationship between inputs and outputs.\nUnderstand the relationship between inputs and outputs.\nClassify observations into categories of the target variable based on the input variables.\nGroup the observations into sets of similar data based on the values of the target variable and based on values of the inputs.\nReduce the dimensionality of the problem by transforming target and inputs from a high-dimensional to a lower-dimensional space.\nTest hypotheses about the target variable.\n\nData science projects can have multiple of these goals. For example, you might be interested in understanding the relationship between target and input variables and use that relationship for predictions.\nTo close the loop, we can now map a teacher teaching a classroom of students to the problem of building a machine learning model through supervised learning.\n\nThe problems asked by the teacher, the learning algorithm, are the data points.\nThe values of the target variable are the the correct answers.\nThe input variables represent the information used by the students to answer the questions.\n\n\n\nRegression and Classification\nWithin the supervised learning category regression and classification models are the most important model types.\nIn a regression context we are interested in the relationship between the mean of a target variable and the inputs. The goal is to make predictions about the mean of the target variable when the input variables take on certain values.\nIn a classification context the target variable is categorical (see Section 1.2.4), for example, makes of cars, or names of fruit, object categories on images, or the 10,000 words in a dictionary. Based on the values of input variables we wish to classify a new observation into one of the possible categories. For example, we measure health attributes such as LDL, weight, age, etc. along with the target variable, whether someone is at low, medium, or high risk of developing coronary heart disease. The process of training the model involves data from people with confirmed low, medium, and high risk of heart disease. Based on the classification model so developed, a physician can classify a new patient as low, medium, or high risk based on information about their LDL, weight, age, etc.\n\n\nGeneralization\nOur teacher has the noble goal of not just drilling the correct answers but to turn the students into problem solvers who can apply concepts to solve new questions. What is the parallel of this concept in supervised machine learning?\nSuppose we can quantify the quality of a student’s answer, for example by using a metric that increases the further their answer is of the mark. When an answer is correct, the metric returns 0. When should the teacher stop teaching? When all the students can answer all the questions correctly? At this point our metric across the training questions would be zero. The teacher has done a good job training students on that set of questions, but have they learned to solve new, previously unseen, problems?\nWe need to throw in some new problems and measure how well the students do on those. Getting great answers (small discrepancy metric) on new questions shows that the students can apply the learned concepts to new problems. They are able to generalize. Recall from Section 1.1 that generalization is one of the steps in computational thinking, just prior to algorithm design. In data science, the process of training a model leads to an algorithm that can perform the analytic goal. The way in which the model is trained ensures that the algorithm generalizes well to new observations, those not in the training data.\nJust like the teacher is not interested in drilling the correct answers to the questions in the lesson plan, we are not interested in building models that follow the training data too closely. Models that suffer from that problem are overfitting the data and do not generalize well. They treat too much of the noise as a worthwhile signal.\n\n\nExample: Melanoma Incidences\n\n\nThe data displayed in Figure 3.7 are from the Connecticut Tumor Registry and represent age-adjusted numbers of melanoma incidences per 100,000 people for the 37 years from 1936 to 1972 (Houghton, Flannery, and Viola 1980).\n\n\n\n\n\n\n\n\nFigure 3.7: Incidences of melanoma per 100,000 people from 1936 to 1972\n\n\n\n\n\nThere is noise and signal in the data. While the data points do not fall on a perfect trend line, there is a general upward trend in the number of melanoma incidences per 100,000 over the 37-year period. There is also a shorter-term oscillation around the overall trend.\nFigure 3.8 shows the data and four possible models to capture the signal in the data. The solid black line tries to capture the overall trend. While this model might be OK to describe the trend over 30+ years, it is underfit with respect to the shorter oscillations of the incidence rate. The dotted (red), dashed (blue), and dot-dashed (dark green) lines try to capture these shorter-term signals in the data. The green line appears overfit, it follows the observed data too closely, almost interpolating the dots. The other two lines fall somewhere between the two extreme models.\n\n\n\n\n\n\n\n\nFigure 3.8: Models of different degree of smoothness for the Melanoma data.\n\n\n\n\n\nDetermining the appropriate amount of signal extraction, balancing overfitting and underfitting, is an important aspect of all data science modeling projects. An underfit model does not generalize well because it is too simple. An overfit model does not generalize well because it is too sensitive to unseen observations. Striking the balance between over- and under-fit is also known as the bias-variance tradeoff in data science.\n\n\n\n\n\nAll Models are Wrong\nGeorge E.P. Box is credited with coining the much-used phrase “all models are wrong, but some are useful”. The phrase appears partially (“all models are wrong”) twice in his 1976 paper on Science and Statistics (Box 1976):\n\nSince all models are wrong the scientist cannot obtain a “correct” one by excessive elaboration.\nSince all models are wrong the scientist must be alert to what is importantly wrong.\n\nThe full phrase appears on p. 424 of his book with Norman Draper (Box and Draper 1987).\nThe first G.E.P. Box quote instructs us not to overdo it in building models; this translates to the problem of overfitting, crafting a model that follows the training data too closely and as a result does not generalize well to new data points. If the goal is to predict, classify, or cluster the unseen; generalizability of the model is key. A model to forecast stock prices or trading volumes is judged by how well it can predict the future, not by how well it can predict the past. The adequate level of generalization for that model must be wrung from current and past stock prices.\nThe second G.E.P. Box quote instructs us that models are abstracting away features of the phenomenon. If these are important features, the model is not useful. In the best case this model does not meet its goal and is revised or abandoned. In the worst case the model leads to bad decisions and harmful outcomes.\nThe important lesson is that any model is an abstraction of a phenomenon and we strive to find a useful abstraction. The model does not attempt to reproduce the phenomenon. The tax algorithm converts the essence of the tax code into machine instructions, it is not an electronic copy of the entire law. The purpose is to accurately calculate an entity’s tax, anything else can be stripped away. An algorithm processing noisy data that reproduces the data is uninteresting. The goal is to abstract the data in such a way to allow separating the signal from the noise and to convert the signal into the desired output.\nThe assumptions we make in building models are very important. Violations of the assumptions can lead to biased conclusions when the model does not represent the phenomenon it is supposed to abstract.\n\n\nAssignment: Compartmental Models in Epidemiology\n\n\nThe SIR (Susceptible–Infectious–Recovered) model is a standard class of compartmental models in epidemiology, describing how an infectious disease moves through a population. On the surface this makes sense: at each point in time an individual is in one of three states, called compartments:\n\nYou have not had the disease but you are susceptible to it (S compartment)\nYou are infected by the disease (I compartment)\nYou are recovered from the disease, or dead (R compartment)\n\nA vaccination, then, is a shortcut that moves an individual directly from the S to the R compartment, bypassing the infected state.\nHowever, there are a number of assumptions in the SIR model that can invalidate the model for some diseases and circumstances:\n\nThe disease progresses in only one direction, from S to I to R.\nEveryone is equally susceptible and behaves the same way.\nEveryone is equally likely to be vaccinated, if a vaccine is available.\nAll members of the populations intermingle at random.\n\nThe rate at which the disease spreads through the population is measured by the basic reproduction number, \\(R_0\\). This number, which we became all too familiar with during the COVID-19 pandemic, measures the number of uninfected people expected to catch the disease from an infected individual. An \\(R_0\\) of 3 means that someone who contracts the disease is expected to pass it on to three other individuals. In the absence of vaccines or quarantines, any disease with \\(R_0 &gt; 1\\) will eventually spread to the entire population.\n\nDiscuss whether the assumptions of the SIR model apply to the COVID-19 pandemic.\nWhat could be reasons why epidemiologists hold on to a SIR model even if one or more of its assumptions are violated?\nIn some countries COVID intervention measures were directly related to \\(R_0\\), often called the R-number. If a three-day average of \\(R_0\\) was above a certain threshold more stringent COVID restrictions took effect. If the three-day average \\(R_0\\) dropped below a threshold restrictions were relaxed. \\(R_0\\) is calculated either retrospectively from epidemiological data (such as data from contact tracing) or using theoretical mathematical models based on differential equations such as the SIR model. Discuss the pros and cons of the two approaches.\nIs \\(R_0\\) a biological constant of a pathogen, or is it a function of human behavior and characteristics of the pathogen?\n\n\n\n\nModels that explicitly incorporate uncertainty are called stochastic models and play an important role in analyzing data. Partly because data are inherently variable and noisy. Partly because describing something in stochastic terms—letting random elements account for some of the variability we see—enables us to model highly complex phenomena in relatively simple terms. The example of modeling coin flips will make the concept clearer.\n\n\nExercise: Modeling Coin Flips\n\n\nSuppose we want to predict whether a coin, when flipped, lands on Heads or Tails. We can try and develop a mathematical model that captures the forces acting on the coin: its starting position, the angle and momentum when it is released, conditions of the environment such as wind and temperature (does that matter?), conditions of the landing surface (angle, softness, etc.), conditions of the coin (surface, weight, material, shape, damage, etc.) and so on and so on. It will turn into a very complicated model with many sub-models. We will have to make assumptions, for example, that wind speed and direction are constant, that the coin is perfectly round and flat.\nWe end up with a very complex model, full of individual abstractions. Whether we will be able to perfectly predict whether the coin lands on Heads or Tails depends on whether we model the sub-processes correctly, whether we combine them correctly into an overall model, and whether the assumptions are met.\nA much simpler model would be to not try and predict any particular flip of the coin and to acknowledge that the forces that determine how a coin lands are essentially random (stochastic). The stochastic model for the coin flip is extremely simple. Every flip is assumed to be the realization of a random experiment with two possible outcomes: Heads and Tails. Our model rules out that the coin lands on each side. If the coin lands at an angle we call the side that faces up. The only parameter of the random experiment is the probability that the coin lands on Heads (or Tails). If the coin is fair and is properly tossed, that probability should be 0.5.\nWe can even run an experiment with a particular coin and flip it 100 times to get an estimate of the parameter. If it lands on Heads 45 times out of 100 tosses, then the probability of Heads is 0.45.\n\n\n\n\n\nFigure 3.5: Structuring and categorizing input, algorithm, and output in data science models.\n\n\n\nBox, George E. P. 1976. “Science and Statistics.” Journal of the American Statistical Association 71 (356): 791–99.\n\n\nBox, George E. P., and Norman R. Draper. 1987. Empirical Model-Building and Response Surfaces. John Wiley & Sons, New York.\n\n\nHoughton, A. N., J. Flannery, and M. V. Viola. 1980. “Malignant Melanoma in Connecticut and Denmark.” International Journal of Cancer 25: 95–104.\n\n\nSilver, Nate. 2012. The Signal and the Noise: Why so Many Predictions Fail–but Some Don’t. Penguin Books.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Science---CT + QT</span>"
    ]
  },
  {
    "objectID": "firstStepsR.html",
    "href": "firstStepsR.html",
    "title": "4  First Steps in R",
    "section": "",
    "text": "4.1 Getting Started with R\nTo get started with R as a statistical programming language you need access to R itself and a development environment from which to submit R code.\nDownload R for your operating system from the CRAN site. CRAN is the “Comprehensive R Archive Network” and also serves as the package management system to add new packages to your installation.\nIf you use VS Code as a development environment, add the “R Extension for Visual Studio” to your environment. We are focusing on RStudio as a development environment here.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>First Steps in `R`</span>"
    ]
  },
  {
    "objectID": "firstStepsR.html#sec-getting-started-R",
    "href": "firstStepsR.html#sec-getting-started-R",
    "title": "4  First Steps in R",
    "section": "",
    "text": "TL;DR What you Need\n\n\n\n\n\nTo work with R in this course, you need to be able to run R code, mix it with text in a notebook-style environment, and turn program and output into pdf and html files. To accomplish this you will need\n\nR. Download from CRAN\nRStudio. Download RStudio Desktop from Posit\n\nYou can skip R and RStudio installs if you do the work in a Posit Cloud account. These are available for free here.\n\n\n\n\n\n\n\nPosit Cloud\nIn today’s cloud world, you can get both through Posit Cloud. Posit is the company behind RStudio, Quarto, and other cool tools. Their cloud offering gives you access to an RStudio instance in the cloud. You can sign up for a free account here. The only drawback of the free account is its limitations in terms of RAM, CPU, execution time, etc. For the work you will be doing in this course, and probably many other courses, you will not exceed the limitations of the free account.\nOnce you have created an account, the work space is organized the same way as an RStudio session on your desktop.\n\n\nR and RStudio\nRStudio is an integrated development environment (IDE) for R, but supports other languages as well. For example, using Quarto in RStudio, you can mix R, Python, and other code within the same document.\nDownload Rstudio Desktop here.\nThe RStudio IDE is organized in panes, each pane can have multiple tabs (Figure 4.1). The important panes are\n\nSource. The files you edit. These can be R files (.R), Rmarkdown (.Rmd), Quarto (.qmd), or any other text files.\nConsole. Here you can enter R commands directly at the command prompt “&gt;”. This pane also has a Terminal tab for an OS terminal and a Background Jobs tab. The latter is important when you knit documents into pdf or html format.\nEnvironment. Displays information about the objects created in the R session. You can click on an object for a more detailed look at it in the Viewer.\nHelp. This pane contains many useful tabs, such as a File browse, package information, access to the documentation and help system. Plots generated from the Console or from an R script are displayed in the Plots tab of this pane.\n\n\n\n\n\n\n\nFigure 4.1: RStudio IDE\n\n\n\n\n\nInstalling Packages\nThe R installation comes with attached base packages, you do not need to install or load those. To access capabilities in other packages you need to go through a two-step process:\n\nInstall the package\nLoad the package in your R session with the library() command.\n\nInstalling the package is done once, this step adds the package to your system. Loading the library associated with the package is done in every R session that needs to use the functionality of the package.\nA standard R package is made available through the CRAN (Comprehensive R Archive Network) repositories. For the analytic work in this module we need to install three packages from CRAN: ISLR2, rpart and rpart.plot.\nCopy the following line of code to the console in RStudio and hit .\n\ninstall.packages(c(\"rpart\",\"rpart.plot\",\"ISLR2\"))\n\nThe ISLR2 library provides data sets that are used in James et al. (2021). rpart is a popular library for training decision trees on data and rpart.plot produces nice-looking visualizations of decision trees.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>First Steps in `R`</span>"
    ]
  },
  {
    "objectID": "firstStepsR.html#regression-trees",
    "href": "firstStepsR.html#regression-trees",
    "title": "4  First Steps in R",
    "section": "4.2 Regression Trees",
    "text": "4.2 Regression Trees\n\nBasic Construction\nFor this application we use the Hitters data on performance and salaries of baseball players in the 1986/1987 seasons, the data are provided in the ISLR2 library. Because the salaries are highly skewed a log transformation is applied prior to constructing the tree (Figure 4.2). We will examine later the effect of the log-transformation on the tree.\n\nlibrary(ISLR2)\ndata(Hitters)\n\npar(mfrow=c(1,2))\nplot(density(na.omit(Hitters$Salary)), main=\"Salary\")\nplot(density(na.omit(log(Hitters$Salary))), main=\"log(Salary)\")\n\n\n\n\n\n\n\nFigure 4.2: Salaries and log(Salaries) for the Hitters data.\n\n\n\n\n\nThe following statements load the rpart library and request a decision tree according to the model formula. Note that not all variables specified in the model are necessarily used in constructing the tree. method=\"anova\" requests a regression tree, the complexity criterion is the residual sum of squares. Since the target variable is continuous, rpart defaults to a regression tree in this case and the method= specification is not necessary. However, because rpart has several tree-building methods to choose from, it is not a bad idea to state explicitly what is intended.\nNote that a random number seed is set prior to the call to rpart since the function performs 10-fold cross-validation by default. While the tree returned by rpart is not affected by the seed, the evaluations of the complexity (penalty) parameter during cross-validation depend on the random number stream.\n\n\nTree Summary\n\nlibrary(rpart)\nset.seed(87654)\nt1 &lt;- rpart(log(Salary) ~ Years + Hits + RBI + Walks + Runs + HmRun + \n                PutOuts + AtBat + Errors, \n            data=Hitters)\n\nsummary(t1, cp=0.2)\n\nCall:\nrpart(formula = log(Salary) ~ Years + Hits + RBI + Walks + Runs + \n    HmRun + PutOuts + AtBat + Errors, data = Hitters)\n  n=263 (59 observations deleted due to missingness)\n\n          CP nsplit rel error    xerror       xstd\n1 0.44457445      0 1.0000000 1.0024572 0.06529349\n2 0.11454550      1 0.5554255 0.5936326 0.06177974\n3 0.04446021      2 0.4408800 0.5190135 0.06426742\n4 0.01831268      3 0.3964198 0.4639453 0.06373984\n5 0.01806689      4 0.3781072 0.4734726 0.06728376\n6 0.01678632      6 0.3419734 0.4776621 0.06718932\n7 0.01617738      7 0.3251871 0.4679886 0.06739159\n8 0.01000000      8 0.3090097 0.4570960 0.06731152\n\nVariable importance\n  Years    Hits    Runs   AtBat     RBI   Walks   HmRun PutOuts \n     38      13      12      10      10      10       5       4 \n\nNode number 1: 263 observations,    complexity param=0.4445745\n  mean=5.927222, MSE=0.7876568 \n  left son=2 (90 obs) right son=3 (173 obs)\n  Primary splits:\n      Years &lt; 4.5   to the left,  improve=0.4445745, (0 missing)\n      Hits  &lt; 117.5 to the left,  improve=0.2229369, (0 missing)\n      RBI   &lt; 43.5  to the left,  improve=0.2161883, (0 missing)\n      AtBat &lt; 472.5 to the left,  improve=0.1865249, (0 missing)\n      Walks &lt; 34.5  to the left,  improve=0.1678353, (0 missing)\n  Surrogate splits:\n      HmRun &lt; 1.5   to the left,  agree=0.696, adj=0.111, (0 split)\n      RBI   &lt; 24.5  to the left,  agree=0.688, adj=0.089, (0 split)\n      Walks &lt; 11.5  to the left,  agree=0.681, adj=0.067, (0 split)\n      Runs  &lt; 13.5  to the left,  agree=0.673, adj=0.044, (0 split)\n      Hits  &lt; 29.5  to the left,  agree=0.669, adj=0.033, (0 split)\n\nNode number 2: 90 observations\n  mean=5.10679, MSE=0.4705907 \n\nNode number 3: 173 observations\n  mean=6.354036, MSE=0.4202619 \n\n\nThe summary function produces a lengthy listing of the tree. The cp= option is used here to specify a cutoff value for displaying nodes that fall below the value of the complexity parameter, simply to limit the amount of output. The cp= option on the summary call prunes the output, it does not prune the tree.\nThe first table lists the result of 10-fold cross-validation of the cost-complexity parameter, more on this below. The variable importance listing ranks variables by a combination measure that accounts for the quality of split where the variable was the primary split variable, and an adjustment for splits where the variable was a surrogate. The sum of the variable importance measures is 100, but due to rounding it might sum to a slightly different value. A variable that has a relative high variable importance might not get used in the final tree, due to pruning.\nThe first node at the trunk of the tree contains 263 observation. The best split variable at this level is Years with split value of 4.5. Using this variable and split point leads to the largest improvement in the complexity criterion. rpart also constructs a list of the surrogate variables should the primary split variable be missing. For this node a missing value in Years would be split on HmRun. The numbers in parentheses tell you how many observations actually had missing values and which split variables were used as surrogates. For these data, only the target variable contains missing values.\nrpart removes observations with missing values from the analysis if the target is missing or if all inputs are missing. The missing values in this data set occur only in the target variable, which means that surrogates were not used in training the model.\nA more concise listing of the tree is obtained with the print method.\n\nprint(t1)\n\nn=263 (59 observations deleted due to missingness)\n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 263 207.153700 5.927222  \n   2) Years&lt; 4.5 90  42.353170 5.106790  \n     4) Years&lt; 3.5 62  23.008670 4.891812  \n       8) Hits&lt; 114 43  17.145680 4.727386 *\n       9) Hits&gt;=114 19   2.069451 5.263932 *\n     5) Years&gt;=3.5 28  10.134390 5.582812  \n      10) Runs&lt; 74.5 21   4.229230 5.379350 *\n      11) Runs&gt;=74.5 7   2.427815 6.193200 *\n   3) Years&gt;=4.5 173  72.705310 6.354036  \n     6) Hits&lt; 117.5 90  28.093710 5.998380  \n      12) Walks&lt; 21.5 26   5.001884 5.687706 *\n      13) Walks&gt;=21.5 64  19.562870 6.124591  \n        26) Years&lt; 6.5 18   6.411418 5.727128 *\n        27) Years&gt;=6.5 46   9.195161 6.280120 *\n     7) Hits&gt;=117.5 83  20.883070 6.739687  \n      14) Walks&lt; 60.5 50  10.225000 6.576444 *\n      15) Walks&gt;=60.5 33   7.306869 6.987024 *\n\n\nThere are a total of 15 nodes in the tree and 9 terminal nodes (leaves) as indicated by asterisks. For each node, the listing shows the split variable and split point, the number of observations, the residual sum of squares, and the representative value for the node. For example, at the root node (trunk) of the tree there are 263 observations, the sum of squares \\(\\sum (y-\\overline{y})^2 = 207.153\\) and the average log salary is 5.9272. Ninety observations are split off to the left at the first split on Years with split value of 4.5. The sum of squares of those ninety observations is 42.353 and their average is 5.1067.\n\nThe easiest way to consume the results of constructing a tree is by visualizing the tree. The rpart.plot function in the rpart.plot package creates good-looking trees and has many options to affect the rendering. Figure 4.3 displays the regression tree for the Hitters data built so far.\n\nlibrary(rpart.plot)\nrpart.plot(t1,roundint=FALSE)\n\n\n\n\n\n\n\nFigure 4.3: Default regression tree for log(salary) constructed by rpart for Hitters data.\n\n\n\n\n\nThe boxes annotating the nodes display the representative (predicted) value for the node and the percent of observations that fall into the node. The intensity of the node color is proportional to the value predicted at the node.\nIf we were to choose a “stump”—that is, a tree with a single split, we would predict a log salary of 6.4 for players who have been in the league for more than 4.5 years and a salary for 5.1 for the players with less tenure. For a player with 7 years experience, 100 hits and 30 walks in 1986 we would predict a log salary of 6.3.\nNote that not all variables in the model are used in constructing the tree. Variables that do not improve a tree at any split in a meaningful way do not show up. For example, the number of RBIs or errors in 1986 are not used as primary split variables. This does not mean that these variables are unrelated to the target, they can enter the model indirectly through correlation with other variables. They might also be used as surrogate split variables, which the figure does not convey.\nAlso note that variables can be reused. A variable that serves as the primary split variable can later be used again as a split variable. For example, Years is the first split variable and is used again on both sides of the tree. The number of Walks in 1986 separates observations differently for longer-tenured players that have less than 118 Hits and those who have more.\n\nWhat would the tree look like if we had not transformed the target variable by taking logarithms?\n\nt2 &lt;- rpart(Salary ~ Years + Hits + RBI + Walks + Runs + HmRun + \n                PutOuts + AtBat + Errors, \n            data=Hitters,\n            method=\"anova\")\n\nrpart.plot(t2,roundint=FALSE)\n\n\n\n\n\n\n\nFigure 4.4: Default regression tree for salary constructed by rpart for Hitters data.\n\n\n\n\n\nClearly, Figure 4.4 is not the same tree as shown in Figure 4.3. Although the log transformation is monotonic for the positive salary values, it affects the distribution of the target variable and its relationship to the inputs.\nHowever, if we apply a monotone transformation to inputs, the constructed tree does not change. The following code fits the initial tree but uses log(years) instead of years. Only the values for the split variable change, but not the tree (Figure 4.5). Compare this to Figure 4.3.\n\nt3 &lt;- rpart(log(Salary) ~ log(Years) + Hits + RBI + Walks + Runs + HmRun + \n                PutOuts + AtBat + Errors, \n            data=Hitters,\n            method=\"anova\")\n\nrpart.plot(t3,roundint=FALSE)\n\n\n\n\n\n\n\nFigure 4.5: Default regression tree for log(salary) with log-transformed year as input.\n\n\n\n\n\nChanging in the model from years to log(years) does not impact the splits or split values for any of the other inputs. This is fundamentally different from a linear regression model where transforming one input variable affects the coefficients of other inputs unless they are completely uncorrelated. Compare the following:\n\nround(\n    lm(log(Salary) ~ Years + Hits + RBI + Walks + Runs + HmRun + \n                PutOuts + AtBat + Errors,\n            data=Hitters)$coefficients,4)\n\n(Intercept)       Years        Hits         RBI       Walks        Runs \n     4.3797      0.0933      0.0128      0.0004      0.0066      0.0014 \n      HmRun     PutOuts       AtBat      Errors \n     0.0033      0.0003     -0.0025     -0.0029 \n\nround(\n    lm(log(Salary) ~ log(Years) + Hits + RBI + Walks + Runs + HmRun + \n                PutOuts + AtBat + Errors,\n            data=Hitters)$coefficients,4)\n\n(Intercept)  log(Years)        Hits         RBI       Walks        Runs \n     3.9394      0.6952      0.0108     -0.0002      0.0059      0.0024 \n      HmRun     PutOuts       AtBat      Errors \n     0.0057      0.0004     -0.0023      0.0000 \n\n\n\n\nRules\nA very helpful extension of rpart in rpart.plot is the rpart.rules function. It expresses the decisions captured in an rpart model as a set of text rules. The variables are listed in the order of frequency in the rules. A variable that appears in more rules is listed first. Adding the cover=TRUE option also displays the percentage of cases covered by each rule. For the initial tree constructed above, this yields:\n\nrpart.rules(t1, roundint=FALSE)\n\n log(Salary)                                                                  \n         4.7 when Years &lt;  3.5        & Hits &lt;  114                           \n         5.3 when Years &lt;  3.5        & Hits &gt;= 114                           \n         5.4 when Years is 3.5 to 4.5                             & Runs &lt;  75\n         5.7 when Years &gt;=        4.5 & Hits &lt;  118 & Walks &lt;  22             \n         5.7 when Years is 4.5 to 6.5 & Hits &lt;  118 & Walks &gt;= 22             \n         6.2 when Years is 3.5 to 4.5                             & Runs &gt;= 75\n         6.3 when Years &gt;=        6.5 & Hits &lt;  118 & Walks &gt;= 22             \n         6.6 when Years &gt;=        4.5 & Hits &gt;= 118 & Walks &lt;  61             \n         7.0 when Years &gt;=        4.5 & Hits &gt;= 118 & Walks &gt;= 61             \n\n\nThe information displayed in the rules can be affected through options, for example,\n\nrpart.rules(t1, cover=TRUE, style=\"tallw\", roundint=FALSE)\n\nlog(Salary) is 4.7 with cover 16% when\n                   Years &lt; 3.5\n                   Hits &lt; 114\n\nlog(Salary) is 5.3 with cover 7% when\n                   Years &lt; 3.5\n                   Hits &gt;= 114\n\nlog(Salary) is 5.4 with cover 8% when\n                   Years is 3.5 to 4.5\n                   Runs &lt; 75\n\nlog(Salary) is 5.7 with cover 10% when\n                   Years &gt;= 4.5\n                   Hits &lt; 118\n                   Walks &lt; 22\n\nlog(Salary) is 5.7 with cover 7% when\n                   Years is 4.5 to 6.5\n                   Hits &lt; 118\n                   Walks &gt;= 22\n\nlog(Salary) is 6.2 with cover 3% when\n                   Years is 3.5 to 4.5\n                   Runs &gt;= 75\n\nlog(Salary) is 6.3 with cover 17% when\n                   Years &gt;= 6.5\n                   Hits &lt; 118\n                   Walks &gt;= 22\n\nlog(Salary) is 6.6 with cover 19% when\n                   Years &gt;= 4.5\n                   Hits &gt;= 118\n                   Walks &lt; 61\n\nlog(Salary) is 7.0 with cover 13% when\n                   Years &gt;= 4.5\n                   Hits &gt;= 118\n                   Walks &gt;= 61\n\n\n\n\nCross-validation and Pruning\nAs mentioned earlier, rpart performs cross-validation for cost complexity values as part of the tree construction. This is the origin of the CP table that appears at the top of the summary.rpart output. You can print this information by itself with the printcp function.\n\ncp &lt;- printcp(t1)\n\n\nRegression tree:\nrpart(formula = log(Salary) ~ Years + Hits + RBI + Walks + Runs + \n    HmRun + PutOuts + AtBat + Errors, data = Hitters)\n\nVariables actually used in tree construction:\n[1] Hits  Runs  Walks Years\n\nRoot node error: 207.15/263 = 0.78766\n\nn=263 (59 observations deleted due to missingness)\n\n        CP nsplit rel error  xerror     xstd\n1 0.444574      0   1.00000 1.00246 0.065293\n2 0.114545      1   0.55543 0.59363 0.061780\n3 0.044460      2   0.44088 0.51901 0.064267\n4 0.018313      3   0.39642 0.46395 0.063740\n5 0.018067      4   0.37811 0.47347 0.067284\n6 0.016786      6   0.34197 0.47766 0.067189\n7 0.016177      7   0.32519 0.46799 0.067392\n8 0.010000      8   0.30901 0.45710 0.067312\n\n\nThe complexity table lists trees in order of the complexity criterion, smaller trees are on top (larger CP value), larger trees at the bottom. The number of (internal) nodes in the tree is nsplit plus one. The error columns are reported relative to the error at the first node. xerror is the error from cross-validation and xstd is the standard error. Scanning the xerror column we see that the error is minimized at 3 splits (4 nodes). The standard error is useful to determine the best number of split points by taking in the uncertainty in the error estimate and the fact that there is often a neighborhood of similar values. The 1-SE rule says to consider all values within one standard error of the achieved minimum. Those are essentially equivalent and we choose the simplest model. In Figure 4.6 this is a tree of size 4 (3 splits). The horizontal line is drawn 1 standard error above the smallest cross-validated error.\n\nplotcp(t1, upper=\"size\")\n\n\n\n\n\n\n\nFigure 4.6: Complexity parameter plot for Hitters data. The size of the tree refers to the number of nodes (number of splits + 1).\n\n\n\n\n\nTo produce the final tree, apply the prune function with the selected value of the complexity parameter (Figure 4.7).\n\nt_final &lt;- prune(t1,cp=cp[cp[,2]==3,1])\n\nrpart.plot(t_final,roundint=FALSE)\n\n\n\n\n\n\n\nFigure 4.7: Final tree for Hitters data after pruning and cross-validation.\n\n\n\n\n\n\n\nControlling the Tree\nMany parameters and setting affect the construction of the tree. The split criterion, cost function, minimum number of observations before attempting a split or in terminal nodes, the use of surrogate variables, the maximum depth, cross-validation settings, and so on.\nChanging tree control parameters affects primarily the depth of the tree and the computational requirements. For example, rpart computes by default up to 5 surrogate split variables. Unless you have missing values in input variables, surrogate splits will not be used. If you are sure that you also do not have missing values in non-training data, then you can skip the determination of the surrogates. Approximately half of the computing time during tree construction is used to find surrogate splits.\nIn rpart, parameters that determine the tree construction are passed as a rpart.control structure to the control= parameter. The most important parameters that determine the depth of the tree are\n\nminsplit: The minimum number of observations in a node for which a split is even attempted, default 20. Setting this to a small number will save computation time since nodes that have fewer than minsplit observations will not be split and are likely chopped off during pruning.\n\nminbucket: The minimum number of observations in terminal nodes (leaves). The default is minsplit/3.\ncp: Threshold for the complexity parameter. The complexity of the tree is measured during pruning as \\[\nC_\\alpha(T) = Q(T) + \\alpha|T|\n\\] where \\(Q(T)\\) is the cost criterion by which tree \\(T\\) is evaluated, \\(|T|\\) is the number of terminal nodes in \\(T\\), and \\(\\alpha\\) is the penalty parameter. The rpart version of this is \\[\nC_\\alpha(T) = Q(T) + cp|T|\\, Q(T_1)\n\\] where \\(Q(T_1)\\) is a tree with no splits. A split that does not improve the fit by at least the cp= threshold value will not be attempted. Setting cp=1 results in a tree with no splits. The default is cp=0.01. During cost-complexity pruning rpart evaluates trees for values that are larger than the threshold parameter. Setting cp=0 asks rpart to evaluate very deep trees, subject to the other parameters such as minsplit, minbucket, and maxdepth.\nmaxdepth: the maximum depth of the final tree, with the root node counting as depth zero. For example, maxdepth=1 results in a single split, also known as a “stump”. (Technically, a stump would have no splits.)\n\nImportant parameters that control the memory and computational requirements beyond computing trees at various depths are\n\nmaxsurrogate: the number of surrogate splits, default is 5. Set it to 0 to prevent the search for surrogate split variables.\nxval: the number of cross-validations, default is 10.\ncp: The complexity parameter threshold also affects the computing requirements as splits that do not improve the overall fit by at least that value will not be attempted.\n\n\nFitting stumps\nA special application of controlling the construction of decision trees are trees with a single split, also called “stumps”. In most scenarios such trees will underfit the data. However, in adaptive boosting, we fit a large number of trees where subsequent trees are weighing observations according to the level of misclassification, those trees focus on observations poorly classified so far. The individual trees are not deep trees, relying only on the most important variables. The use of stumps is common in adaptive boosting.\nThe following code fits a stump to the Hitters data. minsplit=2 specifies that a split will not be attempted unless at least two observations are present. This guarantees that at least one split occurs (setting minsplit=0 as recommended in some places does not seem to always work in guaranteeing a single split). Setting the threshold for the complexity parameter to -1 essentially turns off checking whether the split improves the overall fit. maxdepth=1 sets the maximum depth of the tree to two nodes; zero depth refers to the root node. Finally, xval=0 turns off the cross-validation.\n\nstump &lt;- rpart(log(Salary) ~ Years + Hits + RBI + Walks + Runs + HmRun + \n                PutOuts + AtBat + Errors, \n            data=Hitters,\n            control=rpart.control(maxdepth = 1,\n                                  cp       =-1,\n                                  minsplit = 2,\n                                  xval     = 0),\n            method=\"anova\")\n\nstump\n\nn=263 (59 observations deleted due to missingness)\n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 263 207.15370 5.927222  \n  2) Years&lt; 4.5 90  42.35317 5.106790 *\n  3) Years&gt;=4.5 173  72.70531 6.354036 *\n\nrpart.plot(stump,roundint=FALSE)\n\n\n\n\n\n\n\nFigure 4.8: A stump fit to the Hitters data.\n\n\n\n\n\nThe result of the stump fit are as expected based on the previous analysis. The number of years in the league is the most important variable and the basis of the first split (Figure 4.8).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>First Steps in `R`</span>"
    ]
  },
  {
    "objectID": "firstStepsR.html#classification-trees",
    "href": "firstStepsR.html#classification-trees",
    "title": "4  First Steps in R",
    "section": "4.3 Classification Trees",
    "text": "4.3 Classification Trees\nFitting a classification tree in rpart is simple, when the target variable is a factor the software defaults to building a classification tree. Choosing method=class makes that explicit.\n\nBinary Classification\nFor this example we use the stagec data frame that ships with the rpart package. It contains data on 146 patients with stage C prostate cancer from a study on the prognostic value of flow cytometry.\nInput variables include\n\nage: age of patient in years\neet: a binary variable indicating early endocrine therapy (1=no, 2=yes)\ng2: percent of cells in G2 phase as determined by flow cytometry\ngrade: the grade of the tumor according to the Farrow system\ngleason: the Gleason score of the tumor, higher values indicate a more aggressive cancer\nploidy: a three-level factor that indicates the tumor status according to flow cytometry as diplioid, tetraploid, or aneuploid.\n\nThe outcome (target) variable for this classification is pgstat, a binary variable that indicates progression of the cancer (pgstat=1). The variable is recoded as a factor for a nicer display on the tree. Similarly, the eet variable is recoded as a factor.\n\ndata(stagec)\nhead(stagec)\n\n  pgtime pgstat age eet    g2 grade gleason     ploidy\n1    6.1      0  64   2 10.26     2       4    diploid\n2    9.4      0  62   1    NA     3       8  aneuploid\n3    5.2      1  59   2  9.99     3       7    diploid\n4    3.2      1  62   2  3.57     2       4    diploid\n5    1.9      1  64   2 22.56     4       8 tetraploid\n6    4.8      0  69   1  6.14     3       7    diploid\n\nprogstat &lt;- factor(stagec$pgstat, levels = 0:1, labels = c(\"No\", \"Prog\"))\neetfac &lt;- factor(stagec$eet, levels=1:2, labels=c(\"No, Yes\"))\n\nset.seed(543)\ncfit &lt;- rpart(progstat ~ age + eetfac + g2 + grade + gleason + ploidy,\n              data = stagec, \n              method = 'class')\nprint(cfit)\n\nn= 146 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 146 54 No (0.6301370 0.3698630)  \n   2) grade&lt; 2.5 61  9 No (0.8524590 0.1475410) *\n   3) grade&gt;=2.5 85 40 Prog (0.4705882 0.5294118)  \n     6) g2&lt; 13.2 40 17 No (0.5750000 0.4250000)  \n      12) ploidy=diploid,tetraploid 31 11 No (0.6451613 0.3548387)  \n        24) g2&gt;=11.845 7  1 No (0.8571429 0.1428571) *\n        25) g2&lt; 11.845 24 10 No (0.5833333 0.4166667)  \n          50) g2&lt; 11.005 17  5 No (0.7058824 0.2941176) *\n          51) g2&gt;=11.005 7  2 Prog (0.2857143 0.7142857) *\n      13) ploidy=aneuploid 9  3 Prog (0.3333333 0.6666667) *\n     7) g2&gt;=13.2 45 17 Prog (0.3777778 0.6222222)  \n      14) g2&gt;=17.91 22  8 No (0.6363636 0.3636364)  \n        28) age&gt;=62.5 15  4 No (0.7333333 0.2666667) *\n        29) age&lt; 62.5 7  3 Prog (0.4285714 0.5714286) *\n      15) g2&lt; 17.91 23  3 Prog (0.1304348 0.8695652) *\n\nrpart.plot(cfit, roundint=FALSE)\n\n\n\n\n\n\n\nFigure 4.9: Classification tree for stagec data prior to pruning.\n\n\n\n\n\nFigure 4.9 shows the full tree fit to the data prior to pruning. The initial split is on the grade variable. Note that on the right hand side of the tree, at depth 2, the qualitative input variable ploidy is split into two groups: the left branch contains levels diploid and tetraploid, the right branch aneuploid. This confirms DNA ploidy as a major predictor variable in this study. From the rpart longintro vignette:\n\nFor diploid and tetraploid tumors, the flow cytometry method was also able to estimate the percent of tumor cells in a G2 (growth) stage of their cell cycle; G2% is systematically missing for most aneuploid tumors.\n\nThe boxes annotating the nodes contain three pieces of information:\n\nThe representative value (majority vote) in the node; this is the predicted category for the node. For example, there are 92 patients who did not progress and 54 patients who progressed. The majority vote in the root node would be no progression.\nThe proportion of events in the node. For example, in the first node that proportion is 54/146 = 0.37. There are 61 observations where grade &lt; 2.5, 9 of these progressed. That leads to the terminal node on the far left of the tree: its majority vote is no progression, 9/61 = 0.15 and 42% of the observations fall into this node.\nThe percentage of observations covered by the node.\n\nThis numeric breakdown can be seen easily from the printed tree:\n\ncfit\n\nn= 146 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 146 54 No (0.6301370 0.3698630)  \n   2) grade&lt; 2.5 61  9 No (0.8524590 0.1475410) *\n   3) grade&gt;=2.5 85 40 Prog (0.4705882 0.5294118)  \n     6) g2&lt; 13.2 40 17 No (0.5750000 0.4250000)  \n      12) ploidy=diploid,tetraploid 31 11 No (0.6451613 0.3548387)  \n        24) g2&gt;=11.845 7  1 No (0.8571429 0.1428571) *\n        25) g2&lt; 11.845 24 10 No (0.5833333 0.4166667)  \n          50) g2&lt; 11.005 17  5 No (0.7058824 0.2941176) *\n          51) g2&gt;=11.005 7  2 Prog (0.2857143 0.7142857) *\n      13) ploidy=aneuploid 9  3 Prog (0.3333333 0.6666667) *\n     7) g2&gt;=13.2 45 17 Prog (0.3777778 0.6222222)  \n      14) g2&gt;=17.91 22  8 No (0.6363636 0.3636364)  \n        28) age&gt;=62.5 15  4 No (0.7333333 0.2666667) *\n        29) age&lt; 62.5 7  3 Prog (0.4285714 0.5714286) *\n      15) g2&lt; 17.91 23  3 Prog (0.1304348 0.8695652) *\n\n\nTo prune the tree we print the CP table and plot the results including the 1-SE reference line.\n\nprintcp(cfit)\n\n\nClassification tree:\nrpart(formula = progstat ~ age + eetfac + g2 + grade + gleason + \n    ploidy, data = stagec, method = \"class\")\n\nVariables actually used in tree construction:\n[1] age    g2     grade  ploidy\n\nRoot node error: 54/146 = 0.36986\n\nn= 146 \n\n        CP nsplit rel error  xerror    xstd\n1 0.104938      0   1.00000 1.00000 0.10802\n2 0.055556      3   0.68519 0.98148 0.10760\n3 0.027778      4   0.62963 0.87037 0.10454\n4 0.018519      6   0.57407 0.87037 0.10454\n5 0.010000      7   0.55556 0.88889 0.10511\n\nplotcp(cfit)\n\n\n\n\n\n\n\n\nThe optimal tree is a tree with 5 nodes (4 splits). A tree with 6 splits has the same cross-validation error and standard error. We choose the simpler tree (Figure 4.10).\n\nrpart.plot(prune(cfit,cp=0.027778), roundint=FALSE)\n\n\n\n\n\n\n\nFigure 4.10: Classification tree for stagec data after pruning.\n\n\n\n\n\n\n\nR Resources\n\nR for Data Science, 2nd ed. (Wickham, Cetinkaya-Rundel, and Grolemund 2023).\nAdvanced R (Wickham 2019).\nModern Data Science with R, 2nd ed. (Baumer, Kaplan, and Horton 2021).\nR Graphics Cookbook: Practical Recipes for Visualizing Data, 2nd ed. (Chang 2018)\nR Markdown: The Definite Guide (Xie, Allaire, and Grolemund 2019)\nR Markdown Cookbook (Xie, Dervieux, and Riederer 2021)\nMastering Software Development in R (Peng, Kross, and Anderson 2020)\n\n\n\n\nFigure 4.1: RStudio IDE\nFigure 4.3: Default regression tree for log(salary) constructed by rpart for Hitters data.\nFigure 4.4: Default regression tree for salary constructed by rpart for Hitters data.\nFigure 4.5: Default regression tree for log(salary) with log-transformed year as input.\nFigure 4.6: Complexity parameter plot for Hitters data. The size of the tree refers to the number of nodes (number of splits + 1).\nFigure 4.7: Final tree for Hitters data after pruning and cross-validation.\nFigure 4.8: A stump fit to the Hitters data.\nFigure 4.9: Classification tree for stagec data prior to pruning.\nFigure 4.10: Classification tree for stagec data after pruning.\n\n\n\nBaumer, Benjamin S., Kaplan. Daniel T., and Nicholas J. Horton. 2021. Modern Data Science with r, 2nd Ed. Chapman & Hall/CRC Press. https://mdsr-book.github.io/mdsr3e/.\n\n\nChang, Winston. 2018. R Graphics Cookbook: Practical Recipes for Visualizing Data, 2nd Ed. O’Reilly Media. https://r-graphics.org/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r, 2nd Ed. Springer. https://www.statlearning.com/.\n\n\nPeng, Roger D., Sean Kross, and Brooke Anderson. 2020. Mastering Software Development in r. https://bookdown.org/rdpeng/RProgDA/.\n\n\nWickham, H. 2019. Advanced r, 2nd Ed. Chapman & Hall/CRC Press. http://adv-r.had.co.nz/.\n\n\nWickham, H., M. Cetinkaya-Rundel, and G. Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data, 2nd Ed. O’Reilly Media. https://r4ds.hadley.nz/.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2019. R Markdown: The Definite Guide. Chapman & Hall/CRC Press. https://bookdown.org/yihui/rmarkdown/.\n\n\nXie, Yihui, Dervieux Christophe, and Emily Riederer. 2021. R Markdown Cookbook. Chapman & Hall/CRC Press. https://bookdown.org/yihui/rmarkdown-cookbook/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>First Steps in `R`</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "5  Correlation and Causation",
    "section": "",
    "text": "5.1 Introduction\nOne of the key tasks in analyzing data is uncovering relationships between things. Describing the statistical behavior of a single variable is interesting. Studying how two or more variables behave together is really interesting. That is how we uncover mechanisms, relationships, associations, and patterns. And if things go really well, we might discover cause-and-effect relationships.\nYou might have heard the saying\nWhat do we mean by that?\nCausation implies that one thing is the result of another thing; they stand in a cause-and-effect relationship to each other. The gravitational pull of the moon on earth’s oceans causes the tides. An accident causes a traffic jam. Smoking causes an increase in the risk of developing lung cancer.\nCorrelation, on the other hand, is about establishing association between attributes. The weight of a person is correlated with their height. Taller people tend to be heavier but height alone is not the only factor affecting someone’s weight. Smoking is correlated with alcoholism but does not cause it.\nFigure 5.1 displays the relationship between highway accidents in the U.S. and lemon imports from Mexico for a period of five years, from 1996–2001. The U.S. Dept. of Agriculture tracks agricultural imports and exports, the U.S. National Highway Traffic Safety Administration (NHTSA) tracks highway fatalities. Neither federal agency probably thought much about the data collected by the other agency. But when put together, voilà. A clear trend emerges!\nIf the relationship in Figure 5.1 is causal, public policy to reduce highway fatalities is very clear: reduce fresh lemon imports from Mexico! Clearly, this is not a causal relationship. There must be another explanation why the variables in Figure 5.1 appear related.\nCompare this situation to John Snow’s investigation of the relationship between water quality and cholera incidences in 19th century London (Chapter 2). Snow found higher cholera incidences in houses closer to the Broad Street public water pump. There was a strong association between cholera cases and proximity to the pump. But did the pump—or more precisely the water from the pump or something in the water—cause cholera? Today we know that cholera is caused by the bacterium Vibrio cholerae, but that discovery was not made until 1883.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and Causation</span>"
    ]
  },
  {
    "objectID": "correlation.html#introduction",
    "href": "correlation.html#introduction",
    "title": "5  Correlation and Causation",
    "section": "",
    "text": "correlation does not imply causation\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.1: Relationship between highway fatalities and lemon imports from Mexico.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and Causation</span>"
    ]
  },
  {
    "objectID": "correlation.html#correlation-and-the-correlation-coefficient",
    "href": "correlation.html#correlation-and-the-correlation-coefficient",
    "title": "5  Correlation and Causation",
    "section": "5.2 Correlation and the Correlation Coefficient",
    "text": "5.2 Correlation and the Correlation Coefficient\nWe experience correlation when one attribute changes with another. When the attributes are continuous (see Section 1.2.4), we can display their association with a scatterplot. A positive correlation then implies that the point cloud has a positive slope, as one attribute increases the other one tends to increase as well (Figure 5.2). When the correlation is negative, an increase in one attribute is associated with a decrease in the other attribute (Figure 5.3).\n\n\n\n\n\n\nFigure 5.2: Positive correlation.\n\n\n\n\n\n\n\n\n\nFigure 5.3: Negative correlation.\n\n\n\nWhile the direction of the point cloud indicates whether the correlation (association) is positive or negative, the tightness of the point cloud indicates the strength of the association (Figure 5.4).\n\n\n\n\n\n\nFigure 5.4: Correlations of different strength and directions. The numbers above the point clouds indicate the strength and direction of the correlation\n\n\n\nWhen we are dealing with discrete attributes, the association cannot be revealed through a point cloud. Instead, we cross-tabulate the frequency of occurrence of the attributes.\n\n\nExample: Rater Agreement\n\n\nTable 5.1 shows the results of a study where insect damage on 236 agricultural fields was classified into 5 damage categories by two different inspectors.\n\n\n\nTable 5.1: Results of rating 236 experimental units by 2 raters\n\n\n\n\n\nRater 2\n1\n2\n3\n4\n5\nTotal\n\n\n\n\n1\n10\n6\n4\n2\n2\n24\n\n\n2\n12\n20\n16\n7\n2\n57\n\n\n3\n1\n12\n30\n20\n6\n69\n\n\n4\n4\n5\n10\n25\n12\n56\n\n\n5\n1\n3\n3\n8\n15\n30\n\n\nTotal\n28\n46\n63\n62\n37\n236\n\n\n\n\n\n\nFor example, 16 experimental units were assigned to damage category 3 by rater 1 and to damage category 2 by rater 2. There is relatively strong association between the ratings, the majority of the counts fall on the diagonal of the table and in the cells immediately off the diagonal (where the raters disagree by one damage category).\n\n\nAnother example of cross-tabulating counts to demonstrate association is Table 2.2 in John Snow’s cholera study. It shows that cholera deaths are 10 times more likely to occur in homes supplied by the Southward & Vauxhall water company than in homes supplied by the Lambeth water company.\n\nThe strength of the correlation between continuous attributes is measured by the correlation coefficient, which ranges from -1 to 1. Both of these extremes are called perfect correlations and happen when all points fall on a perfect line, without variability about the line. The relationship is deterministic. Linear mathematical relationships exhibit such patterns, for example, the relationship between degree Celsius and degree Fahrenheit (Figure 5.5): \\[\n^\\circ F = {^\\circ C} \\times \\frac{9}{5}  + 32\n\\]\n\n\n\n\n\n\n\n\nFigure 5.5: Linear relationship between \\(^\\circ F\\) and \\(^\\circ C\\).\n\n\n\n\n\nThis raises another caution about relying on correlation metrics: the relationship between attributes can be quite strong, but their degree of linear relationship can be low. Figure 5.6 shows two variables with a strong nonlinear relationship. \\(Y\\) decreases with increasing \\(C\\) for values \\(X &lt; 0\\) and \\(Y\\) increases with \\(X\\) for values \\(X &gt; 0\\). When the standard linear correlation coefficient is calculated, it turns out to indicate a very weak relationship between the variables—a very weak linear relationship. The takeaway is not to only focus on reported measures of association but to also examine the relationships visually.\n\n\n\n\n\n\n\n\nFigure 5.6: Strong nonlinear relationship with small (linear) correlation coefficient.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and Causation</span>"
    ]
  },
  {
    "objectID": "correlation.html#spurious-correlations",
    "href": "correlation.html#spurious-correlations",
    "title": "5  Correlation and Causation",
    "section": "5.3 Spurious Correlations",
    "text": "5.3 Spurious Correlations\nIt seems obvious that just because two attributes vary with each other—are correlated—one should not infer that they are cause and effect of each other. Unfortunately, that leap of faith is often made and can lead to very problematic decisions.\n\n\n\n\n\n\nFigure 5.7: Chocolate consumption and number of Nobel laureates.\n\n\n\nFigure 5.7 displays the number of Nobel laureates per 10 million population against the chocolate consumption (in kg per capita and year) for various countries (Messerli 2012). An upward trend is clearly noticeable. A greater per-capita chocolate consumption is associated with a lager number of Nobel laureates. Whoa! If the two variables stand in a cause-effect relationship then we have a simple recipe to increase Nobel prizes: we all should eat more chocolate. While one can argue the benefits of chocolate for cognitive function, what we have here is a simple correlation. The two attributes, number of Nobel laureates per 10 million population and chocolate consumption per capita, are related. If one increases so does the other. But why?\nThis is an example of a spurious correlation. The variables are not really dependent on each other, a relationship is induced in some other way. In this particular example, the correlation was “found” by cherry-picking the data: only four years of chocolate consumption were considered on a limited number of chocolate products and no data prior to 2002 was used. The number of Nobel laureates is a cumulative measure that spans a much longer time frame.\nIt appears that the data were organized in such a way as to suggest a relationship between the variables.\nYou can find spurious correlations everywhere, without manipulating the data. Here are some examples and the reasons why the data appear correlated.\n\nCoincidence\nForecasting economic conditions is difficult and highly valuable. Since the end of World War II there have been only eleven economic recessions. On the other hand we are producing thousands of economic indicators. The government alone generates 45,000 economic statistics each year (Silver 2012, 185).\nSo it should not be surprising that when sifting through all those variables we find some that appear to go up or down together, just by coincidence.\nA famous example is the Super Bowl winning conference as an indicator of economic performance. Between 1967 and 1997, in years when the team from the original NFL won, the stock market went up by 14%. When a team from the original AFL won the stock market decreased by almost 10%. Through 1997, the Super Bowl winner “predicted” correctly the direction of the stock market in 28 out of 31 years (Silver 2012). Since 1998 the trend has reversed and the stock market is doing better when an AFL team won the Super Bowl. Coincidence during a period of time leads to mistake noise for signal.\nAnother example of coincidence being mistaken for correlation is Paul, the octopus who correctly predicted the winner in 2008–2010 international soccer matches 12 out of 14 times, an 85% accuracy. Since this success rate is unlikely to happen by chance, it was determined that Paul the octopus has divine powers. When Paul got it wrong in a 2010 FIFA World Cup game between Germany and Spain, the German fans called for Paul to be eaten and the Spanish Prime Minister offered Paul asylum.\n\n\nLatent Variables\nA correlation between variables A and C can be induced by another variable, say B. If A is correlated with (or caused by) B and C is correlated (or caused by) B, then plotting A versus C indicates a correlation between the two variables. However, the relationship is induced by the latent variable B.\n\n\n\n\n\n\nFigure 5.8: Spurious correlation.\n\n\n\nLatent variables are often the real reason why things appear related when we deal with variables that depend on population size or common factors such as the weather or time. Figure 5.8 shows the close relationship over time between the number of high school graduates and donut consumption. More donut consumption appears related to more high school graduates. Notice that we are not plotting graduation rates, these would most likely not have any relationships with donut consumption. The latent variable at work in Figure 5.8 is the size of the population over time. As the population increases, more donuts are consumed and more people graduate from high school.\nA similar spurious correlation is that between ice cream sales and forest fires. Both increase during the summer heat and decrease in the winter.\nYou can imagine the horrible public policy decisions one would make by mistaken those spurious correlations for cause and effect relationships.\n\n\nInduced Correlation\nAnother interesting mechanism to induce correlation is by introducing a mathematical dependence between two attributes. A famous example is the relationship between birth rate and the density of storks.\n\n\n\n\n\n\nFigure 5.9: Storks and Babies\n\n\n\nIn central Europe a persistent myth is that storks bring babies. Movies were made about it! The origin of the association probably goes back to medieval days when conception was more common in mid-summer during the celebration of the summer solstice which is also a pagan holiday of marriage and fertility. The white stork is a migratory bird that flies to Africa in the fall and returns to Europe nine months later. Hence the connection was made that storks brought the babies.\nAlthough the myth has been debunked, there have been several studies of the connection between fertility and the stork abundance. Neyman (1952) describes a study of 54 counties that comprises the following attributes\n\n\\(W\\): Number of women of child-bearing age in the county (in 10,000)\n\\(S\\): Number of storks in the county\n\\(B\\): Number of babies born in the county\n\nSince it is likely that these numbers increase with the size of the county, the variables analyzed were \\(Y = B/W\\) and \\(X=S/W\\), the birth rate per 10,000 women and the density of storks per 10,000 women. A plot of these variables and a smooth estimate of their trend is shown in Figure 5.10.\n\n\n\n\n\n\nFigure 5.10: Storks and babies.\n\n\n\nIt certainly appears that the birth rate increases with the density of storks. Could the myth be true? Is something else going on?\nThe trend in Figure 5.10 is induced by expressing both variables as ratios with the same variable, \\(W\\), the number of women of child-bearing age. If \\(S\\) and \\(B\\) are unrelated, \\(S/W\\) and \\(B/W\\) now share information because they are expressed relative to another variable.\n\n\nAssignment: Mozzarella and Engineering Degrees\n\n\nSpiegelhalter (2021) cites the strong correlation (coefficient 0.96) between the annual per-capita consumption of mozzarella cheese in the U.S. in the period 2000–2009 (\\(X\\)) and the number of civil engineering doctorates awarded in those years (\\(Y\\)).\n\nHow would you interpret this relationship if \\(X\\) causes \\(Y\\)?\nIf this relationship is spurious, what could explain it?\nCorrelation is a symmetrical relationship, mathematically, \\(\\text{Cor}(X,Y) = \\text{Cor}(Y,X)\\). Causation on the other hand is asymmetrical. If \\(X\\) causes \\(Y\\) does not imply that \\(Y\\) causes \\(X\\). \\(Y\\) causing \\(X\\) is called reverse causation. What does reverse causation mean in the Mozzarella–Engineering Ph.D. example?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and Causation</span>"
    ]
  },
  {
    "objectID": "correlation.html#establishing-causality",
    "href": "correlation.html#establishing-causality",
    "title": "5  Correlation and Causation",
    "section": "5.4 Establishing Causality",
    "text": "5.4 Establishing Causality\nEstablishing a causal link between factors is the holy grail of scientific study. When we prove that one event is the result of another event, we have established new, irrefutable knowledge, beyond a reasonable doubt.\nEstablishing cause and effect is also quite difficult. Spiegelhalter (2021, 97) calls causation a “deeply contested subject”. You get a splinter in your finger and it hurts. Did the splinter cause the pain? Is it possible that the finger would have abruptly started to hurt if it wasn’t for the splinter? If it was me, it would be pretty obvious to me that the splinter caused the pain.\nBut wait. Not everyone has the same level of pain tolerance. The splinter that causes me much agony might be a mere scratch, hardly noticeable, for someone else. How do we take this variability in the population into account in making statements about causality?\nWhen we say that smoking causes lung cancer, we do not claim that every smoker will get the disease. Some smokers do not get lung cancer and there are lung cancer patients who never smoked. The second leading cause of lung cancer deaths in the U.S. is exposure to Radon gas. That is why in many regions Radon inspections are required prior to purchasing homes. A more precise statement would be that smoking causes an increase in the likelihood of getting lung cancer. It is not a statement about what happens to Joe or Diana. The statistical notion of causation between \\(X\\) and \\(Y\\) means that if \\(X\\) occurs, \\(Y\\) tends to occur more or less often. The statistical notion of causation is not deterministic (Spiegelhalter 2021, 99).\nIn a study about the association between repeated head impacts (RHI) and chronic traumatic encephalopathy (CTE), Nowinski et al. (2022) state that causation is an interpretation, not an entity. In studies involving complex environmental exposures causation is a continuum from highly unlikely to highly likely, and no single study can prove causation.\nIn the presence of uncertainty some scientific standard needs to be met in order for us to claim that something has been proven and even then, we are not making statements that the something will happen every time, only that the proportion of times that it will happen has been affected.\n\nConfounding\nLet’s return to the 1854 cholera outbreak and the question before John Snow: did something in the water of the Broad Street public water pump cause cholera? If so, this would explain the higher incidence rate of cholera in residences near the pump and it would also explain the other anomalies he found in the data (see Chapter 2).\nThe map Snow drew in 1854 (Figure 2.2) might be convincing to us, his contemporaries did not feel that way. For one, he could not prove that the Broad Street well water caused the cholera cases. And his hypothesis was inconsistent with the prevailing theory of the time, that cholera was caused by airborne particles (miasma) from dirty or decaying biological material.\nThe analysis of the cholera map established a correlation rather than causation because of the possibility of confounding factors: variables that can mask or distort the effect of other variables. In the 1960s it was shown that coffee drinkers had higher rates of lung cancer than non-coffee drinkers. Some thought this was implication of coffee as a cause of lung cancer. That is incorrect. The association is due to a confounding factor: coffee drinkers at the time were more likely to be also smokers. Coffee drinking was associated with lung cancer but does not cause the disease.\nA confounding factor is related to both the cause and the effect and can mislead us into attributing too much or too little importance to the potential cause. Variables such as age, time, temperature, population size are often confounding factors because they act on the factor of interest and on the outcome of interest. For example, age is a confounding factor in studies of exposure to harmful agents. If damage from the agent is more prevalent in older people, age can be a confounding factor because older people have been exposed longer.\nWhen spurious correlations are induced by latent variables, the latent variable is a confounder. The apparent correlation between ice cream sales and shark attacks is explained by the confounder temperature. Ice cream sales and shark attacks increase with temperature as more people buy ice cream and more people go to the beach.\n\n\nAssignment: 1854 Cholera Outbreak\n\n\nIn the case of the 1854 cholera outbreak, there could have been confounding factors that caused cholera incidences in the Broad Street area to be higher, whether the water was or was not the cause of the disease. Maybe the residents of that poorer neighborhood had a different diet that caused the disease. Maybe they had occupations that made it more likely to be exposed to a harmful agent. Maybe. Maybe. Maybe.\nWhat other confounding factors can you think of that would mask, amplify, or suppress the incidence of cholera?\n\n\nIn order to establish a causal link between two variables, the confounding factors must be accounted for—at least beyond a reasonable doubt. Otherwise there will always be some reason to believe another mechanism was at work. There are a few principal mechanisms to deal with confounding variables:\n\nAdjustment\nStratification\nRandomization\n\nWe will discuss these in turn, but note that they are not mutually exclusive. A study might involve experimentation with randomly assigned treatments as well as model adjustments for confounding variables.\n\n\nAdjustment\nAdjusting for confounding variables means to include the variables in models that describe the relationship between the factor of interest and the outcome of interest. Consider the Radon exposure example above. A model to describe the relationship between exposure \\(x\\) (in picocuries per liter air) and cancer risk could be written in two parts: \\[\n\\begin{align}\n\\eta &= \\beta_0 + \\beta_1 x + \\beta_2 \\text{ age} \\\\\n\\Pr(\\text{Develops cancer}) &= \\frac{1}{1+\\exp\\{-\\eta\\}} \\\\\n\\end{align}\n\\tag{5.1}\\]\nThe first term of the model, \\(\\eta\\), is called the linear predictor and is a function of the variables that materially determine the cancer risk. In addition to the exposure \\(x\\), the linear predictor also contains a term for a person’s age. The second expression in Equation 5.1 transforms the linear predictor into a probability—it is called the logistic transformation.\nThis model allows us to estimate how much of the cancer risk is due to the level of radon exposure and how much is due to the age of the person. With the variable of interest (\\(x\\)) and the confounding variable (age) disentangled we can make statements about the risk of cancer as a function of radon exposure and age.\nAn important aspect of adjusting for confounding variables is the functional relationship between the variables. In Equation 5.1 the two variables enter the linear predictor in an additive fashion. Maybe this is not the appropriate adjustment. If the effect of age on cancer risk changes with the level of radon exposure then we say that the two variables interact. A model that includes a multiplicative interaction tterm then might be more appropriate: \\[\n\\eta = \\beta_0 + \\beta_1 x + \\beta_2 \\text{ age} + \\beta_3 x\\,\\text{age}\\\\\n\\]\nIn other words, determining how to model the relationship of confounding variables on other variables and on the outcome of interest, is of great importance.\n\n\nStratification\nStratification is an approach to deal with confounding factors that are qualitative. It means to examine relationships separately for each level of the confounder. To see if there is a general relationship between ice cream sales and shark attacks we can examine the association for different temperature ranges. As a surrogate for that we can look at the association by month or by season.\nWhen performing analyses overall and comparing them to analyses within groups (within strata) we can run into situations where the two seem to provide contradictory results. This is known as Simpson’s paradox.\n\nSimpson’s Paradox\nThe paradox is named after Edward Simpson who described it in a technical paper in 1951, but the phenomenon has been known much longer. It is also not a paradox as it does not lead to nonsensical outcomes or a contradiction. What we know as Simpson’s Paradox is simply the result of looking at an aspect from two viewpoints: The trend that we see in combined data can reverse when we look at the data in groups.\nFigure 5.11 displays a scatter plot of two variables and a linear regression trend. The slope is positive, the average value of \\(Y\\) increases with the value of \\(X\\).\nThe data in Figure 5.11 consists of three groups and when the regression analysis is performed separately for each group, a different picture emerges. Within each group the regression relationship indicates a negative slope, the opposite of the trend in the ungrouped data (Figure 5.12).\n\n\n\n\n\n\n\n\nFigure 5.11: Combined data and regression trend.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.12: Grouped data and group-specific regression trends.\n\n\n\n\n\nThe apparent contradiction comes about because the three groups have different centers and orientation. When “stacked” they create a single point cloud with a positive slope. As mentioned previously, this is not really a paradox, both ways of looking at the data are sensible and the results are meaningful either way. The “paradox” lies in the fact that the two views lead to seemingly different conclusions about the relationship between the variables.\nExamples of Simpson’s Paradox with qualitative data can be found in college admissions data. Wikipedia shows the apparent gender bias effect for 1973 data from UC Berkeley. Spiegelhalter (2021) shows data for Cambridge from 1996. The table in Spiegelhalter (2021, 111) is reproduced below as two separate tables.\n\n\n\nTable 5.2: Application and acceptance rates at Cambridge in 1996 in STEM disciplines for men and women.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWomen\n\n\nMen\n\n\n\n\n\n\n\nApplied\nAccepted\n%\nApplied\nAccepted\n%\n\n\nTotal\n1,184\n274\n23%\n2,740\n584\n24%\n\n\n\n\n\n\nTable 5.2 shows the overall acceptance rates for men and women, with the former slightly higher by one percent.\n\n\n\nTable 5.3: Application and acceptance rates at Cambridge in 1996 by STEM discipline.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWomen\n\n\nMen\n\n\n\n\n\n\n\nApplied\nAccepted\n%\nApplied\nAccepted\n%\n\n\nComputer Science\n26\n7\n27%\n228\n58\n25%\n\n\nEconomics\n240\n63\n26%\n512\n112\n22%\n\n\nEngineering\n164\n52\n32%\n972\n252\n26%\n\n\nMedicine\n416\n99\n24%\n578\n140\n24%\n\n\nVeterinary Medicine\n338\n53\n16%\n180\n22\n12%\n\n\n\n\n\n\nTable 5.3 shows the applications and acceptance rates by discipline. In each discipline the acceptance rate for women is at least as high as that for men, in fact it is higher than that for men, except for Medicine.\nHow do we explain this apparent contradiction? In each discipline the acceptance rate for women is as high or higher than that for men, but overall the acceptance rat for women is lower. The explanation is that women were more likely to apply for subjects that have high application numbers and are more difficult to get in. 64% of the applications from women went to Medicine and Veterinary Medicine (416 + 338 out of 1,184). Only 30% of the male applications went to those subjects. Men applied disproportionately more to Engineering, which has a higher acceptance rate than the other STEM disciplines. It seems that there is no gender bias in these admission numbers, the lower overall admission rate for women is the result in women applying in larger numbers to STEM disciplines that are difficult to get into.\n\n\n\nRandomization\nRandomization is a simple and effective mechanism to create probabilistic equivalence in data and takes two forms: random selection and random assignment. Random does not imply haphazard or arbitrary, the way in which selection or assignment is randomized obeys probability distributions. Because we specify the distribution according to which selection or assignment occurs, we understand the probabilistic behavior of the data we collect.\nFor example, we select a random sample of people from a population such that each person has the same chance of entering the sample. This guarantees that the attributes of the population are represented properly in the sample. The proportion of persons of different age, gender, race, etc. in the sample will be representative of the population from which the sample is drawn, although the exact proportions will not be identical in a particular sample. If we were to draw a sample of \\(n\\) people by a non-random mechanism, for example, by taking the first \\(n\\) folks who drive through an intersection, or \\(n\\) airline passengers on a given day, or \\(n\\) residents of a particular neighborhood, or the first \\(n\\) entries in the list of the U.S. Census Bureau, our sample would not be representative of the population we are interested in studying. The sample would suffer from selection bias, conclusions drawn from analyzing the data in the sample would not apply to the population as a whole. Note that random sampling from a non-representative list also suffers from selection bias. Randomly sampling social media posts does not give insight into the opinions of the general population because social media users are not representative of the general population. Asking questions of a random sample of drivers passing through an intersection does not properly represent those who do not drive or live elsewhere.\nThe random mechanism in random selection has a balancing property. It ensures that sub-groups are not over represented or under represented in the sample, provided that the sample is sufficiently large. This balancing property is also key for the second form of randomization: random assignment when conditions are manipulated on purpose. This leads us to experimentation.\n\n\nExperimentation–The Randomized Controlled Trial (RCT)\n\nExample: RCBD in Agriculture\nSuppose you wish to study how poppies grow on an agricultural field. The poppies are subject to varying growing conditions due to differences in the soil characteristics, topography, weather, plant-to-plant variations, and so on. We are particularly interested to see how six different fertilizer applications affect the poppy growth. The conditions that affect poppy growth can be grouped into three categories:\n\nConditions we manipulate\nConditions we do not manipulate but know about\nConditions we do not know about (lurking factors)\n\nThe fertilizer treatments fall into the first category. We can apply parts of the field and apply fertilizer A to it, other parts receive fertilizer B, and so on. Suppose that the field is large and sloped and we suspect a gradient in soil nutrients and water. This falls into the second category. We do not create or manipulate the soil and water conditions, but we are aware of them. They are a confounding factor. All other potential influences of poppy growth fall into the third category.\nFigure 5.13 displays a popular layout for running such a fertilizer experiment in agricultural science. It is called a randomized complete block design (RCBD). The experimental area is divided up into separate blocks, in this case four of them.\n\n\n\n\n\n\nFigure 5.13: Experimental layout for poppy experiment.\n\n\n\nThe blocks are chosen so that the fields within a block—we call them experimental units—are homogeneous with respect to the conditions we know about. This is the technique to control confounding factors in category 2 by stratification and adjustment. Within each block the fertilizer treatments are assigned randomly to the six experimental units. This random assignment balances out the effects of the confounding factors we do not know about (category 3). If we were to assign treatments to experimental unit in a deterministic way—for example, treatment A always to the upper left unit, treatment B right next to it, etc.—it is possible that confounding factors associated with the position in the block mask or distort the effect of the treatment. We would then not really compare treatment A to treatment B, but a blend of treatment effect and location effect.\nFinally, because the world is uncertain, we do not apply the treatments only once. We use multiple blocks, each with a separate assignment of treatments to experimental units to replicate the per-block layout. This replication allows us to measure the inherent variability in poppy growth, unaffected by treatment or confounding factors.\nIf the experimental units in our agricultural science experiment contain more poppies than we can harvest and analyze in the lab, we would select some of them for lab analysis. This would be done by random selection to make sure that the plants analyzed in the lab are representative of the plants growing on the experimental unit.\nIn this RCBD with four blocks and six treatments we encounter all techniques to manage confounding factors: stratification, adjustment (because block effects will be included in the model during analysis), and randomization. As a result, we are allowed to make cause-and-effect statements about the treatment factor we manipulated, the fertilizer. If plants grown under the fertilizer A treatment are taller than those grown under fertilizer B, then there are only two possible explanations:\n\nFertilizer A causes poppies to grow taller compared to fertilizer B\nCoincidence: the height difference we are seeing is due to chance\n\nOther explanations can be ruled out because we controlled the experiment for other factors.\nIn order to separate the two remaining explanations, we analyze the size of the treatment differences relative to the inherent variability in poppy growth. That is the reason why the experiment uses multiple blocks rather than a single block. The replication of treatment assignments allows us to estimate that inherent variation. If poppy growth varies widely, then a small difference in height between plants from treatment A and treatment B will not surprise us. If inherent growth variation is small, observed differences between treatments point at the fertilizer as the cause.\n\n\nOther Experiments\nExperimentation with random assignment of conditions has a long history. It started in agricultural experiments and has since permeated many domains. Randomized clinical trials are common to test medical drugs, devices, and treatments. Industrial experimentation is used to find the best manufacturing conditions for products.\nRandomized trials are also used in the social sciences. Spiegelhalter (2021, 107) cites the Study of the Therapeutic Effects of Intercessory Prayer (STEP) by Benson H. et al. (2006) to answer the question whether being prayed for improves the recovery of patients after coronary artery bypass graft (CABG) surgery. The Methods section of the paper describes the randomized experiment:\n\nPatients at 6 US hospitals were randomly assigned to 1 of 3 groups: 604 received intercessory prayer after being informed that they may or may not receive prayer; 597 did not receive intercessory prayer also after being informed that they may or may not receive prayer; and 601 received intercessory prayer after being informed they would receive prayer. Intercessory prayer was provided for 14 days, starting the night before CABG. The primary outcome was presence of any complication within 30 days of CABG. Secondary outcomes were any major event and mortality.\n\nThe study concludes\n\nIntercessory prayer itself had no effect on complication-free recovery from CABG, but certainty of receiving intercessory prayer was associated with a higher incidence of complications.\n\nKnowing that they were being prayed for might have made patients more uncertain, wondering whether they are so sick that they had to call in the prayer team.\n\nExperimentation with random assignment is also used frequently in the technology industry, the technique is known as A/B testing. Only two treatments are being evaluated (A and B), one is typically a current product or design. Users of the product/design are randomly directed to the A or B option and the attribute of interest is measured (click-through rate, time on page, use of features on page, checkout, purchase amount, etc.). We are all participating in these ongoing A/B experiments when we operate online. Google is said to run about 10,000 A/B experiments every year.\n\n\n\nWhen Experimentation is Not Possible\nExperimentation with random assignment of treatments is the gold standard to establish cause and effect. But it is not always possible to go down that path.\nSome systems defy manipulation with treatments. We can only observe the weather we cannot change it. The process of manipulation can alter how a system behaves in ways that are not related to the treatment application, so we cannot really study just the treatment effects. Epidemiological studies, like John Snow’s investigation of the 1854 Cholera epidemic are by definition observational studies: we observe what is happening, not conditions we create deliberately.\nWhile one can assign conditions, ethical considerations might prevent us from doing so. How can we justify assigning harmful things and ask a person to smoke two packs of cigarettes a day for the next 10 years? In the case of testing a medical breakthrough against a horrible disease, how can we justify assigning patients to a placebo group and withholding a potentially life-saving treatment? To show that repeated head impact (RHI) causes chronic traumatic encephalopathy (CTE), it would not be ethical to randomize subjects and hit those assigned to the RHI arm of the study repeatedly over the head.\nWhen experimentation is not possible, we rely on observational data, analyzing the data we can collect, and it is often the best we can do. How can we then explain the association we find in the data and get closer to establishing causality?\nTo link RHI to CTE, we can study data on subjects who have been exposed to repeated head trauma, such as boxers and football players, and compare their likelihood of developing CTE to individuals who did not experience such head trauma. Comparisons must be made with care. We would not want to compare star athletes who experience head trauma with non-athletes who did not experience head trauma; there would be too many confounding factors. Maybe we could follow a group of athletes over time and record accumulated head impacts along with brain scans. While we cannot design an experiment, we can design how to collect observational data.\n\n\nAssignment: Why Do Old Men Have Big Ears?\n\n\nSpiegelhalter (2021, 108–9) asks this question based on his personal experience that older men seem to have big ears. This question cannot be answered with a randomized controlled trial, we cannot assign ear lengths. It is what it is.\nObservational studies in the UK and Japan collected cross-sectional data, that is, a sample from the current population, which will include men of different ages. Analyzing the data the studies concluded that there is a positive correlation between age and ear length. For example, the figure below appeared in Heathcote (1995). The study concluded that the regression trend was significant. The slope of the regression line is 0.22mm per year with a 95% confidence interval of [0.17, 0.27] mm per year. Because the confidence interval does not cover the value 0, the trend is statistically significant. It seems that as we get older our ears get bigger by an average by 0.22 mm per year.\n\n\n\n\n\n\nFigure 5.14: Ear length. From Heathcote (1995).\n\n\n\n\nTry and explain the association between age and ear length in men. What are possible reasons ears are/appear larger in older men?\nIf you were to conduct a follow-up study to test the possible reasons in 1., what would the study look like? What kind of data would you collect? What kind of men would you recruit for the study?\n\n\n\n\nQuasi-experiments\nIf you cannot manipulate and control factors in an experiment, maybe you are lucky to find data where the confounding factors have already been controlled for you. Sometimes real life runs these quasi-experiments for us and eliminates the confounding factors. Although the data is observational rather than experimental, it can go a long way toward establishing causality. In Section 2.4 we discussed a second, deeper analysis John Snow conducted in which he compared cases between customers of the Lambeth and the Southward & Vauxhall water companies. For all intents and purposes the groups serviced by the two companies were identical except for the source of the water. Lambeth’s water was drawn upriver from sewage discharge into the River Thames and was cleaner than the water from Southwark & Vauxhall, which drew water below the sewage discharge. The much higher cholera incidence in the group supplied by Southwark & Vauxhall was sufficient evidence to implicate the water.\n\n\nAssignment: Fluoride Exposure and IQ\n\n\nThe National Toxicology Program of the U.S. Department of Health and Human Services conducted a meta-analysis of the relationship between fluoride intake and IQ.\nAmong the findings of the analysis, the article states (emphasis in original):\n\nThe NTP monograph concluded, with moderate confidence, that higher levels of fluoride exposure, such as drinking water containing more than 1.5 milligrams of fluoride per liter, are associated with lower IQ in children. The NTP review was designed to evaluate total fluoride exposure from all sources and was not designed to evaluate the health effects of fluoridated drinking water alone. It is important to note that there were insufficient data to determine if the low fluoride level of 0.7 mg/L currently recommended for U.S. community water supplies has a negative effect on children’s IQ. The NTP found no evidence that fluoride exposure had adverse effects on adult cognition.\n\n…\n\nThe determination about lower IQs in children was based primarily on epidemiology studies in non-U.S. countries such as Canada, China, India, Iran, Pakistan, and Mexico where some pregnant women, infants, and children received total fluoride exposure amounts higher than 1.5 mg fluoride/L of drinking water. \n\nPlease read the full article from the National Toxicology Program here and answer the following questions:\n\nDid the study establish correlation or causation between fluoride intake and children’s IQ?\nDoes the article make it clear whether to take the results as an indication of causation or correlation?\nWhat is meta-analysis?\nHow do you interpret the fact that the study did not analyze data from the U.S.? Does that affect whether the results are applicable to U.S. children?\nCan you think of confounding factors that limit transfer of the results to the U.S?\nAre you surprised that there is no evidence of adverse effects on adults?\n\n\n\nIn domains and applications where experimentation is not possible and confounding factors are present, we try to establish causation by a process called causal inference. By studying which variables act on each other, causality can be inferred.\n\n\nHill’s Criteria\nEstablishing causality from observational data, as in the case of Snow’s water quality study, is a common problem in epidemiology, the study of the distribution of diseases and health risks. Establishing designed experiments with randomized control of factors is often not possible in those studies.\nThe English epidemiologist and statistician Sir Author Bradford Hill established nine principles that allow one to move from association to causation. These are known as Hill’s criteria, developed to establish causation involving environmental exposure. Hill was part of the research team that confirmed the link between smoking and lung cancer. The criteria are:\n\nStrength: strong association is stronger evidence of causality. A small association does not rule out a causal effect, however.\nConsistency: similar studies by others in different places with different samples give consistent findings.\nSpecificity: the more specific the association between a factor and an effect, the more likely we are dealing with cause and effect. The association is specific when the cause leads to only one outcome and the outcome can only come from the one cause.\nTemporarlity: the effect comes after the cause.\nGradient: an increase in level, intensity, duration or total level of exposure to the potential cause leads to progressive increase in (the likelihood of) the outcome.\nPlausibility: the association is plausible based on known scientific facts.\nCoherence: agreement between epidemiological and laboratory findings. The interpretation of the data does not seriously conflict with what is already known about the disease or exposure.\nExperimentation: if experimentation is possible, it provides results in support of the causal hypothesis (provides strong evidence).\nAnalogy: similarity between things that are otherwise different. Scientists can use prior knowledge and patterns to infer similar causal associations.\n\nHill’s criteria should be viewed as a guideline for establishing causality based on association. Proving causality is not done by clicking check boxes. Meeting the criteria increases the likelihood that a factor causes an effect.\n\n\nAssignment: Causal Link between RHI and CTE\n\n\nTiaina Baul “Junior” Seau was an outstanding linebacker who played in the National Football League for 20 years, mostly with the San Diego Chargers and also the Miami Dolphins and New England Patriots. He committed suicide in 2012 by shooting himself in the chest. Junior did not leave a suicide note but a piece of paper with lyrics from the country song “Who I Ain’t”. An autopsy confirmed that Junior Seau had suffered from chronic traumatic encephalopahty (CTE) believed due to repeated head trauma he experienced as a football player.\n\n\n\n\n\n\nFigure 5.15: Junior Seau playing for the New England Patriots. Source: Wikipedia\n\n\n\nNowinski et al. (2022) applied the Hill criteria to establish causality between repeated head impacts (RHI) and CTE. The authors discuss previous research on the association of the two and resistance to calling the link between RHI and CTE causal. Interestingly, it appears that the causal link between the two was settled throughout the 20th century when causality was called into question again. The article is available online.\n\nWhat reasons were given by some of the organizations involved in the debate (like CISG) to resist declaring a causal link between RHI and CTE?\nWhat do you believe was their motivation to do so?\nIn the section Understanding Causation, the article examines each of Hill’s nine criteria. Cite one argument from the article in support of each criterion.\nPer the Discussion section of the article, what are valid reasons why scientists might remain skeptical of a causal link between RHI and CTE?\nWhat language did the authors use in the Conclusion to indicate a causal relationship between RHI and CTE?\nBased on the evidence provided, should the conclusions drawn from data about adult athletes be applied to children? Argue for or against and why or why not?\n\n\n\n\n\n\nFigure 5.1: Relationship between highway fatalities and lemon imports from Mexico.\nFigure 5.2: Positive correlation.\nFigure 5.3: Negative correlation.\nFigure 5.7: Chocolate consumption and number of Nobel laureates.\nFigure 5.13: Experimental layout for poppy experiment.\nFigure 5.14: Ear length. From Heathcote (1995).\n\n\n\nBenson H., Dusek J. A., Sherwood J. B., P. Lam, C. F. Bethea, W. Carpenter, S. Levitsky, et al. 2006. “Study of the Therapeutic Effects of Intercessory Prayer (STEP) in Cardiac Bypass Patients: A Multicenter Randomized Trial of Uncertainty and Certainty of Receiving Intercessory Prayer.” American Heart Journal 151 (4): 934–42.\n\n\nHeathcote, James A. 1995. “Why Do Old Men Have Big Ears?” BMJ 311 (7021): 1668. https://doi.org/10.1136/bmj.311.7021.1668.\n\n\nMesserli, F. H. 2012. “Chocolate Consumption, Cognitive Function, and Nobel Laureates.” New England Journal of Medicine 367: 1562–64.\n\n\nNeyman, Jerzy. 1952. Lectures and Conferences on Mathematical Statistics and Probability. Graduate School, Dept. of Agriculture, Washington.\n\n\nNowinski, Christopher J., Samantha C. Bureau, Michael E. Buckland, Maurice A. Curtis, Daniel H. Daneshvar, Richard L. M. Faull, Lea T. Grinberg, et al. 2022. “Applying the Bradford Hill Criteria for Causation to Repetitive Head Impacts and Chronic Traumatic Encephalopathy.” Frontiers in Neurology 13. https://doi.org/10.3389/fneur.2022.938163.\n\n\nSilver, Nate. 2012. The Signal and the Noise: Why so Many Predictions Fail–but Some Don’t. Penguin Books.\n\n\nSpiegelhalter, David. 2021. The Art of Statistics. How to Learn from Data. Basic Books.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and Causation</span>"
    ]
  },
  {
    "objectID": "dataliteracy.html",
    "href": "dataliteracy.html",
    "title": "6  Data Literacy",
    "section": "",
    "text": "6.1 Uncertainty",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Literacy</span>"
    ]
  },
  {
    "objectID": "dataliteracy.html#sec-literacy-signal-noise",
    "href": "dataliteracy.html#sec-literacy-signal-noise",
    "title": "6  Data Literacy",
    "section": "6.2 Signal and Noise",
    "text": "6.2 Signal and Noise\nSeparating the signal from the noise is a key objective in all data analysis. The signal represents the systematic, non-random effects in the data. Data scientists and statisticians define the noise as the unpredictable randomness around the signal. A slightly different, and also useful, definition of noise stems from intelligence analysis. The signal is the information we are trying to find, the noise is the cacophony of other information that obscures the signal. That information might well be a signal for something else but it is irrelevant or useless for the event the intelligence analyst is trying to predict.\nInformation not being relevant for the signal we are trying to find is the key. In the view of the statistician, that information is due to random events.\nFinding the signal is not trivial, different analysts can arrive at different models to capture it. Signals can be obscured by noise. What appears to be a signal might just be random noise that we mistake for a systematic effect.\n\n\nExample: Theophylline Concentration\n\n\nFigure 6.1 shows the concentration of the drug theophylline over 24 hours after administration of the drug in two groups of patients. There are 98 data points of theophylline concentration and measurement time. What are the signals in the data? What is noise?\n\n\n\n\n\n\n\n\nFigure 6.1: Theophylline concentration over time in two groups of patients.\n\n\n\n\n\nThe first observation is that the data points are not all the same over time, otherwise they would fall on a horizontal straight line: there is variability in the data. Separating signal and noise means attributing this variability to different sources: some systematic, some random.\nFocusing on either the open circles (group 1) or the triangles (group 2), you notice that points that are close in time are not necessarily close in the concentration measurement. Not all patients were measured at exactly the same time points, but at very similar time points. For example, concentrations were measured after about 7, 9, and 12 hours. The differences in the concentration measurements among the patients receiving the same dosage might be due to patient-to-patient variability or measurement error.\nFocusing on the general patterns of open circles and triangles, it seems that the triangles appear on average below the average circle a few hours after administration. Absorption and elimination of theophylline appears to behave differently in the two groups.\nMuch of the variability in the data seems to be a function of time. Shortly after administering the drug the concentration rises, reaches a maximum level and then declines as the drug is eliminated from the body. Note that this sentence describes a general overall trend in the data here.\nWhich of these sources of variability are systematic—the signals in the data— and which are random noise?\n\nPatient-to-patient variability within a group at the same time of measurement: we attribute this to random differences among the participants.\nPossible measurement errors in determining the concentrations: random noise\nOverall trend of drug concentration over time: signal\nDifferences among the groups: signal\n\nThese assignments to signal and noise can be argued. For example, we might want to test the very hypothesis that there are no group-to-group differences. If that hypothesis is true, any differences between the groups we discern in Figure 6.1 would be due to chance; random noise in other words.\nThe variability between patients could be due to factors such as age, gender, medical condition, etc. We do not have any data about these attributes. By treating these influences as noise, we are making important assumptions that their effects are irrelevant for conclusions derived from the data. Suppose that the groups refer to smokers and non-smokers but also that group 1 consists of mostly men and group 2 consists of mostly women. If we find differences in theophylline concentration over time among the groups, we could not attribute those to either smoking status or gender.\n\n\nA common reason to mistake signal for noise is overfitting a model, a concept we return to in Section 6.5.1.\nAnother reason is if there is simply no signal at all. Figure 6.2 is taken from Silver (2012, 341) and displays six “trends”. Four of them are simple random walks, the result of pure randomness. Two panels show the movement of the Dow Jones Industrial Average (DJIA) during the first 1,000 trading days of the 1970s and 1980s. Which of the panels are showing the DJIA and which are random noise?\n\n\n\n\n\n\nFigure 6.2: Figure 11-4 from Silver (2012), Random walk or stock market data?\n\n\n\nWhat do we learn from this?\n\nEven purely random data can appear non-random over shorter sequences. We can easily fall into the trap of seeing a pattern (a signal) where there is none. After drawing two unlikely poker hands in a row there is not a greater chance of a third unlikely hand unless there is some systematic effect (cards not properly shuffled, game rigged). Our brains ignore that fact and believe that we are more lucky than is expected by chance.\nData that contains clear long-run signals—the stock market value is increasing over time—can appear quite random on shorter sequences. One a day to day basis predicting whether the market goes up or down is very difficult. In the long run ups and downs are almost equally likely. Upswings have a slight upper hand and on average are greater than the downswings, increasing the overall value in the long term. Traders who try to beat the market over the short run have their work cut out for them.\n\nBy the way, panels D and F in Figure 6.2 are from actual stock market data. Panels A, B, C, and E are pure random walks. It would not be surprising if investors would bet money on “trend” C.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Literacy</span>"
    ]
  },
  {
    "objectID": "dataliteracy.html#sec-literacy-accuracy-precision",
    "href": "dataliteracy.html#sec-literacy-accuracy-precision",
    "title": "6  Data Literacy",
    "section": "6.3 Accuracy and Precision",
    "text": "6.3 Accuracy and Precision\nThe terms accuracy and precision are often used interchangeably. In the context of measurement and data analysis they mean different things and the distinction is important.\nWhen talking about measuring devices, accuracy and precision are defined as\n\nAccuracy: How close are measurements to the true value\nPrecision: How close are measurements to each other\n\nWhen folks say that something is precise, what they often mean is that it is accurate. For example, when you measure something that is 8 feet long with a tape measure and the tape reads 8 feet, then it is accurate. If you repeat the measurement several times and all readings are close to 8 feet, then the tape (and you as its operator) is precise.\nPrecision refers to the repeatability of a method, accuracy refers to its proximity to a target value.\nTo demonstrate the difference between accuracy and precision, the dart board bullseye metaphor is helpful. Figure 6.3 shows four scenarios of shooting four darts each at a dart board. Suppose we are trying to hit the bullseye in the center of the board; it is the true value we are trying to measure.\n\nPattern A is the result of a thrower who is neither accurate nor precise. The throws vary greatly from each other (lack of precision), and the average location is far from the bullseye (inaccurate).\nPattern B is the result of a thrower who is inaccurate but precise. The throws group tightly together (high precision) but the average location misses the bullseye (the average distance from the bullseye is not zero).\nPattern C is typical for a player who is not precise, but accurate. The throws vary widely (lack of precision) but the average distance of the darts from the bullseye is close to zero—on average the thrower hits the bullseye.\nPattern D is that of an accurate and precise player; the darts group tightly together and are centered around the bullseye.\n\n\n\n\n\n\n\nFigure 6.3: Accuracy and precision—the dart board bullseye metaphor.\n\n\n\nAnother way to phrase accuracy and precision are in terms of the distribution of the measurements. Accuracy is about the average (central tendency) of the values, precision is about their variability. A statistical method that is not accurate—does not hit the target on average—is said to be biased.\n\nWhy do we worry about the distinction of precision and accuracy?\nIf we have a choice among different methods of drawing conclusions from data, we would opt for the one that is most precise and most accurate. To decide whether to buy, hold, or sell stock of company X, it would be great if we can forecast the stock value accurately and precisely. Imagine that you are the coach of the dart team. It is the end of the tournament, your team has one last dart left to throw; the team that gets closest to the bullseye will win the tournament. Which of the four types of players would you ask to make the throw? The precise and accurate player D is the safe bet.\nUnfortunately, we often do not have the luxury of a uniformly optimal method that has no bias and highest precision among all methods. We often need to make a compromise, and that means we have to weigh precision and accuracy against each other.\nStatisticians historically resolve the tension by demanding that methods are accurate (have zero statistical bias). Among all competing methods in that collection they then choose the one that has the highest precision (smallest variance). That is a reasonable approach especially if accuracy is most important.\nHowever, it is possible that the most precise unbiased method is much more variable than a biased method. In the dartboard example, if player D has retired from the sport, then we might bet on player B for the last throw. She has a higher likelihood of getting close to the bullseye than the more erratic—but close on average—player C.\nWe also worry about the distinction between accuracy and precision because our confidence in statements about data should include both. The precision of a method translates into uncertainty about its results. An imprecise method leads to uncertain conclusions. Pundits who offer ostensibly expert opinions about topics frequently provide predictions without telling us how precise they are:\n\n“Candidate A is ahead of candidate B by three points.”\n“Inflation will decrease by half a point in the next quarter.”\n“60% of adults between 35 and 45 support this legislation.”\n“The average temperature on the planet will increase by 1.4 degrees centigrade by 2030”.\n\nThese statements let us tacitly assume that the prediction is accurate and infinitely precise. If accompanied by a measure of precision—that is, uncertainty—the message is much more nuanced:\n\n“Candidate A is ahead of candidate B by three points. The margin of error is +/- 5 points”.\n\nThe margin of error covers the possibility that candidate B is in the lead.\n\n“The average temperature on the planet will increase by 1.4 degrees centigrade by 2030”. The 95% prediction interval in 2030 ranges from 0.2 to 2.6 degrees.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Literacy</span>"
    ]
  },
  {
    "objectID": "dataliteracy.html#sec-literacy-correlation",
    "href": "dataliteracy.html#sec-literacy-correlation",
    "title": "6  Data Literacy",
    "section": "6.4 Causation and Correlation",
    "text": "6.4 Causation and Correlation\n\nObservational and experimental data",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Literacy</span>"
    ]
  },
  {
    "objectID": "dataliteracy.html#sec-literacy-bias-variance",
    "href": "dataliteracy.html#sec-literacy-bias-variance",
    "title": "6  Data Literacy",
    "section": "6.5 Bias and Variance Tradeoff",
    "text": "6.5 Bias and Variance Tradeoff\n\nOverfitting\n\n\nUnderfitting",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Literacy</span>"
    ]
  },
  {
    "objectID": "dataliteracy.html#all-models-are-wrongsome-are-useful",
    "href": "dataliteracy.html#all-models-are-wrongsome-are-useful",
    "title": "6  Data Literacy",
    "section": "6.6 All Models are Wrong–Some are Useful",
    "text": "6.6 All Models are Wrong–Some are Useful\nIn a 1976 paper, George E.P. Box declared “All models are wrong” (Box 1976). Later, he added a qualification which turned into a frequently cited quote about modeling:\n\nAll models are wrong, some are useful. (Box and Draper 1987, 424).\n\n\nCompartmental Models in Epidemiology\nSIR (Susceptible–Infectious–Recovered) model is a standard class of compartmental models in epidemiology, describing how an infectious disease moves through a population. On the surface this makes sense: at each point in time an individual is in one of three states, called compartments:\n\nYou have not had the disease but you are susceptible to it (S compartment)\nYou are infected by the disease (I compartment)\nYou are recovered from the disease, or dead (R compartment)\n\nA vaccination, then, is a shortcut that moves an individual directly from the S to the R compartment, bypassing the infected state.\nHowever, there are a number of assumptions in the SIR model that can invalidate the model for some diseases and circumstances:\n\nThe disease progresses in only one direction, from S to I to R.\nEveryone is equally susceptible and behaves the same way\nEveryone is equally likely to be vaccinated, if a vaccine is available\nAll members of the populations intermingle at random\n\nThe rate at which the disease spreads through the population is measured by the basic reproduction number, \\(R_0\\). This number, which we became all too familiar with during the COVID-19 pandemic, measures the number of uninfected people expected to catch the disease from an infected individual. An \\(R_0\\) of 3 means that someone who contracts the disease is expected to pass it on to three other individuals. In the absence of vaccines or quarantines, any disease with \\(R_0 &gt; 1\\) will eventually spread to the entire population.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Literacy</span>"
    ]
  },
  {
    "objectID": "dataliteracy.html#agent-based-models",
    "href": "dataliteracy.html#agent-based-models",
    "title": "6  Data Literacy",
    "section": "6.7 Agent-based Models",
    "text": "6.7 Agent-based Models\nA different tactic than to rely on sophisticatedly simple models that abstract the essence of the thing we wish to study is to simulate the process in all its complexity, essentially building a digital twin of the thing we wish to study.\nThat is essentially the approach taken in weather prediction. Rather than relying on statistical models that predict based on historical data, weather predictions are based on complex physical simulations of the atmosphere.\n\n\n\nFigure 6.3: Accuracy and precision—the dart board bullseye metaphor.\n\n\n\nBox, George E. P. 1976. “Science and Statistics.” Journal of the American Statistical Association 71 (356): 791–99.\n\n\nBox, George E. P., and Norman R. Draper. 1987. Empirical Model-Building and Response Surfaces. John Wiley & Sons, New York.\n\n\nSilver, Nate. 2012. The Signal and the Noise: Why so Many Predictions Fail–but Some Don’t. Penguin Books.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Literacy</span>"
    ]
  },
  {
    "objectID": "intuition.html",
    "href": "intuition.html",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "",
    "text": "7.1 The Prisoner’s Dilemma\nSuppose that Andy and Brie are arrested as members of a criminal gang and held separately by the police. They cannot communicate. There is enough evidence to convict them on a lesser charge, but not on the principal charge. The police offers the following deal:\nHow should Andy and Brie behave to optimize their positions, that is, look out after their own interest?\nThe result of such a game is typically displayed in a payoff matrix that shows in each cell the payoff for the two players.\nThe “payoffs” are shown in the matrix as negative numbers, as they represent a penalty, years of imprisonment. The goal is to maximize the payoff, a number as large as possible.\nThe best situation for Andy is to testify when Brie remains silent. He would go free in this case (and does not mind Brie spending three years behind bars). Similarly, the best situation for Brie is to testify when Andy remains silent. These are the two diagonal cells in Table 7.1.\nThe situation does not play out as well for them if one testifies and the other also testifies. What is the best strategy?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#the-prisoners-dilemma",
    "href": "intuition.html#the-prisoners-dilemma",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "",
    "text": "If they both remain silent, they will each serve one year in prison.\nIf one testifies against the other, but the other one does not, the one who testified will be set free while the other serves three years in prison.\nIf Andy and Brie both testify against each other, they will each serve two years.\n\n\n\n\n\n\nTable 7.1: Expected payoffs in prisoner dilemma\n\n\n\n\n\n\n\n\n\n\n\nBrie remains silent\nBrie testifies\n\n\n\n\nAndy remains silent\n\\((-1, -1)\\)\n\\((-3, 0)\\)\n\n\nAndy testifies\n\\((0, -3)\\)\n\\((-2, -2)\\)\n\n\n\n\n\n\n\n\n\n\nNash Equilibrium\nThe Nash equilibrium is a concept in game theory. It applies to non-cooperative games where players compete against each other. In the equilibrium state, no player can gain an advantage by changing their strategy. This assumes that the other player’s strategies do not change.\nSuppose players Andy and Brie have chosen strategies A and B, respectively. In the Nash equilibrium, there is no other strategy available to Andy that would increase his expected payoff if Brie stays with strategy B. Similarly, there is no other strategy available to Brie that would increase her expected payoff from the game if Andy stays with strategy A.\nThe Nash equilibrium tells us not to consider player’s action in isolation. Instead, we need to take into account what other players are expected to do in evaluating a player’s choices.\n\nThe best outcome for either Andy and Brie would be to go free. But they do not know how the other one will behave. So what is the best strategy to play this game? Let’s rephrase testifying and remaining silent in terms of defecting and collaborating players of a game.\n\n\n\nTable 7.2: Expected payoffs in prisoner dilemma\n\n\n\n\n\n\nBrie collaborates\nBrie defects\n\n\n\n\nAndy collaborates\n\\((-1, -1)\\)\n\\((-3, 0)\\)\n\n\nAndy defects\n\\((0, -3)\\)\n\\((-2, -2)\\)\n\n\n\n\n\n\nIf Andy defects, his penalty will be less, regardless of whether Brie is collaborative or not (0 or 2 years compared to 1 or 3 years). The same applies to Brie, if she defects her penalty will be less regardless of what Andy does. The Nash equilibrium is that both players defect although they suffer worse penalties than if they had both cooperated.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#guarding-criminals",
    "href": "intuition.html#guarding-criminals",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.2 Guarding Criminals",
    "text": "7.2 Guarding Criminals\nSuppose you are guarding \\(n\\) criminals in an open field. You have one gun with a single bullet. You are a good shot and being fired at means death—the criminals know that. Their behavior is governed by the following rules:\n\nIf any of them has a non-zero probability of surviving, they will attempt to escape.\nIf a criminal is certain of death, they will not attempt to escape.\n\nHow do you guard the criminals and stop them from escaping?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nImagine there is only a single criminal, \\(n=1\\). Since he/she would definitely be shot at during an escape, they would face certain death and not escape.\nWhat happens if there are two criminals? If they both try to escape, there is a 50:50 chance to survive, hence they will both try to escape. To prevent that from happening you would tell one of the two (you do not need to tell both!) that you would shoot them, should they both attempt to escape. That criminal now faces certain death and will not escape. That brings you back to the situation with a single criminal.\nHow does this generalize to larger groups of criminals? Assign a number from 1 to \\(n\\) to the criminals and tell them that should any subgroup of them try to escape, the one with the highest number in the group will be shot.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#three-jars",
    "href": "intuition.html#three-jars",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.3 Three Jars",
    "text": "7.3 Three Jars\nThree opaque jars are sitting on a table. The jars are labeled “Apples”, “Oranges”, and “Apples & Oranges”. Unfortunately, all three are labeled incorrectly.\n\n\n\n\n\n\nFigure 7.1: Three opaque jars.\n\n\n\nYour task is to assign the labels correctly to the jars. What is the smallest number of fruit you have to choose in order to correctly label the three jars?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou choose one fruit from the jar that is labeled incorrectly as “Apples & Oranges”. If you pull an apple, you know this is the jar with the apples, otherwise it is the jar with the oranges. Now you have two jars left whose labels just need to be flipped since you were told that all three jars are labeled incorrectly.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#pattern-recognition-1",
    "href": "intuition.html#pattern-recognition-1",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.4 Pattern Recognition #1",
    "text": "7.4 Pattern Recognition #1\nFigure 7.2 shows a logic reasoning puzzle. The first row makes sense if the strange operator is addition, but that does not work for the next rows. You have to find the meaning of that operator, then apply the pattern to solve the last equation.\n\n\n\n\n\n\nFigure 7.2: Can you solve this?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe need to find a pattern that expresses the operations in terms of familiar algebra. If the operator in Figure 7.2 is interpreted as multiplication then we get 4, 10, 18, all smaller than the values on the right hand side. How much smaller? Exactly by the left-most number. The pattern that seems to apply to the first three rows is\n\nmultiply the two numbers\nthen add the number on the left\n\nApplying this pattern to the last row yields 96 as the solution (Figure 7.3).\n\n\n\n\n\n\nFigure 7.3: A solution.\n\n\n\nThis, by the way, is not the only solution. There are other patterns that will lead to a different result for the last row. Those patterns are equally valid. Can you find another pattern that yields a solution?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#pattern-recognition-2",
    "href": "intuition.html#pattern-recognition-2",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.5 Pattern Recognition #2",
    "text": "7.5 Pattern Recognition #2\nHere is a sequence of numbers.\n\\[\n\\begin{array}{c}\n1 \\\\\n11 \\\\\n21 \\\\\n1211 \\\\\n111221 \\\\\n312211 \\\\\n??\n\\end{array}\n\\]\nWhat is the next number in the sequence?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWhat is the pattern in the sequence of numbers?\n\nThe first row is the number 1, it is also “one one”.\nThe second row is the number 11, it is also “two ones”.\nThe third row is the number 21, it is also “one two and one one”.\n\nThe pattern is that the numbers for the following row are obtained by spelling out the numbers in the current row, then replacing the words with the numbers they represent. For example, take 1211 in the fourth row. Spelling it out gives “one one one two two ones”. Now replace the words with numbers: “111221”.\nThe missing entry at the end of the sequence is thus \\[\n13112221\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#birthday-problem",
    "href": "intuition.html#birthday-problem",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.6 Birthday Problem",
    "text": "7.6 Birthday Problem\nThis is a classical problem in probability, and a popular one because it is relatable yet somewhat counterintuitive. The probability is higher than what most people expect. It goes like this:\n\nWhat is the probability that in a group of \\(n\\) randomly chosen people, at least two share the same birthday?\n\n“Birthday” is meant as one of 365 days of the year, not adjusting for leap years. Also, we are not taking the birth year into account. A birthday for the purpose of this problem is April 10, or August 15, etc.\nThe standard version of the problem uses \\(n=23\\), because you can imagine yourself in a group of that size—a classroom, for example—and the probability of at least two shared birthdays is also relatable.\nHow likely do you think at least two people share a birthday in a group of 23?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe probability of at least two shared birthdays in a group of 23 is about 0.5; it is 0.05073, to be more exact. How do you interpret that? If you were to assemble groups of 23 randomly chosen people, than half of those groups would have at least two shared birthdays. Pretty high, eh?\nWhat happens to the probability of a shared birthday when the groups get larger? How about in a group of 35 people? The probability of a shared birthday increases to 0.814. In a group of 50 people, the probability is 0.97. In a group of 100, it is virtually certain that there are at least two identical birthdays, \\(p=0.999999\\). With only 10 people in a group, it would be surprising to have identical birthdays, but it is not a rare event, \\(p=0.117\\).\n\nFor those interested, how do you calculate those probabilities? First, whenever you see the expression “at least” in a probability statement, it is probably easier to calculate the probability of the complement event and subtract that from 1. \\[\n\\Pr(\\text{at least two identical birthdays}) = 1 - \\Pr(\\text{no matching birthdays})\n\\]\nWhat is the probability that no birthdays match in a group of \\(n\\)? You can compute this by considering the possible choices as people enter the group. The birthday of the first person can be chosen from 365 days, but the birthday for the second person has only 364 choices, otherwise we would have a match. Since the members of the group are chosen at random, the birthdays are independent and the probability of no matches is the product \\[\n\\Pr(\\text{no matches}) = \\frac{365}{365} \\times \\frac{364}{365} \\times \\cdots \\times \\frac{365-n+1}{365}\n\\]\nYou can write this in terms of factorials as \\[\n\\Pr(\\text{no matches}) = \\frac{1}{365^n} \\frac{365!}{(365-n)!}\n\\] Finally, the probability of at least two shared birthdays is \\[\n\\Pr(\\text{at least two shared birthdays}) = 1 - \\frac{1}{365^n} \\frac{365!}{(365-n)!}\n\\]\nIf you were to compute this, you’d run into problems because the factorials are larger than what a finite precision computer can represent. The following R function uses two tricks to compute the birthday probability efficiently:\n\nCompute the probability on the logarithmic scale, then exponentiate at the end\nUse the fact that for an integer \\(k\\), \\(k!\\) is \\(\\Gamma(k+1)\\), where \\(\\Gamma()\\) is the Gamma function.\n\nThe lgamma function in R computes the log of the Gamma function, and that gives us access to an efficient way to compute the components of the probability on the log scale.\n\nbirthday_prob &lt;- function(n) {\n   log_p &lt;- lgamma(365+1) - lgamma(365-n+1) - n*log(365)\n   return (1-exp(log_p))\n}\n\nbirthday_prob(10)\n\n[1] 0.1169482\n\nbirthday_prob(23)\n\n[1] 0.5072972\n\nbirthday_prob(35)\n\n[1] 0.8143832\n\nbirthday_prob(50)\n\n[1] 0.9703736\n\nbirthday_prob(100)\n\n[1] 0.9999997",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#minimum-cuts",
    "href": "intuition.html#minimum-cuts",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.7 Minimum Cuts",
    "text": "7.7 Minimum Cuts\nImagine that you hire a consultant to work for you for five days. At the end of each day you need to pay them 1/5th of a gold bar. You have a single gold bar (worth 5 fifths) and need to cut it up so you can pay the consultant at the end of each day.\n\n\n\n\n\n\nFigure 7.4: A gold bar that needs to be cut up.\n\n\n\n\nWhat is the minimum number of cuts that allow you to pay the consultant every day?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou need only two cuts to cut the gold bar into three pieces of sizes 1/5, 1/5, and 3/5.\n\n\n\n\n\n\nFigure 7.5: No more than two cuts are needed.\n\n\n\nThen you pay the consultant as follows:\n\nDay 1: give them a 1/5 gold bar\nDay 2: give them the second 1/5 gold bar\nDay 3: take back the two 1/5 bars and hand them the 3/5 bar\nDay 4: give them a 1/5 gold bar\nDay 5: give them the second 1/5 gold bar",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#when-to-choose-the-ticket",
    "href": "intuition.html#when-to-choose-the-ticket",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.8 When to Choose the Ticket",
    "text": "7.8 When to Choose the Ticket\nAn airline has a single seat open on a flight, but \\(n=100\\) standby passengers hoping to get on the flight. You are one of the passengers on standby. To be fair to all standby passengers, the airline decides to drop 100 equal-sized pieces of paper into a bucket. 99 of them are blank, one says “Last Seat”. The papers are folded and shuffled in the bucket.\nThe standby passengers queue and each passenger gets to pick one piece of paper without replacement—that is, they keep the slip and do not return it to the bucket. Also they cannot unfold and look at the slip until all of them re drawn. After the last slip is drawn the standby passengers announce who is the lucky person that drew the “Last Seat” by checking their slip.\nHere is the question: if you have your choice to pick first, second, last, or at any particular position in the queue, which position would you choose?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt does not matter when you draw the paper if the pieces were properly shuffled. This is a completely random sample even if the sampling is done sequentially. Your chance of drawing the “Last Seat” slip is 1/100, whether you draw first, last, or at any other position in the queue.\nNote that this would be different if passengers would announce the result of their draws before the next draw. The conditional probability of choosing the “Last Seat” slip on the next draw increases with every bank slip that preceded.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#how-many-squares-on-a-chessboard",
    "href": "intuition.html#how-many-squares-on-a-chessboard",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.9 How Many Squares on a Chessboard",
    "text": "7.9 How Many Squares on a Chessboard\nA chess board is made up of eight rows and columns of black and white positions (Figure 7.6). How many squares are on a board?\n\n\n\n\n\n\nFigure 7.6: Chess board.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe quick answer is \\(8 \\times 8 = 64\\) squares. However, that is only part of the story. The entire board is a single square as well, made up of the 64 individual squares. And we could place all kinds of \\(2 \\times 2\\) squares inside the larger frame.\nIf you think about it for a bit there are \\(8^2\\) squares of size \\(1 \\times 1\\), \\(7^2\\) squares of size \\(2 \\times 2\\), \\(6^2\\) squares of size \\(3 \\times 3\\) and so on. The total number of squares on a chess board is\n\\[\n8^2 + 7^2 + 6^2 + 5^2 + 4^2 + 3^2 + 2^2 + 1^2 = 204\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#book-sorting",
    "href": "intuition.html#book-sorting",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.10 Book Sorting",
    "text": "7.10 Book Sorting\nSuppose you are working in a library and are sorting books from a box that contains 32 fiction (F) and 17 non-fiction (NF) books. A steady supply of new books is available to add to the box. Your sorting algorithm goes as follows:\n\nYou randomly choose 2 books from the box.\nBased on the types of books chosen you add another book from the supply to the box:\n\nif you choose two fiction books (F,F) you add a new fiction book to the box\nif you choose two non-fiction books (NF, NF) you also add a fiction book to the box\nif you choose one fiction and one non-fiction book (F,NF or NF,F) then you add a non-fiction book to the box.\n\n\nThe entire procedure is depicted in Figure 7.7.\n\n\n\n\n\n\nFigure 7.7: Book sorting routine. Source\n\n\n\nSince you add only one book to the box for every two books you remove, the box will eventually be empty. What is the type of the last book in the box? Is it a fiction or a non-fiction book?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe number of books in the bin goes down by one with each cycle: two books are removed from the bin, one book is added. How does this affect the number of fiction and non-fiction books that remain?\nLet’s see how the number of non-fiction books in the bin changes in cases 1.–3. In the first case, there is no change. In the second case, the number of non-fiction books goes down by 2. In the third case, the number of non-fiction books also does not change: one is removed, one is added.\nSince the number of NF books initially is an odd number, 17, we can conclude that after each cycle the number of NF books remains an odd number. It can never be an even number. Which leads to the conclusion that if there is only one book left in the bin it must be a non-fiction book.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#inverted-triangle",
    "href": "intuition.html#inverted-triangle",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.11 Inverted Triangle",
    "text": "7.11 Inverted Triangle\nFigure 7.8 shows a triangle made from 10 coins. Can you change this into an upside-down triangle by moving only 3 coins?\n\n\n\n\n\n\nFigure 7.8: Inverting the coin triangle.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe solution is shown in Figure 7.9. First, focus on the seven coins in the center of the triangle. The original and the inverted triangle share these; they do not need to move at all. We can focus on the three coins at the edges.\n\n\n\n\n\n\nFigure 7.9: Moving the three coins.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#the-spare-tire",
    "href": "intuition.html#the-spare-tire",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.12 The Spare Tire",
    "text": "7.12 The Spare Tire\nYour car has four tires mounted to the wheels and a spare tire (S). That gives you five tires to work with. Each of the tires lasts at most 30,000 miles. If you can exchange tires among the five as many times as you wish, what is the furthest distance you can travel before you need to purchase a new tire?\nFigure 7.10 depicts the initial tire life prior to driving the first mile. All tires, including the spare (S) have the same life expectancy of 30,000 miles.\n\n\n\n\n\n\nFigure 7.10: Tire life before driving the first mile.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe maximum total distance the five tires could travel before they are all worn out is 30,000 x 5 = 150,000 miles. The minimum distance of travel before you have to buy at least one new tire is 30,000 miles; it is achieved if you do not use the spare tire and run down the four tires currently mounted.\nBy optimizing how you use the spare tire, there must be an achievable distance between 30,000 and 150,000 miles. The optimal strategy is to wear all tires equally and to use the spare tire as much as possible. But we cannot use the spare for more than 30,000 miles, same as with the other four tires.\nIf the four tires on the car are equally worn, we can go at most 150,000/4 = 37,500 miles. The strategy is to get 30,000 miles from each of the tires on the car and 4 times 7,500 = 30,000 miles from the spare tire. In other words, the spare will have to give each of the four tires a 7,500 mile break.\nFigure 7.11 shows how the spare tire is rotated for another tire after each leg of 7,500 miles. The right rear tire comes off after the fourth leg, it is worn out. The other tires still have 7,500 miles of life to go.\n\n\n\n\n\n\nFigure 7.11: Remaining tire life after 7,500, 15,000, 22,50, and 30,000 miles., miles",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#robot-triangle",
    "href": "intuition.html#robot-triangle",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.13 Robot Triangle",
    "text": "7.13 Robot Triangle\nThere are many versions of this basic puzzle, using ants, camels, and other animals. We use robots here, the puzzle goes like this. Three robots are placed at the corners of a triangle. A robot can choose to move along either side of the triangle that meet at its corner (Figure 7.12). What is the probability that any two robots will collide?\n\n\n\n\n\n\nFigure 7.12: Robot triangle.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nEach robot has two possible movements, so there are a total of 2 x 2 x 2 = 8 possible moves on the triangle. There are two ways in which there won’t be any collisions, if all choose to go clockwise or counter-clockwise. In those cases they will follow each other around the triangle (Figure 7.13).\n\n\n\n\n\n\nFigure 7.13: Robots moving without running into each other.\n\n\n\nAny other choice of movements will result in at least one collision. So the probability of any collision if the robots choose their movements at random is 6/8 = 3/4. There is a 75% chance that any two robots will collide.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#truth-telling",
    "href": "intuition.html#truth-telling",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.14 Truth Telling",
    "text": "7.14 Truth Telling\nThis puzzle is about logic reasoning and not about probability. Surprisingly, it is related to the previous robot movement puzzle.\nConsider the following three statements:\n\nGavin says that Brian is a liar.\nBrian says that Jenn is a liar.\nJenn says that both Gavin and Brian are liars.\n\nWho is telling the truth and who is lying?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis puzzle is related to the robot movement in that there are \\(2^3 = 8\\) possible choices, each of the three characters could either be truthful or lying. It is different from the robot movement in that it is not a question of probability. While robots choose one of the two directions at random, our characters are either lying or telling the truth. We have to reason which one it is.\nWith 8 possible choices you can go about it by finding combinations that are inconsistent, a process of elimination.\nSuppose that Jenn tells the truth. Then Gavin and Brian are liars. According to Gavin’s statement, that would mean Brian is telling the truth. But Brian’s statement contradicts the assumption that Jenn tells the truth. Jenn must be a liar.\nIf Jenn is not telling the truth, there are three possibilities:\n\nGavin is truthful and Brian is not\nGavin is a liar and Brian is truthful\nBoth are truthful.\n\nLet’s look at the first option. If Gavin tells the truth than Brian is lying, which means Jenn would be truthful. We already ruled out this possibility. But if Gavin is not truthful, then 3. cannot be the case either.\nWe are down to the second option: Brian speaks the truth and the other two are liars. Let’s see if everything makes sense in this scenario: If Gavin does not speak the truth, then Brian is not a liar. Brian’s statement that Jenn is a liar is consistent with what we already found.\nConclusion: Only Brian is truthful.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#clock-made-with-matches",
    "href": "intuition.html#clock-made-with-matches",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.15 Clock Made With Matches",
    "text": "7.15 Clock Made With Matches\nYou have two wooden sticks and a box of matches. When a sticks is lit it will burn completely in exactly one hour. How do you use these ingredients to measure exactly 45 minutes?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLight the first stick on one end and light the second stick on both ends. Since an entire stick burns in one hour, the stick lit on both ends will burn down in 30 minutes (Figure 7.14).\n\n\n\n\n\n\nFigure 7.14: Initial lighting of sticks.\n\n\n\nAt that point light the first stick on the other end. This will double the speed with which that stick, now reduced to 30 minutes burn time, will burn.\nWhen the first stick is completely burned down, 45 minutes will have passed (Figure 7.15).\n\n\n\n\n\n\nFigure 7.15: After 30 minutes, light the other end of the first stick.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#two-stacks-of-cards",
    "href": "intuition.html#two-stacks-of-cards",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.16 Two Stacks of Cards",
    "text": "7.16 Two Stacks of Cards\nYou have two stacks of cards. The first is a regular 52-card deck. The second stack contains two regular 52-card decks, thus has 104 cards. Both stacks are shuffled well. You choose two cards in sequence and you win if they are both red. Would you prefer to choose from the 52-card stack or the 104-card stack?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou want to choose from the larger stack. The probability to draw two red cards in sequence from a stack of \\(n\\) cards (with \\(n/2\\) red ones) is \\[\n\\frac{n/2}{n} \\times \\frac{n/2-1}{n-1}\n\\] For the first draw the probabilities are identical: \\(26/52\\) and \\(52/104\\). But for the second draw the probabilities are \\[\n\\frac{51}{103}=0.495 &gt; \\frac{25}{51}=0.49\n\\]\nThere is a slightly higher chance to draw two red cards from the larger stack.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#rapid-fire",
    "href": "intuition.html#rapid-fire",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.17 Rapid Fire",
    "text": "7.17 Rapid Fire\nCowboy Billy carries a Colt single action 6 shooter revolver. When he fires all 6 shots in a row, the time between the first bullet and the last is 60 seconds. How long would it take him to fire 3 shots?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt will take him 24 seconds to fire three shots. Wait, what?\nThe relevant pattern here is about the time elapsed between shots. If the shots are fired at regular intervals, then Billy will take 12 seconds between the six shots. 12 seconds after the first shot he fires the second bullet, 12 seconds after that he fires the third bullet.\nAnother way of thinking about this is the distance at which fence posts are placed. In a fence with six posts, the first one is at 0/5th total distance, the second post is located 1/5th of the total distance, and so on.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "intuition.html#crossing-the-river",
    "href": "intuition.html#crossing-the-river",
    "title": "7  Quantitative Intuition and Problem Solving",
    "section": "7.18 Crossing the River",
    "text": "7.18 Crossing the River\nA farmer is on his way back from the market, with him he has a fox, a chicken and some grain. To get home he needs to cross a river using a small boat that can accommodate only him and one of the other items. Unfortunately, if the fox is left alone with the chicken it will eat it. If the chicken is left alone with the grain, it will eat it. How can the farmer cross the river and bring home the fox, the chicken, and the grain?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis will take several trips across the river:\n\nHe takes the chicken across the river.\nHe returns in an empty boat and picks up the fox.\nHe takes the fox across the river and picks up the chicken.\nHe returns with the chicken in the boat and deposits it while picking up the grain.\nHe takes the grain across the river. Now he has the chicken on the near side of the river and the fox and the grain on the far side.\nHe returns in an empty boat and picks up the chicken.\nHe takes the chicken across the river, now all three items have crossed.\n\nThe trick is to take one item—here, the chicken—back and forth to make sure it is not alone with the item it would destroy.\n\n\n\n\n\n\nFigure 7.11: Remaining tire life after 7,500, 15,000, 22,50, and 30,000 miles., miles",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quantitative Intuition and Problem Solving</span>"
    ]
  },
  {
    "objectID": "predictions.html",
    "href": "predictions.html",
    "title": "8  Making Predictions",
    "section": "",
    "text": "8.1 Introduction\nIn The Signal and the Noise, Silver (2012, 52) cites a study at the University of Pennsylvania that found when political scientists claim that a political outcome had no chance of occurring, it happened about 15% of the time. And of the absolutely sure things they proclaimed, 25% failed to occur. Now we know that predictions are themselves uncertain, that is why polling results have a margin of error. But when you predict that something has a zero chance of happening, then you ought to be pretty confident in that, the margin of error should be small, definitely not more than 15%.\nThis is an example of a prediction that is not very good.\nIf a plane had a 29% of crashing, you would consider the risk of flying too high and stay on the ground. In the run-up to the 2016 presidential election, FiveThirtyEight predicted a 71% chance for Clinton to win the Electoral College and a 29% chance for Trump to win. This was a much higher chance of a Trump victory than the 1% to 15% chance many other models produced. As it turned out, the FiveThirtyEight model was much better than the models that treated the Clinton victory as a near certainty.\nSilver (2012) points out\nSometimes we predict things more accurately (small margin of error in the prediction) but we do not believe it or act on it. Predictions that contradict our intuition or preferred narrative can be ignored or explained away. Our personal judgment is not as good as you might think. We tend to overvalue our own opinion and this trend increases the more we know. 80% of doctors believe they are in the top 20% of their profession. More than half of them are clearly wrong.\nSo when a prediction does not come true, does the fault lie with the model of the world or the world itself? If there is a 80% chance of rain tomorrow, then you might see sunny skies. If, in fact, the long run ratio of days that have sunny skies when the forecast calls for an 80% chance of rain is 1 in 5, then the forecast model is correct. Compare the scenario to the following:\nIn the build-up of the the 2008 financial crisis, Standard & Poor gave CDOs, a type of mortgage-backed securities, a stellar AAA credit rating, meaning that there is only a 0.12 probability that they would fail to pay out (Silver 2012, 20). In reality, of the AAA-rated CDOS, 28% defaulted. Had the world of financial markets drastically changed to bring about such a massive change in default rates (200x!)? Or is it more likely, that the default models of the rating agencies were wrong? It was the latter.\nWe predict all the time. On the drive to work we choose this route over that route because we predict it has less traffic, fewer red lights, or we are less likely to get caught behind a school bus. We probably make this choice instinctively, without much deliberation, based on experience, instantaneously processing information about the time of day, weather, etc.\nYou might choose Netflix over Paramount+ one evening because you think it is more likely that you’ll find content that interests you. This is also a prediction problem. A company offers a customer a discount because it predicts that without an incentive the customer might leave for a competitor. Information about the weather consists of a status report of current conditions and a forecast.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Predictions</span>"
    ]
  },
  {
    "objectID": "predictions.html#sec-predict-intro",
    "href": "predictions.html#sec-predict-intro",
    "title": "8  Making Predictions",
    "section": "",
    "text": "“Most people fail to recognize how much easier it is to understand and event after the fact when you have all the evidence at your disposal. [] But making better first guesses under conditions of uncertainty is an entirely different enterprise than second-guessing.”\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrediction and Forecast\n\n\n\nTechnically, a prediction and a forecast are different things. In statistics, a prediction results from the application of a model to data. If the data falls outside of the range of observed training data, then it is referred to as a forecast, in particular when the prediction is about a future event.\nForecasting is also referred to as planning in the presence of uncertainty, taking a systematic, methodological approach. Predicting, on the other hand is any proclamation about things we do not know yet. We predict the outcome of a football game based on gut feeling or allegiance to a team, but we forecast the weather based on meteorological models.\nIn seismology, the distinction between prediction and forecast is taken very seriously. A prediction of an earthquake is a specific statement about when and where it will strike. A forecast, on the other hand is a statement of probability: there is a 60% chance of an earthquake in Northern Italy over the next fifty years. Leaning on this distinction, the U.S. Geological Service party line is that earthquakes cannot be predicted.\nFor the purpose of our discussion here, predicting and forecasting are interchangeable.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Predictions</span>"
    ]
  },
  {
    "objectID": "predictions.html#supervised-and-unsupervised-learning",
    "href": "predictions.html#supervised-and-unsupervised-learning",
    "title": "8  Making Predictions",
    "section": "8.2 Supervised and Unsupervised Learning",
    "text": "8.2 Supervised and Unsupervised Learning\nMaking predictions is probably the fundamental task of algorithms that process data by training some form of model. Statistical learning techniques can be categorized in broad strokes into supervised and unsupervised methods. The name stems from the concept of students learning in the presence of a teacher. In supervised learning the teacher knows the correct answer to problems, and can measure the discrepancy between truth and a student’s answer. In unsupervised learning there is no teacher to guide the quality of the answer. Instead, the student is trying to find patterns and associations in the data.\nThe two primary tasks in supervised learning are predictions and classifications. Classification problems involve assigning an observation to one of a set of possible outcome categories, for example, whether an animal is healthy or diseased, or choosing a word from a dictionary, or which one of the digits 0–9 a handwriting sample represents. Prediction in the sense of supervised learning is making a statement about the average of an attribute, for example, the monthly revenue or the weight of an animal.\nWe go on this detour of supervised/unsupervised learning and of prediction/classification to uncover that the process of prediction is fundamental to all these applications. Consider you are presented with a handwritten digit and you have trained a classification model in digit detection. The algorithm does not actually say which of the ten digits it was presented with. The algorithm will compute a vector of 10 quantities each between 0 and 1, and subject to the constraint that they sum to 1.\nHere is the vector for one observation \\[\n[0.00000, 0.00001, 0.00001, 0.00003, 0.00000, 0.00000, 0.00000, 0.99994, 0.00000, 0.00001]\n\\]\nand here is the vector for another observation\n\\[\n[0.00056, 0.00002, 0.00000, 0.00001, 0.00424, 0.74695, 0.18760, 0.00001, 0.00616, 0.05446]\n\\]\nThe first element of the vector corresponds to digit 0, the second to digit 1, and so on. Considering the first vector, how would you convert the numbers into a classification? Since the numbers are between 0 and 1, and sum to 1, it is tempting to interpret them as probabilities. The overwhelming evidence points at the 8th position in the vector with a large probability of 0.99994. The algorithm is almost certain that the digit is a “7”.\nIn the second case we see more of a spread in the probabilities. Digit “5” is considered most likely (probability 0.74695), but other digits also have a non-zero probability (“6” and “9”). Given the large probability for “6” we would probably classify the digit as a six.\n\n\n\n\n\n\nFigure 8.1: Digit classifications.\n\n\n\nFigure 8.1 shows the actual and classified digits for nine data points. The vectors shown above correspond to the probabilities for the images in the upper left and lower right corners. We are not surprised that the algorithm classified the first observation as a “7”. We are also not surprised that the sloppy “5” in the lower right corner could be mistaken for a “6” (which had the second largest probability).\nThe point of this example is twofold:\n\nThe process of classifying something through a statistical algorithm often goes first through a process of predicting a measure of confidence or likelihood associated with the possible outcomes. We then classify the observation into the category that has the highest probability. This is known as the Bayes Rule of classification and it can be shown to be optimal in the sense of achieving the greatest accuracy.\nWhen making predictions we need to think in terms of probabilities and uncertainties. Ultimately, the classification model will spit out one category and we interpret this as “the model says the digit is a seven”. In the case of the first observation, there is not much uncertainty with the prediction, as the model deems other choices very unlikely. In the second case we should appreciate that the algorithm decided on a “5”, but also that there is uncertainty in the prediction. We only know that the algorithm got it right because we know the author of the digit was writing a “5” (the label). As Silver (2012) puts it,\n\n\nOur brains, wired to detect patterns, are always looking for a signal, when instead we should appreciate how noisy the data is.\n\nA common task in unsupervised learning is finding patterns in the data that allow us to group the observations. The algorithm finds that observations are somehow similar and dissimilar based on their attributes. This process is called a cluster analysis and the result are a certain number of clusters formed from the training data. If a new observation comes along, we can assign it to one of the clusters by applying the clustering model. We predict which of the clusters the observation is closest to in the sense of the model. Again, prediction is the fundamental task by which we draw conclusions about new observations.\n\nPredictions are everywhere. We make them, consciously or subconsciously all the time. The human brain is a highly efficient pattern matching machine and we constantly make predictions about the world based on the patterns we receive. Predictions are at the heart of data processing, whether it is for the purpose of forecasting, classifying, or clustering (grouping).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Predictions</span>"
    ]
  },
  {
    "objectID": "predictions.html#bad-predictions",
    "href": "predictions.html#bad-predictions",
    "title": "8  Making Predictions",
    "section": "8.3 Bad Predictions",
    "text": "8.3 Bad Predictions\nSilver (2012, 20) summarizes the attributes bad predictions have in common:\n\nFocus on the signals that tell a story abou the world as we would like it to be, not how it really is.\nIgnore the risks that are most difficult to measure, although they pose the greatest risk to our well-being.\nMake approximations and assumptions that are much cruder than we realize.\nDislike (abhor) uncertainty, even if it is an integral part of the problem.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Predictions</span>"
    ]
  },
  {
    "objectID": "predictions.html#predicting-versus-measuring",
    "href": "predictions.html#predicting-versus-measuring",
    "title": "8  Making Predictions",
    "section": "8.4 Predicting versus Measuring",
    "text": "8.4 Predicting versus Measuring\n\nDestructive tests\nExpensive tests\nFuture performance",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Predictions</span>"
    ]
  },
  {
    "objectID": "predictions.html#things-that-are-notoriously-difficult-to-predict",
    "href": "predictions.html#things-that-are-notoriously-difficult-to-predict",
    "title": "8  Making Predictions",
    "section": "8.5 Things that are Notoriously Difficult to Predict",
    "text": "8.5 Things that are Notoriously Difficult to Predict\n\nNoisy Systems, Sparse Data\n\nLow signal-to-noise ratio\n\n\n\nChaotic Systems\n\nWeather\nEconomy\n\n\n\nComplex Systems\nComplex systems are those governed by the interaction of many separate individual parts. They can seem at once very predictable and very unpredictable. The laws governing earth quakes are well understood and the long-term frequency of a magnitude 6.5 earthquake in Los Angeles can be estimated well. But we are not very good at predicting earthquake activity.\nComplex systems periodically undergo violent and highly nonlinear phase changes from orderly to chaotic and back again. Bubbles in the economy and significant weather events such as hurricanes, tornadoes, or tsunamis are examples.\n\nEarthquakes\n\n\n\nNonlinear or Exponential Growth\n\nInfectious diseases\n\n\n\nSystems with Feedback Loops\nThe act of predicting can change the system being predicted. Economic predictions can change the way people have and that can affect the outcome of the prediction itself. Those changes can make the prediction more accurate or less accurate.\nSelf-fulfilling predictions, where the prediction reinforces the outcome, are common in political polling. A poll showing a candidate surging can cause voters to switch to the candidate from ideologically similar candidates. Or it can make undecided voters to finally get off the fence.\nAnother example of a self-fulfilling prediction is when increased media coverage of a medical condition leads to increased diagnosis of the condition. Not just because the condition is more prevalent, but because of increased attention people are more likely to identify symptoms and are doctors are more likely to diagnose them. The rise of autism diagnoses in the U.S. from 1992 to 2008 correlates highly with the media coverage of autism (Silver 2012, 218).\nWhen the police increase presence in an area where crime rate is believed to be high, they will report more incidences because of their presence and that will in turn increase the crime rate.\nA self-cancelling prediction works the opposite way, it undermines itself. When GPS systems became more commonplace, drivers were guided to routes which the systems thought had less traffic. If the systems cannot adjust in real time to the actual traffic density, the guidance can result in more traffic on the suggested routes.\n\n\n\nFigure 8.1: Digit classifications.\n\n\n\nSilver, Nate. 2012. The Signal and the Noise: Why so Many Predictions Fail–but Some Don’t. Penguin Books.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Making Predictions</span>"
    ]
  },
  {
    "objectID": "life_algorithms.html",
    "href": "life_algorithms.html",
    "title": "9  Algorithms to Live By",
    "section": "",
    "text": "9.1 Introduction\nIn the introduction we defined an algorithm as a set of repeatable step-by-step instructions to solve a particular problem. Algorithms are not necessarily the machinations of computer scientists and mathematicians—recall making pumpkin soup. We design and apply algorithms in every day life. You probably have a personal algorithm or two for brushing teeth or making the bed.\nAlgorithms are designed to perform a task in some optimal way. My dog applies the above algorithm to achieve the optimal sleeping arrangement—by his standard. Mathematicians and computer scientists have developed many algorithms to solve certain problems optimally. If you have to put five books on a shelf in order, you might not worry much about the sorting algorithm. When you have to sort 100,000 items choosing between a bubble, quick, or merge sort matters much more.\nCan we apply what mathematicians and computer scientists know about optimal algorithms to solve problems in everyday life? The discussion that follows draws on the book Algorithms to Live By by Christian and Griffiths (2017).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Algorithms to Live By</span>"
    ]
  },
  {
    "objectID": "life_algorithms.html#introduction",
    "href": "life_algorithms.html#introduction",
    "title": "9  Algorithms to Live By",
    "section": "",
    "text": "Getting into bed: a dog’s perspective\n\n\n\n\n\nMy dog has developed an elaborate algorithm for nights when he wants to sleep in the bed. He starts out sleeping in his dog bed on the floor and the algorithm goes something like this:\n\nGet up and pretend there is something outside that needs his attention: a squirrel in a tree, a raccoon on the porch, a deer in the yard, etc.\nMake sure that I understand the severity of the threat and follow him to the living room. This can involve growling at the door to the deck, running back and forth between living room and bed room.\nI finally get up, follow him into the living room and open the door to the deck.\nAt this point he pretends to realize that there is no threat after all and we can go back to bed.\nHe follows me into the bedroom, positioning himself next to the bed in the unmistakable posture that says “lift me up on the bed please.”\n\nTo his credit, he could jump on the bed by himself and is going through this routine as a means to get permission. The algorithm is complicated, but it is a repeatable step-by-step recipe, and it works every time.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Algorithms to Live By</span>"
    ]
  },
  {
    "objectID": "life_algorithms.html#optimal-stoppingthe-37-rule",
    "href": "life_algorithms.html#optimal-stoppingthe-37-rule",
    "title": "9  Algorithms to Live By",
    "section": "9.2 Optimal Stopping–The 37% Rule",
    "text": "9.2 Optimal Stopping–The 37% Rule\nSuppose you just landed a job at a Silicon-Valley startup company and are locating to San Francisco. Finding an apartment in San Francisco can be a rough experience: there is a lot of demand for a short supply. As a newcomer to town you do not have a good feeling for the market. What do typical apartments in your price range look like? What is available on the market?\nHere is the dilemma: you cannot procrastinate much when you see an available apartment; they go very fast. But if you choose the first available apartment you will miss out on better ones that you have not seen yet. If you pass on the first available apartment there is no guarantee that the next ones will be any better. But you need to look at least at a few of them to get a feeling for what is on the market. How else could you make an informed decision? But if you keep looking at apartments and pass on them you might end up with no apartment at all or you have to choose from what is left on the market. Is looking at 2 apartments enough? 20?\nThere are two ways in which you can fail: stop looking at apartments too early and stop looking too late. When you stop too early a better apartment goes undiscovered. If you stop too late you hold out for a better apartment that does not exist, you have passed on the best possible apartment already.\nThis is known as an optimal stopping problem. How much effort should you spend on looking at choices and when should you leap and make a decision?\nIt turns out that there is an answer. The optimal decision rule is to spend 37% of your budget on calibrating and looking at apartments to establish a standard. Then pick the next apartment that beats the standard. If your budget is one month to find an apartment, then you should look and calibrate for 11 days.\nWe can thank computer science for having an optimal solution to balance overthinking and impulse decision. No more analysis paralysis if you apply the 37% rule. You can see how the algorithm could be applied to other situations in real life:\n\nHow much time should you spend circling a parking lot before committing to a spot in the hope that a better spot opens up?\nHow many offers should you reject before selling the car or house in the hope to get a better offer?\nHow long should you be dating before finding the “optimal” partner?\n\nThe 37% rule is not optimal in all decision problems. Situations where the rule applies are characterized by the following:\n\nYou have to choose a singleton (one apartment, one parking spot, one partner).\nThere are \\(n\\) possible choices and \\(n\\) is known.\nIt is possible to rank all \\(n\\) choices from best to worst.\nThe choices appear sequentially in a random order.\nAfter meeting a choice, a decision is made immediately to reject or accept it.\nA choice cannot be revisited after rejection or acceptance.\nIf you come to the final (\\(n\\)th) choice you have to take it.\n\nThe optimum achieved by the 37% rule is to maximize the probability to select the best choice among the \\(n\\) possibilities. The rule does not guarantee that the best choice is made. But no other rule has a higher probability of finding the best choice under the circumstances.\n\nThe Secretary Problem\nThe optimal stopping problem is also known as the secretary problem, the sultan’s dowry problem, the fussy suitor problem or the marriage problem. In terms of hiring a secretary, it goes as follows.\nYou want to hire the best secretary out of \\(n\\) applicants for a position. Applicants are interviewed in a random order one by one. After the interview you decide whether to accept or reject the candidate. If you reject an applicant they take another job and cannot be reconsidered. While interviewing the applicant you gather information that allows you to rank the candidate against those interviewed so far but you do not know the quality of the yet to-be-interviewed candidates.\nThe 37% rule states that you should be evaluating the first 37% applicants to create a ranking. That completes the looking phase. Then leap and make an offer to the next applicant that ranks higher than the best applicant interviewed during the looking phase.\nBut wait, what if there is only one applicant? Or two?\nWith a single applicant you need to hire them, and you are guaranteed to have hired the best of the (single) bunch. With two applicants, if you pass on the first, you have to hire the second. In this case you can flip a coin, either hire the first or pass and be forced to hire the second—you cannot do better than chance. With \\(n=3\\) candidates the optimal rule is to look at the first candidate, then choose the second candidate if they beat the first. This is a 1/3 x 100% = 33.3% rule and has a 50% probability of finding the best candidate.\nAs \\(n\\) increases, the optimal rule approaches \\(1/e \\times 100\\% = 36.8\\%\\) and that is also the probability of finding the best candidate. Round that percentage up and you see why it is called the 37% rule.\nYou might say that a 37% chance of making the best choice is not very good. In 63% of all cases we are not finding the best candidate. The result is not that bad if you compare it against a pure random chance. If there are 100 candidates for the position, then a random choice has a 1% chance of getting the best candidate. In a pool of 1,000 candidates, random selection has a 0.1% chance of locating the best. However, the odds of finding the best candidate under the optimal stopping rule does not change. The more choices you have, the better optimal stopping performs relative to random selection. But even with only three candidates the likelihood of finding the best candidate has improved from 1 out of 3 to 1 out of 2.\n\n\nIssues with the Optimal Stopping Rule\nBut wait, you say. Pure random selection is not the appropriate benchmark against which to compare the optimal stopping rule. This is not how we hire people in the real world. In other words, the situation where the 37% rule applies does not describe how we do things.\nWhat are some of the ways in which the optimal stopping setup is unrealistic:\n\nWe often do not know \\(n\\), the number of choices. For example, we might not know how many apartments in San Francisco are on the market. There is a workaround: if \\(n\\) is unknown, we can base the 37% rule on a time interval: we give ourselves one month to find an apartment and enter the leap phase after 10 or 11 days.\nThe choices do not come to us in a random order. In an interview process there are short lists and candidates are screened and filtered. The order in which candidates come to the interview is not completely at random.\nAfter rejecting a candidate, we do have the option to go back. If it turns out that none of the other candidates was better than a candidate rejected earlier, then we can go back and reconsider the choice. After rejecting a date, we can ask them on another date—theoretically.\nIn real life, we are not making decisions after seeing a candidate. Instead, applicants are grouped (pre-screening, initial phone interview, on-site interview, …) and choices are made after each batch of candidates.\nThe optimal stopping problem assumes that we do not have a ranking of the candidates. It only assumes that there is an objective method of ranking them and that we rank as we go. Obviously, candidates are evaluated based on many criteria and these are known ahead of time. The look phase of the optimal stopping exists because it is needed to calibrate what good candidates look like. We know this a priori of time, and we can evaluate candidates (somewhat) based on their resumes.\nWe do not only have a sense of which candidate ranks higher or lower, but also by how much. Meeting a candidate that really stands out changes the likelihood of making a decision. If you know that a candidate is in the top 10% of SAT or GRE scores, there is only a 1 in 10 chance that others will be better.\nThe job will be offered to a candidate in the leap phase, not the looking phase. The Human Resources and the Legal departments are probably going to have a conniption if the first 37% of candidates being interviewed have no chance of getting the job. Queue the lawsuits. Your dates might not respond kindly when they find out that you are “just looking”.\n\n\nThe optimal stopping scenario and the 37% rule are useful to construct a mental model of how to go about a decision that involves sequentially evaluating choices and deciding on a particular choice. As with any model, we have to examine the assumptions and conditions under which the model applies. We quickly see that the assumptions do not map cleanly to real-life situations such as hiring, finding a partner, or even circling a parking lot for the best empty space.\nAs G.E.P. Box said\n\nAll models are wrong. Some are useful.\n\nThe utility of casting real-life decision situations in terms of an optimal stopping problem lies in understanding that under special circumstances there is an optimal way to balance impulse and procrastination and to realize in what ways our decision situation deviates from that scenario. Are we in a no-information scenario as in the classical secretary problem where we have no a-priori data about the quality of the applicants or are we in a full-information scenario where the quality of all applicants can be quantified a priori? If it is the latter we should come to a decision more quickly than going through at least 37% of the stack.\nA situation where the algorithm applies cleanly to a real-life situation is when we are placing things in order—the task of sorting. The question then is what is the most efficient way to go about it?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Algorithms to Live By</span>"
    ]
  },
  {
    "objectID": "life_algorithms.html#searching-and-sortinga-tradeoff",
    "href": "life_algorithms.html#searching-and-sortinga-tradeoff",
    "title": "9  Algorithms to Live By",
    "section": "9.3 Searching and Sorting—A Tradeoff",
    "text": "9.3 Searching and Sorting—A Tradeoff\nSearching means finding things in a collection. Sorting means arranging a collection in order. Both are fundamental tasks in data processing. Sorting was one of the first applications for computers, making sense of the 1890 census. Tallying the previous census by hand took 8 years, just enough to finish the task before the next census (Christian and Griffiths 2017). With growing number of questions asked on every census, something needed to be done, the 1890 census would not be tallied by hand in time for the 1900 census. Enter the Tabulator, a punchcard-based machine invented by Herman Hollerith and adapted for the 1890 census (Figure 9.1).\n\n\n\n\n\n\nFigure 9.1: Tabulator with sorting machine invented by Herman Hollerith. Source: Wikipedia\n\n\n\nSorting was one of the most important tasks for computers. Christian and Griffiths (2017) state\n\nBy the 1960s, one study estimated that more than a quarter of the computing resources of the world were being spent on sorting.\n\nand it is still at the heart of many algorithms. Every time you see a ranking, a largest or smallest value, a 95th percentile, a leader board or a top-10, a list had to be arranged by value, sorted at least partially.\nAs an exercise, can you think of some ways in which you encounter sorted things every day? Here is a start:\n\nSports teams are ranked by criteria such as number of wins and losses, and displayed from highest to lowest.\nMusic is ranked by popularity, measured as number of downloads, sales, etc.\nMusic is grouped (sorted) by genre.\nStreaming services recommend things to watch according to an algorithm that ranks shows based on your watch history.\nDisplays of countries are sorted by all kinds of attributes. The medal count in the Olympic Games, gross domestic product (GDP), annual oil production, emissions, etc.\nA Google search returns the search results sorted according to relevance. The relevance is determined through a number of factors such as Google’s famed PageRank algorithm, which sites are sponsored, and so forth. Without sorting the search results you would have to sift through millions of links to determine which ones are most relevant to your search. The ranking is very effective, less than 1% of Google searches scroll to the second page.\nThe email Inbox is sorted by some criteria, read/unread, message thread, time of arrival, etc.\nOn computers, files and folders can be arranged by attributes such as size, time of creation, time of modification, name, etc.\nThe U.S. Postal Service sorts mail by zip code.\nSocial media sites create personalized feeds based on sorting contributions according to their algorithms.\nSearching for items in an online store you can arrange the results by brand, price, popularity, etc.\nBooks in the library are organized into buckets by call numbers and are sorted alphabetically within buckets.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Algorithms to Live By</span>"
    ]
  },
  {
    "objectID": "life_algorithms.html#asymmetric-cryptography",
    "href": "life_algorithms.html#asymmetric-cryptography",
    "title": "9  Algorithms to Live By",
    "section": "9.4 Asymmetric Cryptography",
    "text": "9.4 Asymmetric Cryptography\n\nConsider the following scenario.\nBob and Alice are pen pals and are sending letters to each other by snail mail. At one point, Bob wants to send Alice a package that contains a valuable item. Unfortunately, thieves have been breaking open packages and stolen the contents. To prevent that from happening, Bob places the item inside a box and places a lock on it. The problem now is, how can Alice open the lock without having the key to it? One option is for Bob to communicate the lock combination by some other means to Alice. If it is a keyed lock, he could send the key to her in another mailing. In any event, he does not want to include the key in the package itself, that would be pointless. And sending the combination or key separately still comes with risks. The information could be intercepted and when a thief gets their hands on the package with the valuable item they could steal it.\nHow can Bob send the item safely to Alice and she is guaranteed to receive it without exchanging information about the lock itself?\nThe solution lies in asymmetric cryptography, also called asymmetric encryption. Prior to the invention of asymmetric encryption secrets were sent around that enable recipients to decrypt messages. The obvious problem of symmetric encryption is that if someone gets hold of message and decryption key, they can decipher the message. Everything hinges on keeping the keys safe.\nHow could Bob and Alice handle the situation without exchanging secrets (keys) about the box? It involves two keys. When Bob sends the package to Alice, he attaches his lock to the box. Upon receipt, Alice attaches her lock as well and sends the package back to Bob. He recognizes the package came from Alice and removes his lock, then sends it back to Alice.\nAlice now receives the box that contains the valuable item protected with her own lock. She uses her own key to open it and retrieves the item.\nOnly the owners of the keys used it to lock and unlock the box. The keys were never exchanged. This came at the expense of sending the package back and forth a few times. This could be optimized in the following way: suppose we use a special (single) lock with two keys: a key anyone can use to lock it, but a key unique to Alice to unlock it. Alice shares the first key with anyone who sends her packages and keeps the key to unlock a secret (shares it with no one). When Bob sends a package to Alice, he locks the box with her public key. When he sends a package to Fred, he secures it with his public key.\nThis is essentially how most messages traveling over the internet are secured, based on a pair of related keys.\n\n\n\nFigure 9.1: Tabulator with sorting machine invented by Herman Hollerith. Source: Wikipedia\n\n\n\nChristian, Brian, and Tom Griffiths. 2017. Algorithms to Live by. The Computer Science of Human Decisions. Henry Holt; Company, New York.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Algorithms to Live By</span>"
    ]
  },
  {
    "objectID": "genAI.html",
    "href": "genAI.html",
    "title": "10  Generative AI",
    "section": "",
    "text": "10.1 A Brief History of AI\nArtificial Intelligence (AI) is in every conversation. Everywhere you turn you hear “AI this”, “AI that”, and (too) many people are now claiming to be AI experts. This has not always been the case. Some decades ago, when you’d tell anyone you work in artificial intelligence they would not take you serious. If you’d try to get a research grant you’d better not mention the term artificial intelligence. This was the time of one of the the AI winters that followed a period of overblown expectations and exuberant predictions what machines would be capable of doing.\nPeriods of AI hype (AI summers) and AI disappointment (AI winters) are cyclic. The peak of inflated expectations in a hype cycle is followed by the trough of disillusionment. Industry analyst firm Gartner makes a living from this cycle. Since the arrival of capable foundation models and GPT-based large language models in late 2022 we find ourselves in another AI summer. And once more you hear voices claiming machines have/are becoming sentient, that all our jobs are on the line, and that the era of artificial general intelligence, when machines can think for themselves, is just around the corner. We heard the same hype in the 2010s when deep learning-based neural networks bested us at image recognition and at playing games such as Go.\nDo we know more about AI now then we did back then? Is the hype now more justified? Are we better at predicting the future of technology now than we were back then?\nArtificial intelligence (AI) refers to building systems that can perform tasks or make decisions that a human can make. The systems can be built on multiple technologies, mechanization, robotics, software engineering, among them. Many AI systems today are entirely software based, large language models (ChatGPT, Gemini, Claude Sonnet) for text processing or diffusion-based systems for image generation are examples.\nFigure 10.1 shows an example of a mechanized AI system, called the Mechanical Turk. It was an automaton, a device made to imitate an human action. The action in this case was to play chess.\nYou can imagine that building a purely analog machine that plays chess is difficult. To accomplish this in the 18th century is really remarkable. Well, it turned out to be impossible. The Mechanical Turk was a hoax. The cabinet concealed an human player who operated the chess pieces from below the board.\nPerforming human tasks by non-human means is as old as humanity. Goals are increased productivity through automation, greater efficiency and strength, elimination of mundane, boring, or risky tasks, increasing safety, etc.\nThe two major AI winters occurred during the periods 1974–1980 and 1987–2000. One was triggered by disappointment with progress in natural language processing, in particular machine translation. The other was triggered by disappointment with expert systems.\nDuring the Cold War the government was interested in the automatic, instant translation of documents from Russian to English. Neural network architectures were proposed to solve the task. Today, neural network-based algorithms can perform language translation very well, it is just one of the many text analytics tasks that modern AI is good at. In the 1950s and 1960s progress was hindered by a number of factors:\nThe expectations for the effort were sky high, however. Computers were described as “bilingual” and predictions were made that within the next 20 years essentially all human tasks could be done by machines. These expectations could not be met.\nAn expert system is a computerized system that solves a problem by reasoning through a body of knowledge (called the knowledge base), akin to an expert who uses their insight to find answers based on their expertise. Expert systems were an attempt to create software that “thinks” like a human. The problem is that computers are excellent at processing logic, but not at reasoning. The reasoning system of these expert systems, called the inference engine, consisted mainly of rules and conditional (if-else) logic. We are still using expert systems today, but a very special kind, those that can operate with captured logic rather than asking them to reason. Tax software is an example of such an handcrafted knowledge system—a very successful one at that. Most taxpayers would not think twice to use software such as TurboTax or TaxSlayer to prepare their income tax return.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Generative AI</span>"
    ]
  },
  {
    "objectID": "genAI.html#a-brief-history-of-ai",
    "href": "genAI.html#a-brief-history-of-ai",
    "title": "10  Generative AI",
    "section": "",
    "text": "Figure 10.1: Mechanical Turk, a chess-playing automaton in the 18th century.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCynics might say that 2 1/2 centuries removed, we are still operating by a similar principle. When you ask a large language model to write a poem in the style of Edgar Allan Poe, there is no real poet behind the curtain crafting words. However, there are digital poets behind the curtain, represented by the volumes of literature used to train the language model so that it can respond in the requested style.\n\n\n\n\n\n\nlack of computing power to build large, in particular, deep networks\nlack of large data sets to train the networks well\nneural networks specifically designed for text data had not been developed\n\n\n\n\nEasy for Computers, Easy for Humans\nTax software works well and is a successful AI effort because it performs a task that is easy for computers that is intellectually difficult for humans. The tax code is essentially a big decision tree with many branches and conditions. Such a structure is easily converted into machine instructions. There is no reasoning involved, we just have to make sure to get all the inputs and conditionals right. If the adjusted gross income is $X, and the deductions are $Y, and the payer is filing jointly with their spouse, …, then the tax is this amount. Figuring out the correct tax is trivial based on the program. On the other hand it is impossible for us to memorize the entire logic and execute it without errors.\nAn expert system that performs logic reasoning is the exact opposite: it performs a task that is easy for us but very difficult to perform for a computer. Imagine to create an expert system that can operate a car by converting how a human operator drives a car into machine instructions. We instantly recognize an object in the road as a deer and plan an evasive action. A machine would need to be taught how to recognize a deer in the first place. It would have to be taught to choose an action when a deer appears in the road, or when a deer is in one lane of traffic and an oncoming car is in the other lane.\nHumans excel solving problems that require a large amount of context and knowledge about the world. We look at a photo or glance out the window and instantly see what is happening. We choose between hitting the deer, hitting the other car, and running off the road almost immediately, intuitively. Our value system and humanity drive the decision. Seeing, sensing, speaking, operating machinery are such problems. Unlike the tax code, they are very difficult to describe formally.\nThis changed—to some degree—in the mid 2000s. Computers were suddenly getting much better at these hard to formalize tasks such as sensing the world. You could call this period the AI spring before the ChatGPT AI summer we are in now. Our ability to solve problems that require knowledge about the world increased by orders of magnitude. The key was a new discipline, deep learning, which turned out to be a renaissance of decade-old ideas.\n\n\nNeural Networks–Again\nImagine writing computer software to recognize objects on images, for example facial recognition software. Explicitly programmed software had been around and was doing an OK’ish job at that. Algorithms were specifically designed to discover edges such as the outline of the face, identify eyes and noses and so on. We call them explicitly programmed algorithms because software developers created the algorithms that took an image as input and processed the pixels to discover faces.\nIn an implicit program, on the other hand, the software developer does not need to handle all aspects of the program. Many programming languages have implicit features. For example, a language can infer the data type of a variable without it being explicitly declared.\nAn extreme form of implicit programming is when the algorithm is generated as the result of other instructions. That is the case with deep neural networks trained on large volumes of data.\nA neural network is essentially an algorithm to predict or classify an input. The input could be a photo, the output of the algorithm are bounding boxes around the objects it classified on the photo, along with their labels and a photo caption. Neural networks are made up of many nonlinear functions and lots of parameters, quantities that are unknown and whose value is determined by training the network on data. Networks with tens of thousands or millions of parameters are not unusual. The layers of a neural network are related to levels of abstraction of the input data. Each layer processes a different aspect of the structural information in the input data. Whereas an explicit programmer knows when they write code that detects edges and which step of the program is locating the eyes of a face, what structure a particular layer of a neural network is abstracting is not known.\nFigure 10.2 shows a schema of the popular AlexNet network for image processing. It is a special kind of neural network, called a convolutional neural network, and won the ImageNet competition in 2012, classifying objects into 1,000 categories with a smaller error rate than a human interpreter.\n\n\n\n\n\n\nFigure 10.2: AlexNet, a convolutional neural network (Mallick and Nayak 2018).\n\n\n\nThe various layers of AlexNet tell us what their role is, some layers convolve the results of previous layers, others aggregate by pooling neighboring information, yet others flatten the structure, and so on. But they do not tell us exactly how the information in an input pixel is transformed as the data flows through the network. Nearly 63 million parameters act on the data as it is processed by the network; it is a big black box. Yet once the 63 million parameters are determined based on many training images, the neural network has turned into an algorithm with which new images can be processed. The process of training the network on labeled data, that is, images where the objects were identified to the network, implicitly programmed the classification algorithm.\nThis process of training deep neural networks on large data sets, made possible by the availability of large data and compute resources, overcame the limitations that held back neural networks previously. Implicitly programmed prediction and scoring algorithms were handily beating the best algorithms humans had been able to write explicitly. In the area of game play, deep learning algorithms implicitly programmed based on reinforcement learning were beating the grand masters and the best traditional computer algorithms.\n\n\n\n\n\n\nNote\n\n\n\nStockfish, one of the most powerful chess engines in the world is an open-source software project that has been developed since 2008. Many view it as the best chess engine humans have been able to build.\nIn 2017, Google’s DeepMind released AlphaZero, a system trained using reinforcement learning, a machine learning technique in which an agent (player) optimizes decisions in an environment (moves in a game) by maximizing the sum of future scores (rewards). Earlier, a Go system that was trained against millions of recorded expert-level games beat the best human Go player handily. What made AlphaZero special is that it was trained entirely by self-play, it improved by playing against itself.\nAfter only 24 hours of training, this data-driven system, crushed Stockfish, the best chess engine humans have been able to build.\n\n\nWhen decades-old neural network technology met up with Big Data and massive computing resources, capabilities made a huge leap forward in areas such as natural language understanding, image processing, autonomous driving, etc. The resulting hype was predictable: AI is coming for our jobs, the machines are out to get us, yada yada yada. Since deep learning algorithms could read images, it was predicted that radiologists would be replaced within a few years by machines. Yet not one radiologist has lost their job because of AI. Not one New York City cab driver has lost their job to a robo cab. Instead, they lost jobs to Uber and Lyft. Autonomous driving is still not fully possible. The ability to translate language based on recurrent neural networks and its cousins remained limited to relatively short sequences.\nWith the rise of deep learning neural networks and implicit programming algorithms pushed deep into domains we felt were uniquely human. The change feels more personal when machines replace brain function rather than muscle (brawn) function. We also gave up a very important attribute of decision systems: interpretability. The black box models do not make themselves understood, they do not explain how they work. We can only observe how they perform and try to make corrections when they get it wrong. These models do not tell us why they decide that an animal on a photo is more likely a cat than a dog. They do not understand “catness” or “dogness”. They are simply performing pixel pattern matching, comparing what we feed them to the patterns they have encountered during the training phase.\nWith the arrival of generative AI this seemed to have changed. AI algorithms appeared much smarter and to understand much more about the world. The length of text generated by GPT models seemed unlimited. With the release of GPT-3.5 and ChatGPT in late 2022 we all experienced a massive change in AI capabilities.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Generative AI</span>"
    ]
  },
  {
    "objectID": "genAI.html#what-is-generative-ai",
    "href": "genAI.html#what-is-generative-ai",
    "title": "10  Generative AI",
    "section": "10.2 What is Generative AI?",
    "text": "10.2 What is Generative AI?\nGenerative Artificial Intelligence (GenAI) refers to artificial intelligence systems that are not explicitly programmed and are capable of producing novel content or data. You can say that GenAI systems can generate data of the same kind that was used for training. A GenAI image system generates new images based on an input image, a text system generates new text based on input text. However, GenAI systems now can handle input and output of different modality: generating images or video from text, for example.\nThe underlying technology of a GenAI system can be a generative adversarial network (GAN), a variational autoencoder (VAE), a diffusion-based system for image generation, or a generative pre-trained transformer (GPT). Whatever the technology, GenAI systems have some common traits that are relevant for our discussion. Our previous discussion is relevant because some of these traits connect back to the properties of large neural networks.\nThe “T” in GPT stands for Transformer, a neural network architecture designed for sequence-to-sequence learning: take one sequence, for example, a text prompt and generate another sequence based on it, for example, a poem. Or, translate a sequence of text from one language to another language.\nVaswani et al. (2017) introduced transformer architecture to overcome the shortcomings of sequence-to-sequence networks at the time: lack of contextual understanding, difficulties with longer sequences, limited opportunities to parallelize training algorithms. This is the technology behind GPT, the generative pre-trained transformer. We now know what the “G” and “T” stand for. How about the “P”, pre-trained?\nPrevious AI systems such as the convolutional neural networks of the previous section were trained in an approach called supervised learning. During training, the algorithm is presented with labeled data, identifying the correct value of the data point. An image with a cat is labeled “cat”, an image with a mailbox is labeled “mailbox”. The algorithm associates features of the image with the provided labels. Presented with a new image it evaluates the probabilities that its contents match the patterns it has previously seen. The predicted label is that for the category with the highest probability.\nA GPT system is not trained on labeled data. It learns in a self-supervised way, finding patterns and relationships in the data that lead to a foundation model, fundamental understanding of the data used in training. GPT-3.5, a large language model with 175 billion parameters, was trained on text data from Wikipedia, books, and other resources available on the internet through 2021. Based on what GPT 3.5 learned from that database in a self-supervised way, applications can be built on top of the foundation model. ChatGPT, for example, is a “question-answer” system built on the GPT models.\nI am using quotation marks here to describe ChatGPT as a “question-answer” system because it is not trained to produce answers. It is trained to generate coherent text. The system is optimized for fluency, not for accuracy. That is an important distinction. Responses from large language models are coherent, fluent, and sound authoritative. That does not mean they are factually correct. If you consider that generating output in sequence-to-sequence modeling means to choose the most likely next word or token given the sequence of tokens generated so far, the fact that the responses are grammatically correct is an astounding achievement. The systems do not know a right from a wrong answer or a plausible response from an implausible response without human intervention. Unfortunately, we fall into the trap of mistaking a well-worded response for a factual response.\n\nLike all neural networks, transformers and GenAI tools have random elements. For example, the starting values of the network parameters are usually chosen at random. During training there are other random mechanisms at work, the selection of randomly chosen batches of observations for gradient calculations, etc. Once the parameters are determined, the responses from the model should be deterministic, right? Not true. Large language models contain random elements during the inference phase, when the model generates content based on user input. This makes the responses non-deterministic. Ask the same question three times and you might get three different responses.\nWhile this seems troubling, it is considered an important property of generative models that increases novelty and serendipity. Even with the so-called temperature parameter dialed all the way down, a small amount of variability remains.\n\nWe spend a bit of ink on past approaches to AI, neural networks, and transformers to get to this point. It helps to inform the next topic of conversation about the ethics of AI in general, and of generative AI in particular.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Generative AI</span>"
    ]
  },
  {
    "objectID": "genAI.html#ethical-considerations",
    "href": "genAI.html#ethical-considerations",
    "title": "10  Generative AI",
    "section": "10.3 Ethical Considerations",
    "text": "10.3 Ethical Considerations\nIn this section we draw, among other sources, on a presentation by Scott Mutchler, formerly of Trilabyte, now Associate Professor of Practice in the Academy of Data Science at Virginia Tech. The presentation is available here.\nEthics is the systematic study of what constitutes good and bad conduct, including moral judgments and values. It examines questions such as:\n\nWhat actions are considered right or wrong?\nWhat makes a good life or society?\nHow should moral values guide individual and collective decision-making?\n\nEthical AI usage deals with defining and implementing good conduct that is generally considered to be good for both individuals and society as a whole. It is an emerging and an important field that no one involved with or touched by AI can ignore.\nWe do not discuss malfeasance, illegal behavior, and other intentionally unethical acts here. These are not ethical. Period. If you use AI to imitate a voice in order to deceive someone, it is clearly unethical. Using AI generated content to disinform is unethical because disinformation is unethical, regardless of the channel.\n\n\n\n\n\n\nMisinformation and Disinformation\n\n\n\nThe difference between misinformation and disinformation is important. In both cases incorrect information is communicated. When you say something that is not true, it is misinformation. If the goal is to deceive, it is disinformation.\nFor example, telling Bob that the party starts at 9 pm when it starts at 8 pm is misinformation if you got the facts wrong. If you tell Bob the party starts at 9 pm because you want him to show up late, you engage in disinformation.\n\n\nWhat we want to discuss here are the ways in which AI, in particular generative AI, has ethical implications that you need to be aware of. Harmful results can come from unintended consequences, habitual behavior, built-in biases, and so on.\n\nHarm\nThe legal definition of harm is to cause loss of or damage to a person’s right, property, or physical or mental well-being. The ethical definition of harm goes further: to limit someone’s opportunities and to deny them the possibility of a good life because of our actions or decisions. Perpetuating stereotypes or misallocating/withholding resources is harmful, and therefore unethical, if it limits opportunities; it might not be illegal.\nThe types of of harm associated with generative AI can be grouped in several categories.\n\nWillful Malicious Intent\n\nFraud\nViolence\nDisinformation\nMalware generation\nInappropriate content (sexually explicit, hate speech)\n\nImpact on Jobs\nImpact on the Environment\nBias\nHallucinations\nIntellectual Property Rights\nPrivacy\n\nWe are not going to dwell on willful malicious actions, these are obviously harmful and unethical.\n\n\nImpact on Jobs\nAll technical developments affect jobs. New technology might give us new tools to do our jobs better, it might create new jobs, and it might eliminate jobs. The greater the technological step forward, the greater typically the impact on how we live our lives and how we earn a living. The industrial revolution caused major job losses in the agricultural society, by replacing human and animal labor with machines and folks moving into the cities to take non-agricultural jobs. It created many more new jobs than it eliminated and increased employment overall. Hindsight is 20:20, most occupations in the post-industrial period were unimaginable at the time of the industrial revolution. Try explaining to a loom operator in 1840 what a software engineer does.\nEvery major technological advance is accompanied with hype about the impact on society, lives, and jobs. The rise of AI is no different. When it seemed that machines can perform tasks that were previously the sole domain of humans during the deep neural network area, fears initially were stoked about machines taking over humanity Terminator-style. That did not happen and anxiety was modulated into machines taking all our jobs. But that did not happen either and the story changed again from machines replacing us to machines augmenting what we do, making us better at what we do. Instead of AI image processing replacing the radiologist, they now turned into better radiologists assisted by AI to handle the routine MRIs so that they can focus their expertise on the difficult cases.\nThe hype and story line around generative AI is even worse, as algorithms are now able to create quality content that previously required extensive training: art, writing, video, code, etc.\nGoldman Sachs estimated that as many as 300 million full-time jobs could be lost or diminished globally by the rise of generative AI, with white-collar workers likely to be the most at risk.\nWe should ask some questions here:\n\nWhat is the relationship between the number of jobs created by generative AI versus the number of jobs lost due to GenAI?\nWhich occupations are impacted and how?\nWhich jobs are augmented and enhanced by generative AI instead of eliminated or diminished?\n\nIn order for a technology to completely eliminate a job, all its constituent activities must be replaced. If GenAI generates content that a digital marketer produces, it cannot replace the marketer unless her other tasks are accounted for or we redefine what it means to do digital marketing.\nQuestions 1. Think of occupations that could be eliminated entirely by GenAI. 2. Which parts of jobs are susceptible to be replaced by GenAI? 3. Can you imagine new jobs created by GenAI?\nMcKinsey and Company believe that by 2030 generative AI could account for automation of 30% of the hours worked today (Figure 10.3). Most affected are those working in STEM fields, education and training, creatives and arts management, and the legal profession. Do you agree?\n\n\n\n\n\n\nFigure 10.3: Impact of automation through generative AI according to McKinsey\n\n\n\nIt is widely believed that generative AI will make us more productive. You can now generate large amounts of text or code in seconds. Software developers can whip up programs much more quickly thanks to AI coding assistants. The conclusion is that the first jobs to be impacted are those where generative AI excels and those that have mundane, repetitive tasks. For example,\n\nData Entry\nAdministrative\nCustomer Service\nManufacturing\nRetail – Check Out\nLow Level Analysts\nEntry-Level Graphic Design\nTranslation\nCorporate Photography\n\nThat is the theory.\nDespite the advances in (generative) AI, interactions with AI customer service agents continue to be disappointing. AI is used in self checkout lines in retail stores. Instead of cashiers staffing the registers, employees are now monitoring the self checkout station to help customers, perform age checks, etc. It does not appear that AI eliminated any jobs, it is making shopping more odious.\nLockett (2024) discusses a survey by Intel of 6,000 users of AI PCs in Europe. Hoping that their productivity is greatly enhanced by AI-assisted computing, they found the opposite. Rather than saving time and boosting productivity, users of AI PCs were less productive, spending more on tasks than users of traditional, non-AI PCs. Ooops. The users spent more time figuring out how to best communicate with the AI and moderating and correcting the output from AI.\nAmazon’s walk-out grocery stores, where you can just pick an item of the shelf and walk out while AI takes care of the rest, are converting to self-scan stores. AI made too many mistakes which required many new employees to monitor the video feeds and verify most purchases. Instead of automating the shopping experience, and saving human resources, the system created jobs and was not economical.\nLabor cost is the most important cost factor in many companies. In startup companies the human resource expenses can be 80% of the total operating expenses. In larger companies, you might still spend 50–60% of the operating expenses on personnel. The pressure to reduce cost by reducing headcount will not go away.\nWill generative AI create new jobs? The one job that was talked about when ChatGPT hit the scene was the Prompt Engineer. How the AI acts and the way in which it responds depends on how it is prompted (how it is asked). While prompt engineering is a thing, the prompt engineer as a profession is not. Writing good prompts will be more done behind the scenes, by apps and agents that call the LLM API on your behalf.\nThe disconnect between expectation and reality is evident when the vast majority of company executives believe (generative) AI will boost productivity while the majority of their employees state that AI has increased their workload, spending more time on moderating AI output than doing the work themselves.\nThis is a common refrain for AI tools. They are great for ideation, brainstorming, drafting. Getting a polished end product from AI is more difficult. A particular issue with generative AI are hallucinations.\n\n\nImpact on the Environment\nThe impacts of generative AI on the environment are positive and negative. Proponents of GenAI cite greater efficiency and productivity due to AI which allows organizations to run better and that saves resources. In a cost-benefit analysis it seems that these perceived resource savings due to efficiency gains are far outweighed by the negative impact of GenAI on the environment.\nThe enthusiasm around GenAI is due to its potential benefits, many of them have not been realized as organizations are struggling to implement GenAI solutions. Concerns such as hallucinations, bias, and others discussed in this chapter are contributing factors. The negative impacts on the environment are very real, however.\nBashir et al. (2024) point out this imbalance between perceived potential good and real downsides:\n\nThis incomplete cost calculation promotes unchecked growth and a risk of unjustified techno-optimism with potential environmental consequences, including expanding demand for computing power, larger carbon footprints, shifts in patterns of electricity demand, and an accelerated depletion of natural resources. \n\nWhat are the reasons for the environmental impact of GenAI? Training these models requires immense compute resources. The models have billions of parameters and training is an iterative process during which the performance of the model slowly improves over iterations. In fact, parameters such as the learning rate are managed during the training process to make sure that the model does not learn too fast.\nUntil the rise of artificial neural network in the 2010s, training statistical and machine learning models relied primarily on traditional CPU-style chip architectures. The prevalent calculations in neural networks are operations on vectors of numerical data not unlike those encountered in processing graphical images. Graphical processing units (GPUs) were quickly adopted by the AI community to speed up the training and inference of neural networks. GPUs provide much greater levels of parallelism than CPU technology.\nThis created a massive demand for GPUs, one that propelled GPU maker NVIDIA into the position of one of the most valuable companies in the world. GPUs require a lot of electricity and they generate a lot of heat. Even a high-end personal computers equipped with many GPUs might require water cooling instead of fans to keep the machine from overheating. Data centers consume a lot of power and require a lot of water for cooling. The rise of GenAI has increased the demand for data centers and will make these trends only worse.\nBased on calculations of annual use of water for cooling systems, it is estimated that a session of questions and answers with GPT-3 (roughly 10 t0 50 responses) drives the consumption of a half-liter of fresh water (Berreby 2024).\nScientists have estimated that the power requirements of data centers in North America increased from 2,688 megawatts at the end of 2022 to 5,341 megawatts at the end of 2023, partly driven by the demands of generative AI (Zewe 2025). Note that this increase coincides with the release of GPT-3 at the end of 2022. Data centers are among the top 10 electricity consumers in the world, consuming more than entire nations. By 2026, data centers are expected to rank 5th in the world in electricity consumption, between Russia and Japan. Experts agree that it will not be possible to generate that much power from green technology, increasing demand for fossil fuel. Some large technology companies are considering nuclear energy to power their data centers.\nA 2019 study estimated the carbon footprint of training a transformer AI model (as in GPT) as 626,000 lbs of CO2. Not that this predates the large GPT models like GPT-3 and GPT-4. We can only assume that their carbon footprint is much greater. The 2019 number alone is staggering when compared to the carbon footprint of a mid-size car. Training one transformer model is equivalent to the carbon footprint of 5 cars over their entire lifetime.\nThis is just the training of the model. The usage of the models also requires large amounts of computing resources. A single ChatGPT query has been estimated to consume about five times more electricity than a simple web search (Zewe 2025).\nThe casual user sending prompts to ChatGPT is not aware of the environmental costs of their queries.\n\n\nHallucinations\nA hallucination in a GenAI response occurs when the AI perceives patterns that do not exist and generates output that is incorrect or even nonsensical. It is a nice way of saying that generative AI are “bullshit” machines. Studies have found a hallucination rate of large language model (LLM) as high as 10–20%.\nRecall that large language models are optimized for fluency and coherence of the response, not for accuracy. Therein lies a danger because a plausible, authoritative, and well-crafted response seems more accurate than gibberish. Even if the responses contain the same information. Given that sequence-to-sequence models chose the next word of the response based on the likelihood of words, it is surprising that LLMs perform as well as they do. One explanation for this phenomenon is the human feedback the systems received during training. Machine learning through reinforcement learning relies on a reward function that ranks the possible actions. The change in score in a game is an easy way to track possible moves a player can make. When generating text in response to a prompt it is more difficult to rank possible answers. Human evaluators were used in the training phase to rank different answers so the LLM can learn.\n\n\n\n\n\n\nTip\n\n\n\nA good hallucination check is to ask an LLM a question for which you know the answer. For example, ask it to write a short bio of yourself.\nWhen I tried this in 2023, ChatGPT produced a lot of incorrect information about me—a lot. For example, it stated I was the president of a professional society on Bayesian statistics. I had never been the president of a professional society and have not been a member of any Bayesian societies. When I tried the experiment again in January 2024, I received the following response:\n\n\n\n\n\n\nFigure 10.4: My January 2024 biography according to ChatGPT.\n\n\n\nThis is factually correct. Also, ChatGPT indicated the sources from which it compiled the biography. It also did not construct any new text but combined passages from different sources into a coherent write up. A click on the Sources icon on ChatGPT showed that the system relied on 16 resources across the internet to articulate its response.\n\n\nYou can affect the LLM response and thereby the extent of hallucinations by prompting the AI. Two prompts are important in this respect: the system prompt and the constitutional prompt. They precede the actual inquiry to the system. A system prompt is more general, it specifies for example the format of the response or how the AI solves math problems. The constitutional prompt tells the AI exactly how to act.\nYou can find the system prompts for Claude Sonnet here. Notice that they change over time. Below is an excerpt from the November 22, 2024 prompt. Notice that the system prompt explains when Claude’s response uses the term hallucination.\n\nClaude cannot open URLs, links, or videos. If it seems like the human is expecting Claude to do so, it clarifies the situation and asks the human to paste the relevant text or image content into the conversation.\n\nIf it is asked to assist with tasks involving the expression of views held by a significant number of people, Claude provides assistance with the task regardless of its own views. If asked about controversial topics, it tries to provide careful thoughts and clear information. Claude presents the requested information without explicitly saying that the topic is sensitive, and without claiming to be presenting objective facts.\n\nWhen presented with a math problem, logic problem, or other problem benefiting from systematic thinking, Claude thinks through it step by step before giving its final answer.\n\nIf Claude is asked about a very obscure person, object, or topic, i.e. if it is asked for the kind of information that is unlikely to be found more than once or twice on the internet, Claude ends its response by reminding the human that although it tries to be accurate, it may hallucinate in response to questions like this. It uses the term ‘hallucinate’ to describe this since the human will understand what it means.\n\nAn example of a constitutional prompt that presses the AI to distinguish fact from opinion, avoid speculation, and express uncertainty, follows:\n\nYou are an AI assistant committed to providing accurate and reliable information. Always express uncertainty when you’re not completely sure about something.\n\nClearly distinguish between facts and opinions. When referencing specific information, mention its general source. Be upfront about the limitations of your knowledge, including your knowledge cutoff date.\n\nAvoid speculation and making up information beyond your training data. If a query is ambiguous or you lack sufficient information, ask for clarification rather than making assumptions.\n\nIf you realize you’ve made an error, acknowledge it immediately and provide a correction. When discussing uncertain topics, use probabilistic language like “evidence suggests” rather than making absolute statements. Encourage users to verify important information from authoritative sources, especially for critical decisions.\n\nIf asked about topics outside your area of knowledge, clearly state that you cannot provide reliable information on that subject. By following these guidelines, you will help ensure that your responses are as accurate and reliable as possible, minimizing the risk of hallucinations or misinformation.\n\nHere are a few other ways to reduce the amount of hallucinations:\n\nCross-referencing with reliable external sources (web search), other LLMs\nExternal validation, for example by using a compiler and debugger to check code\nConsistency by prompting the LLM in several ways and compare results\nConfidence monitoring by asking LLMs to express uncertainty and asking for confidence in prompt\nAsk for sources\n\n\n\nBias\n\n\n\nReading Assignment: Bias in Generative AI\n\n\nRead this 2023 article on Bloomberg.com about race and gender bias in images generated by Stable Diffusion.\n\nWhich forms of bias discussed below contribute to the results discussed in the article?\nYou cannot change the training data of the diffusion model. How can you use constitutional prompts to change its output?\nThe study establishes that the Stable Diffusion data base is not representative of the race and gender distributions in the U.S. That raises many follow-up questions:\n\nHow do these results fair in different regions around the world?\nWhat “population” is the Stable Diffusion training data representative of?\nWhat are the everyday consequences of presenting a group in a non-representative way?\nIf 90% of the internet imagery are generated by AI in the future, what does that mean for fairness and inclusiveness?\n\n\n\n\nGenAI models are essentially large machine learning models. The considerations regarding bias in machine learning (ML) apply here as well. Suresh and Guttag (2021) distinguish a number of sources of bias in ML. Important among those are\n\nHistorical Bias. Models do not extrapolate what they learned from the training data to situations that go beyond the training data. They can create associations only from what is in the training data: the past is prologue. These are not oracles. These are stochastic parrots, assembling a likely response based on past data.\n\nThis bias is also called pre-existing bias; it is rooted in social institutions, practices and attitudes that are reflected in training data. Baking these into the algorithm reinforces and materializes the bias. We’ll see an example of this bias below in an analysis of images generated from Stable Diffusion.\nRepresentation Bias. This bias occurs because there is a mismatch between the training data and the application of the data. LLMs, for example, are trained on data up to a certain time point. They cannot extrapolate into the future. Also, data from the internet has a recency bias, current events are more likely found in the data than past events.\nSampling Bias. This is a special form of representation bias where the data points in the sample do not reflect the target population. For example, sampling data from the internet will over-represent countries that have a larger footprint on the internet. Self-selection is another source of sampling bias. Those whose contributions happen to be chosen for the training data have a disproportionate likelihood of having their voices heard. By sampling certain social media sites more than others, or by relying on social media at all, the opinions of the general population are not fairly represented.\nLearning Bias. The learning process of the model favors certain outcomes over others. This bias is also introduced by human labelers that define the ground truth for a machine learning algorithm or by human evaluators who rank different responses.\n\nHern (2024) explains why terms like “delve” and “tapestry” appear more frequently in ChatGPT responses compared to the internet at large. “Delve” is much more common in business English in Nigeria, where the human evaluators of ChatGPT evaluate the responses–because human evaluation is expensive and labor in Nigeria is cheap. The feedback by the workers training the AI system biases the system to write slightly like an African.\n\nBecause generative AI models are un-supervised and require massive amounts of data to be trained, the process of removing bias often falls to the end of the process.\n\nNicoletti and Bass (2023) write about the proliferation of generative AI tools\n\nAs these tools proliferate, the biases they reflect aren’t just further perpetuating stereotypes that threaten to stall progress toward greater equality in representation — they could also result in unfair treatment. Take policing, for example. Using biased text-to-image AI to create sketches of suspected offenders could lead to wrongful convictions.\n\nThey conducted a fascinating study of the GenAI text-to-image generator Stable Diffusion. Stable Diffusion was trained on pairs of images and captions taken from LAION-5B, a publicly available data set derived from CommonCrawl data scraped from the web. 5 billion image-text pairs were classified based on language and filtered into separate data sets by resolution.\nThe authors asked Stable Diffusion to generate over 5,000 images of representations of various jobs. What they found is not surprising, yet quite disturbing:\n\nThe analysis found that image sets generated for every high-paying job were dominated by subjects with lighter skin tones, while subjects with darker skin tones were more commonly generated by prompts like “fast-food worker” and “social worker.”\n\n\nFor each image depicting a perceived woman, Stable Diffusion generated almost three times as many images of perceived men. Most occupations in the dataset were dominated by men, except for low-paying jobs like housekeeper and cashier.\n\nFigure 10.5 shows images of doctors generated by Stable Diffusion. This is what the AI thinks represents doctors. Almost all of them are men, the skin tones are mostly white-to-light. Teachers, on the other hand, were predominantly white females (Figure 10.6).\n\n\n\n\n\n\nFigure 10.5: Images of doctors generated by Stable Diffusion.\n\n\n\n\n\n\n\n\n\nFigure 10.6: Images of teachers generated by Stable Diffusion.\n\n\n\nOne could argue that these results reflect reality and should not be compared to how we want the world to be. Unfortunately, the results do not even reflect reality, they are worse than that. Compared to the data of the US Bureau of Labor Statistics, which tracks the race and gender of workers in every occupation, Stable Diffusion over-represents women in occupations such as dishwasher, cashier, house keeper, and social worker. It under-represents women, compared to the US average, in occupations such as CEO, lawyer, judge, doctor, and janitor.\nWe conclude that the Stable Diffusion training data is not representative of occupations in the U.S. Stereotypes are further perpetuated when asked for images of politicians, criminals, terrorists, etc.\n\n\nIntellectual Property Rights\nYour car is property. Your laptop and your pet are property. What is intellectual property? The creations of the human intellect, such as literary works, designs, inventions, art, ideas, are called intellectual property (IP). IP laws exist to protect the creators of intellectual property through copyright, trademark, and patents.\n\n\n\n\n\n\nMy Patents\n\n\n\n\n\nOver the years I was awarded a few patents, all of them date to my time as software developer of an analytics company (Figure 10.7). Intellectual property created in the course of employment is typically the property right of the employer. You do get your name on the patent, however.\n\n\n\n\n\n\nFigure 10.7: My patent wall at home.\n\n\n\n\n\n\nIntellectual property infringement occurs when someone uses, copies, or distributes another person’s intellectual property without permission. Reproducing or distributing unauthorized copies of copyrighted works is an infringement as is the use of a trademark, even if it is similar, to a registered trademark. Creating a product based on a patented idea infringes on the rights of the patent holder.\nIf you add a company’s logo (assuming it is trademarked) to a presentation requires their permission. Using someone’s music in a video requires the permission of the copyright holder. Making a copy of text or a map requires permission.\n\n\n\n\n\n\nAgloe, New York\n\n\n\nTo trap copyright violators, the founder of General Drafting, a road mapping company, included a fictitious hamlet named Agloe in Delaware County, New York on their map. If Agloe showed up on maps by other publishers they knew that their work had been copied. This is known as a map trap or “trap street”.\nCopyright infringement of maps is more difficult to prove than with, say, a text book. Maps can appear identical because they map the same things. A road is a road. To write the same exact book by accident is not plausible, on the other hand. But when your map contains features that do not exist, and they reappear on someone else’s map, then you have a strong case that your intellectual work was copied.\nAccording to Wikipedia, Agloe appeared on a Rand McNally map and briefly on Google Maps. Check out this amazing story.\n\n\nThis seems pretty cut-and-dried and it seems that you can get into trouble for things we all might have done. Who hasn’t used a copyrighted or trademarked logo of a company in a presentation. When your slide is about Google or Amazon Web Services, then putting their logo on the slide makes sense.\n\n\nReading Assignment: U.S. Copyright Fair Use Index\n\n\nU.S. Copyright Fair Use Index\n\n\nWith respect to copyright, the fair use legal doctrine comes to our help. While there are no specific rules that spell out what fair use is, the doctrine exists to allow use of copyrighted works without requiring permission, under certain circumstances. Examples of fair use of copyrighted works include critic or commentary, news reporting, teaching, scholarly research. Section 107 of the Copyright Act considers four factors in evaluating fair use:\n\nThe first factor relates to the character of the use, whether the allegedly infringing work is transformative or is merely duplicating the original work. Transformative uses that add something new are more likely to be considered fair. Use of a commercial nature is less lilely to be considered fair than use for *nonprofit educational purposes**.\nUsing a creative or imaginative work (a novel or song) is more likely fair use compared to a factual work such as a scientific paper.\nThe quantity and quality of the copyrighted material. The more material is included, the less likely is it considered fair use. If the copyrighted material is the heart of the work, then even a small amount of copyrighted material can void fair use.\nWhether the use is hurting the market for the original work, for example by displacing sales.\n\nThe kind of copyright challenges the courts have to adjudicate are listed in the fair use index. It makes for some entertaining reading.\n\nThe fair use doctrine is being tested in many lawsuits that challenge how generative AI fits within the intellectual property landscape. There are many questions to be resolved:\n\nWhen an AI company uses copyrighted material without permission as training data does this fall under fair use?\nIs generating content based on copyrighted material sufficiently transformative to be considered derivative work that falls under fair use?\nOutput from AI models can reproduce parts of the training data. Is regurgitating the training data an unauthorized copy of the original? This aspect of GenAI models is called memorization. For example, you can ask an LLM what are the first two paragraphs of the Washington Post article about topic X on day Y.\nIf AI can be used to generate responses (prose, images) in the style of someone else, is this sufficiently transformative from the protected works? Appel, Neelbauer, and Schweidel (2023) report in Harvard Business Review that in late 2022, three artists filed class action suit against multiple generative AI platforms for using their original work without license to train AI so that it can respond in the style of their work. The image licensing service Getty Images sued Stable Diffusion for violating its copyright and trademark rights.\n\nThe New York Times has sued OpenAI and Microsoft for the unauthorized use of “millions of The Times’s copyrighted articles, in-depth investigations, opinion pieces, reviews, how-to guides, and more”. Because GPT can reproduce and/or summarize content from The Times, ChatGPT and Microsoft’s CoPilot and Bing search (both built on top of GPT) essentially circumvent the Times’ paywall. This argument of the complaint speaks to factor 4 above, hurting the Times’ current market. Since LLMs hallucinate, the tools also can wrongly attribute false information to The Times. From the complaint:\n\nUsers who ask a search engine what The Times has written on a subject should be provided with neither an unauthorized copy nor an inaccurate forgery of a Times article, but a link to the article itself.\n\nOpenAI and Microsoft insist that their use of material from the Times is fair use and serves a transformative purpose. They appeal to the transformative element of factor 1.\nHow this and other cases resolve in the courts can potentially redefine the intellectual property landscape. For example, is the model trained on data itself a derivative of the copyrighted work, or not? Some say that a statistical model is not in the realm of copyright at all. Others say it is when it is derived from copyrighted material. If you are interested in how Harvard Law experts weigh in on this legal dispute, click here.\n\n\nAssignment: Intellectual Property Questions\n\n\nConsider the following questions regarding intellectual property implications of generative AI.\n\nHow does the legal uncertainty around intellectual property in generative AI affect organizations use of GenAI? Note that copyright infringements can result in damages up to $150,000 for each instance of knowing use.\nWhat can developers of AI tools do if the current approach of copying content into training data, creating new content from it or reproducing it, does not fall under fair use?\nWhat can/should customers of AI tools do?\nWhat can/should content creators do?\nWhat should the user of the AI tool do?\n\nReading suggestions: Harvard Business Review Harvard Law\n\n\nWith respect to the last question in the assignment, you can take (some) control over how GenAI tools handle copyright issues through the constitutional prompt. The prompt below ensures that generated content respects creators’ rights and provides appropriate credit to original sources while maintaining clarity and brevity in responses.\n\nWhen generating content, provide clear attribution for any copyrighted works or proprietary data used. For copyrighted materials, include the title, creator’s name, publication year, and source. For proprietary information, state the owner, relevant trademark or copyright notices, and permission status if known.\n\nUse appropriate citation methods for direct quotes or close paraphrasing, such as quotation marks or block quotes, and provide specific sources. If uncertain about copyright status or attribution requirements, explicitly state this in your response.\n\nAvoid using or reproducing content protected by copyright or proprietary rights without proper attribution or permission. When asked to generate potentially infringing content, suggest alternatives or ways to obtain proper permissions.\ntransparent about the origin of information and respect intellectual property rights. If using AI-generated content, acknowledge this fact.\n\n\nA final interesting intellectual property question we raise here is whether AI can make inventions in the sense of the patent law. As of now, patents are awarded to natural persons, the argument goes that only humans can make the inventions. An implicitly programmed algorithm, one produced by AI, is not eligible for patent protection. What about the human who controlled and directed the AI to create a novel algorithm?\n\n\nPrivacy\nOne of the top issues limiting the adoption of generative AI in business is the protection of corporate data, trade secrets, and personally identifiable information (PII). Data breaches and privacy risks in protecting user data are listed as the two most important topics that influence an organization’s position about generative AI. This is followed by transparency of AI outcomes.\nAny data a user submits to a GenAI tool as part of the prompt leaves the organization’s premises and becomes part of the AI tool’s training corpus. Obviously, company leaders are mortified to think that employees chat with LLMs about confidential information that should never be shared outside the organization.\n\n\n\nAssignment: LLM Response With/Without Prompt\n\n\nHave ChatGPT, Claude, or another LLM of your choice write a story about a family that lives in the suburbs of Chicago that visits a family in downtown Chicago.\nPrompt with and without the following prompt:\n\nPlease provide an objective and balanced response to the following question, considering multiple perspectives and avoiding any cultural, gender, racial, or other biases. If relevant, acknowledge the complexity of the issue and potential limitations in your knowledge. Here’s the question: [INSERT QUESTION HERE]\n\n\nWhat are the key differences in the responses?\n\nDo you detect differences in biases?\n\n\n\n\n\nAssignment: Role Play Scenarios\n\n\nRole play as a data scientist at an online retailer. Answer these questions using what you have learned about ethics and generative AI.\nScenario: Your boss has asked you to build a customer service chatbot that answers questions about the products you sell online. She/he asks that you put a “positive spin” on all chatbot answers.\n\nWhat techniques could you use to modify the LLM output you are using?\n\nAre you crossing ethical lines by giving “positive spin”?\n\nHow do you respond to your boss?\n\nScenario: You are building an AI agent that helps the HR department filter hundreds of resumes for data science openings on your team. HR asks for a solution that filters the candidates to ones with a strong track record of delivery, relevant skills and a solid educational background. They also want a summary of why each remaining candidate would be a good fit for the role.\n\nHow do you respond?\n\nHow might ethics govern your response?\n\nGive some possible techniques to ensure the application supports Diversity, Eauity, and Inclusion (DEI).\n\n\n\n\n\n\nFigure 10.2: AlexNet, a convolutional neural network (Mallick and Nayak 2018).\nFigure 10.3: Impact of automation through generative AI according to McKinsey\nFigure 10.5: Images of doctors generated by Stable Diffusion.\nFigure 10.6: Images of teachers generated by Stable Diffusion.\nFigure 10.7: My patent wall at home.\n\n\n\nAppel, Gil, Juliana Neelbauer, and David A. Schweidel. 2023. “Generative AI Has an Intellectual Property Problem.” Harvard Business Review. https://hbr.org/2023/04/generative-ai-has-an-intellectual-property-problem.\n\n\nBashir, Noman, Priya Donti, James Cuff, Sydney Sroka, Marija Ilic, Vivienne Sze, Christina Delimitrou, and Elsa Olivetti. 2024. “The Climate and Sustainability Implications of Generative AI.” An MIT Exploration of Generative AI.\n\n\nBerreby, David. 2024. “As Use of A.I. Soars, so Does the Energy and Water It Requires.” https://e360.yale.edu/features/artificial-intelligence-climate-energy-emissions.\n\n\nHern, Alex. 2024. “TechScape: How Cheap, Outsourced Labour in Africa Is Shaping AI English.” The Guardian. https://www.theguardian.com/technology/2024/apr/16/techscape-ai-gadgest-humane-ai-pin-chatgpt.\n\n\nLockett, Will. 2024. “Intel Admits AI Decreases Productivity.” https://medium.com/predict/intel-admits-ai-decreases-productivity-226681d1af18.\n\n\nMallick, Satya, and Sunita Nayak. 2018. “Number of Parameters and Tensor Sizes in a Convolutional Neural Network (CNN).” https://learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/.\n\n\nNicoletti, Leonardp, and Dina Bass. 2023. “Humans Are Biased. Generative AI Is Even Worse.” Bloomberg Technology + Equality. https://www.bloomberg.com/graphics/2023-generative-ai-bias/.\n\n\nSuresh, H., and J. Guttag. 2021. “A Framework for Understanding Sources of Harm Throughout the Machine Learning Life Cycle.” https://arxiv.org/pdf/1901.10002.pdf.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Proceedings of the 31st International Conference on Neural Information Processing Systems, 6000–6010. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.\n\n\nZewe, Adam. 2025. “Explained: Generative AI’s Environmental Impact.” https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Generative AI</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Adhikari, Ani, John DeNero, and David Wagner. 2022. Computational\nand Inferential Thinking: The Foundations of Data Science. 2nd Ed.\nhttps://inferentialthinking.com/chapters/intro.html.\n\n\nAndreessen, Mark. 2011. “Why Software Is Eating the World.”\nhttps://a16z.com/why-software-is-eating-the-world/.\n\n\nAppel, Gil, Juliana Neelbauer, and David A. Schweidel. 2023.\n“Generative AI Has an Intellectual Property Problem.”\nHarvard Business Review. https://hbr.org/2023/04/generative-ai-has-an-intellectual-property-problem.\n\n\nBashir, Noman, Priya Donti, James Cuff, Sydney Sroka, Marija Ilic,\nVivienne Sze, Christina Delimitrou, and Elsa Olivetti. 2024. “The\nClimate and Sustainability\nImplications of Generative\nAI.” An MIT Exploration of Generative AI.\n\n\nBaumer, Benjamin S., Kaplan. Daniel T., and Nicholas J. Horton. 2021.\nModern Data Science with r, 2nd Ed. Chapman & Hall/CRC\nPress. https://mdsr-book.github.io/mdsr3e/.\n\n\nBenson H., Dusek J. A., Sherwood J. B., P. Lam, C. F. Bethea, W.\nCarpenter, S. Levitsky, et al. 2006. “Study of the Therapeutic\nEffects of Intercessory Prayer (STEP) in Cardiac Bypass Patients: A\nMulticenter Randomized Trial of Uncertainty and Certainty of Receiving\nIntercessory Prayer.” American Heart Journal 151 (4):\n934–42.\n\n\nBerreby, David. 2024. “As Use of A.I. Soars, so Does\nthe Energy and Water It Requires.” https://e360.yale.edu/features/artificial-intelligence-climate-energy-emissions.\n\n\nBox, George E. P. 1976. “Science and Statistics.”\nJournal of the American Statistical Association 71 (356):\n791–99.\n\n\nBox, George E. P., and Norman R. Draper. 1987. Empirical\nModel-Building and Response Surfaces. John Wiley & Sons, New\nYork.\n\n\nChang, Winston. 2018. R Graphics Cookbook: Practical Recipes for\nVisualizing Data, 2nd Ed. O’Reilly Media. https://r-graphics.org/.\n\n\nChristian, Brian, and Tom Griffiths. 2017. Algorithms to Live by.\nThe Computer Science of Human Decisions. Henry Holt; Company, New\nYork.\n\n\nGrue, Lars, and Arvid Heiberg. 2006. “Notes on the History of\nNormality–Reflections on the Work of Quetelet and Galton.”\nScandinavian Journal of Disability Research 8 (4): 232–46.\n\n\nHeathcote, James A. 1995. “Why Do Old Men Have Big Ears?”\nBMJ 311 (7021): 1668. https://doi.org/10.1136/bmj.311.7021.1668.\n\n\nHern, Alex. 2024. “TechScape: How Cheap, Outsourced Labour in\nAfrica Is Shaping AI English.” The Guardian. https://www.theguardian.com/technology/2024/apr/16/techscape-ai-gadgest-humane-ai-pin-chatgpt.\n\n\nHoughton, A. N., J. Flannery, and M. V. Viola. 1980. “Malignant\nMelanoma in Connecticut and Denmark.” International Journal\nof Cancer 25: 95–104.\n\n\nHuff, Darrell. 1954. How to Lie with Statistics. W.W. Norton\n& Company, New York.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning: With Applications in\nr, 2nd Ed. Springer. https://www.statlearning.com/.\n\n\nLockett, Will. 2024. “Intel Admits AI Decreases\nProductivity.” https://medium.com/predict/intel-admits-ai-decreases-productivity-226681d1af18.\n\n\nMallick, Satya, and Sunita Nayak. 2018. “Number of Parameters and\nTensor Sizes in a Convolutional Neural Network (CNN).” https://learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/.\n\n\nMesserli, F. H. 2012. “Chocolate Consumption, Cognitive Function,\nand Nobel Laureates.” New England Journal of Medicine\n367: 1562–64.\n\n\nNeyman, Jerzy. 1952. Lectures and Conferences on Mathematical\nStatistics and Probability. Graduate School, Dept. of Agriculture,\nWashington.\n\n\nNicoletti, Leonardp, and Dina Bass. 2023. “Humans Are Biased.\nGenerative AI Is Even Worse.” Bloomberg Technology +\nEquality. https://www.bloomberg.com/graphics/2023-generative-ai-bias/.\n\n\nNowinski, Christopher J., Samantha C. Bureau, Michael E. Buckland,\nMaurice A. Curtis, Daniel H. Daneshvar, Richard L. M. Faull, Lea T.\nGrinberg, et al. 2022. “Applying the Bradford Hill Criteria for\nCausation to Repetitive Head Impacts and Chronic Traumatic\nEncephalopathy.” Frontiers in Neurology 13. https://doi.org/10.3389/fneur.2022.938163.\n\n\nO’Neil, Cathy. 2017. Weapons of Math Destruction. How Big Data\nIncreases Inequality and Threatens Democracy. Crown, New York.\n\n\nPeng, Roger D., Sean Kross, and Brooke Anderson. 2020. Mastering\nSoftware Development in r. https://bookdown.org/rdpeng/RProgDA/.\n\n\nRubino, Francesco, David E Cummings, Robert H Eckel, Ricardo V Cohen,\nJohn P H Wilding, Wendy A Brown, Fatima Cody Stanford, et al. 2025.\n“Definition and Diagnostic Criteria of Clinical Obesity.”\nThe Lancet Diabetes & Endocrinology. https://doi.org/10.1016/S2213-8587(24)00316-4.\n\n\nSilver, Nate. 2012. The Signal and the Noise: Why so Many\nPredictions Fail–but Some Don’t. Penguin Books.\n\n\nSnow, John. 1855. On the Mode of Communication of Cholera, 2nd.\nEd. John Churchill, London. https://archive.org/stream/b28985266#page/n3/mode/2up.\n\n\nSpiegelhalter, David. 2021. The Art of Statistics. How to Learn from\nData. Basic Books.\n\n\nSuresh, H., and J. Guttag. 2021. “A Framework for Understanding\nSources of Harm Throughout the Machine Learning Life Cycle.” https://arxiv.org/pdf/1901.10002.pdf.\n\n\nTent, M. B. W. 2006. The Prince of Mathematics: Carl Friedrich\nGauss. CRC Press, Boca Raton, FL.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems,\n6000–6010. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.\n\n\nWickham, H. 2019. Advanced r, 2nd Ed. Chapman & Hall/CRC\nPress. http://adv-r.had.co.nz/.\n\n\nWickham, H., M. Cetinkaya-Rundel, and G. Grolemund. 2023. R for Data\nScience: Import, Tidy, Transform, Visualize, and Model Data, 2nd\nEd. O’Reilly Media. https://r4ds.hadley.nz/.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2019. R Markdown:\nThe Definite Guide. Chapman & Hall/CRC Press. https://bookdown.org/yihui/rmarkdown/.\n\n\nXie, Yihui, Dervieux Christophe, and Emily Riederer. 2021. R\nMarkdown Cookbook. Chapman & Hall/CRC Press. https://bookdown.org/yihui/rmarkdown-cookbook/.\n\n\nZewe, Adam. 2025. “Explained: Generative AI’s Environmental\nImpact.” https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117.",
    "crumbs": [
      "References"
    ]
  }
]